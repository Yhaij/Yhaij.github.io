<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MapReduce 多路径输出]]></title>
    <url>%2F2018%2F11%2F20%2FMapReduce-%E5%A4%9A%E8%B7%AF%E5%BE%84%E8%BE%93%E5%87%BA%2F</url>
    <content type="text"><![CDATA[mapreduce中实现多路径输出主要使用MulitipleOutputs类通过两个例子可以掌握 输入样例 mulitipleInput.txt1234567file1 001file2 002file3 003file2 004file1 005file1 006file3 007 输出：file1和file3开头的记录归到一个文件下file2和file3开头的记录归到一个文件下 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class MultipleOutputsExample extends Configured implements Tool&#123; public static class MultipleMapper extends Mapper&lt;Object, Text, Text, NullWritable&gt;&#123; private MultipleOutputs&lt;Text, NullWritable&gt; mos; @Override protected void setup(Mapper&lt;Object, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // TODO Auto-generated method stub mos = new MultipleOutputs&lt;Text, NullWritable&gt;(context); &#125; @Override protected void map(Object key, Text value, Mapper&lt;Object, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // TODO Auto-generated method stub String[] infos = value.toString().split(" "); if(infos[0].equals("file1"))&#123; mos.write("file1", value, NullWritable.get()); &#125;else if (infos[0].equals("file2")) &#123; mos.write("file2", value, NullWritable.get()); &#125; else &#123; mos.write("file1", value, NullWritable.get()); mos.write("file2", value, NullWritable.get()); &#125; &#125; @Override protected void cleanup(Mapper&lt;Object, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // TODO Auto-generated method stub mos.close(); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; // TODO Auto-generated method stub Configuration conf = new Configuration(); String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); if (otherArgs.length &lt; 2)&#123; System.err.println("Usage: Data Deduplication &lt;in&gt; &lt;out&gt;"); System.exit(2); &#125; Job job = Job.getInstance(conf); job.setJarByClass(MultipleOutputsExample.class); job.setMapperClass(MultipleMapper.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); MultipleOutputs.addNamedOutput(job, "file1", TextOutputFormat.class, Text.class, NullWritable.class); MultipleOutputs.addNamedOutput(job, "file2", TextOutputFormat.class, Text.class, NullWritable.class); FileInputFormat.addInputPath(job, new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); return job.waitForCompletion(true)? 0:1; &#125; public static void main(String[] args) throws Exception &#123; System.exit(ToolRunner.run(new MultipleOutputsExample(), args)); &#125;&#125; 结果12345678910111213$ hadoop fs -cat /user/test/wordTest/mulitipleOutput/file1-m-00000file1 001file3 003file1 005file1 006file3 007$ hadoop fs -cat /user/test/wordTest/mulitipleOutput/file2-m-00000file2 002file3 003file2 004file3 007file2 008 如果想把file1和file2的内容放入不同的目录下，可以通过指定baseOutputPath，将file1开头的文件放在同一个目录中管理。将mos.write(&quot;file1&quot;, value, NullWritable.get());和mos.write(&quot;file2&quot;, value, NullWritable.get());改为mos.write(&quot;file1&quot;, value, NullWritable.get(),&quot;file1/part&quot;);和mos.write(&quot;file2&quot;, value, NullWritable.get()，&quot;file2/part&quot;);或mos.write(value, NullWritable.get(),&quot;file1/part&quot;);和mos.write(value, NullWritable.get()，&quot;file2/part&quot;);可以看到输出结果 123456$ hadoop fs -ls /user/test/wordTest/mulitipleOutputFound 4 items-rw-r--r-- 3 hdfs hdfs 0 2018-06-30 16:18 /user/test/wordTest/mulitipleOutput/_SUCCESSdrwxr-xr-x - hdfs hdfs 0 2018-06-30 16:18 /user/test/wordTest/mulitipleOutput/file1drwxr-xr-x - hdfs hdfs 0 2018-06-30 16:18 /user/test/wordTest/mulitipleOutput/file2-rw-r--r-- 3 hdfs hdfs 0 2018-06-30 16:18 /user/test/wordTest/mulitipleOutput/part-r-00000 指定baseOutputPath输出路径和输出文件名直接按照baseOutPutPath指定，但是默认输出文件名后缀会跟上-r-00000，如果想更改可以继承FileOutputFormat重写RecordWriter实现。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 小文件的处理]]></title>
    <url>%2F2018%2F11%2F20%2FHadoop-%E5%B0%8F%E6%96%87%E4%BB%B6%E7%9A%84%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[hadoop的HDFS和MapReduce本身都是用户处理大量数据的大文件，对于小文件来说，由于namenode会在记录每个block对象，如果存在大量的小文件，会占用namenode的大量内存空间，而且HDFS存储文件是按block来存储，即使一个文件的大小不足一个block的大小，文件还是会占用一个block的存储空间，所以大量的小文件会对HDFS的存储和访问都带来不利的影响。 hadoop对于小文件的处理主要有Hadoop Archive，Sequence file和CombineFileInputFormat三种方式。 Hadoop ArchiveHadoop Archive是hadoop的归档命令，可以将hdfs上的小文件打包成一个har文件，这种方式虽然不会减少小文件占用大量存储空间的问题，但是会减少namenode的内存空间。同时har文件支持hdfs命令对其的访问。 命令：hadoop archive -archiveName 归档名称 -p 父目录 [-r &lt;复制因子&gt;] 原路径（可以多个） 目的路径 -archiveNames设置归档生成文件的名字 -p 需要进行归档的文件的父目录 例子：123456789$ hadoop fs -ls /user/test/yhj/input/Found 3 items-rw-r--r-- 3 root hdfs 760 2018-07-04 11:48 /user/test/yhj/input/word1.txt-rw-r--r-- 3 root hdfs 82 2018-07-04 11:48 /user/test/yhj/input/word2.txt-rw-r--r-- 3 root hdfs 1738 2018-07-04 11:48 /user/test/yhj/input/word3.txt$ hadoop archive -archiveName word.har -p /user/test/yhj/input/ word1.txt word2.txt word3.txt /user/test/yhj/harInput/$ hadoop fs -ls /user/test/yhj/harInput/Found 1 itemsdrwxr-xr-x - hdfs hdfs 0 2018-07-05 20:18 /user/test/yhj/harInput/word.har HAR文件的生成是通过运行一个mapreduce的程序生成，所以需要集群环境中装个mapreduce HAR是在Hadoop file system之上的一个文件系统，因此所有fs shell命令对HAR文件均可用，但使用不同的URI。另外，请注意档案是不可变的。所以，重命名，删除并创建返回一个错误，例如：1234567891011$ hadoop fs -ls /user/test/yhj/harInput/word.harFound 4 items-rw-r--r-- 3 hdfs hdfs 0 2018-07-05 20:18 /user/test/yhj/harInput/word.har/_SUCCESS-rw-r--r-- 5 hdfs hdfs 255 2018-07-05 20:18 /user/test/yhj/harInput/word.har/_index-rw-r--r-- 5 hdfs hdfs 22 2018-07-05 20:18 /user/test/yhj/harInput/word.har/_masterindex-rw-r--r-- 3 hdfs hdfs 2580 2018-07-05 20:18 /user/test/yhj/harInput/word.har/part-0$ hadoop fs -ls har:/user/test/yhj/harInput/word.harFound 3 items-rw-r--r-- 3 hdfs hdfs 760 2018-07-04 11:48 har:///user/test/yhj/harInput/word.har/word1.txt-rw-r--r-- 3 hdfs hdfs 82 2018-07-04 11:48 har:///user/test/yhj/harInput/word.har/word2.txt-rw-r--r-- 3 hdfs hdfs 1738 2018-07-04 11:48 har:///user/test/yhj/harInput/word.har/word3.txt 可以看到Hadoop存档目录包含元数据（采用_index和_masterindex形式）、数据部分data（part- *）文件、归档文件的名称和部分文件中的位置（_index文件）。 HAR文件也可以被mapreduce读取，路径的URI可以使用不同的URI,比如例子中的文件输入的路径URI可以下面两种方式使用12hdfs://10.1.13.111:8020/user/test/yhj/harInput/word.harhar://hdfs-10.1.13.111:8020/user/test/yhj/harInput/word.har 但是这个例子的文件来说，两个输入路径产生map的个数是不同的，har的路径产生的map有三个，对应三个word*.txt,而hdfs的路径只有一个，对应word.har/part-0 如果是文件支持行记录切分使用mapreduce来处理数据（文件的前后数据不相互影响），建议使用hdfs的URI路径,因为存档目录的part-*可能包括多个小文件的数据，这样可以减少map的个数，不会为每个单独的小文件启动一个map。 CombineFileInputFormat将大量小文件做为mapreduce的输入是不合适的，因为FileInputFormat只会分割大文件（文件大小超过设定的分片大小，默认为HDFS的块大小），对于小于分片大小的文件，每个文件作为一个分片，如果文件大小小于一个块的大小，mapreduce会为每个小文件产生一个map，这样会产生大量小文件，而每个map只会处理少量数据，每次map操作都会产生开销。当然可以通过mapred.min.split.size和mapred.max.split.size来控制map数量。 CombineFileInputFormat是mapreduce针对小文件而设计的，CombineFileInputFormat可以将多个小文件打包进一个分片，另外，比直接设置map数量好的在于，CombineFileInputFormat在决定将那些块放入一个分片是会考虑到块所在的节点和机架的位置，避免操作分片是过多的数据传输。 CombineFileInputFormat是一个抽象类，hadoop自带的实现的有CombineTextInputFormat，我们可以通过继承CombineFileInputFormat实现createRecordReader方法，自定义RecordReader类来实现理海量小文件的MapReduce。 InputFormat主要有两个方法，getSplits（计算得到分片），createRecordReader（产生返回RecordReader，RecordReader生成输出map读入的键值对） CombineFileInputFormat中已经实现了getSplits，即将多个小文件打包进一个分片中CombineFileSplit，我们需要实现createRecordReader方法，返回一个可以读取该分片中内容的RecordReader。 MyCombineInputFormat的实现123456789101112public class MyCombineInputFormat extends CombineFileInputFormat&lt;LongWritable, Text&gt;&#123; @Override public RecordReader createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException &#123; RecordReader&lt;LongWritable, Text&gt; reader = new CombineFileRecordReader&lt;&gt;((CombineFileSplit) inputSplit, taskAttemptContext, MyCombineFileRecordReader.class); try &#123; reader.initialize(inputSplit, taskAttemptContext); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return reader; &#125;&#125; 这里实际返回了一个CombineFileRecordReader的对象，CombineFileRecordReader通过CombineFileSplit，context和Class&lt;? extends RecordReader&gt;类型构造，MyCombineFileRecordReader是我们对于CombineFileSplit中每一个文件的产生map的输入的方法。CombineFileRecordReader中的nextKeyValue方法，会为每一个打包在CombineFileSplit中的文件构造一个RecordReader方法，读取文件中的记录。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class CombineFileRecordReader&lt;K, V&gt; extends RecordReader&lt;K, V&gt; &#123; ... public CombineFileRecordReader(CombineFileSplit split, TaskAttemptContext context, Class&lt;? extends RecordReader&lt;K, V&gt;&gt; rrClass) throws IOException &#123; this.split = split; this.context = context; this.idx = 0; this.curReader = null; this.progress = 0L; try &#123; this.rrConstructor = rrClass.getDeclaredConstructor(constructorSignature); this.rrConstructor.setAccessible(true); &#125; catch (Exception var5) &#123; throw new RuntimeException(rrClass.getName() + " does not have valid constructor", var5); &#125; this.initNextRecordReader(); &#125; protected boolean initNextRecordReader() throws IOException &#123; if(this.curReader != null) &#123; this.curReader.close(); this.curReader = null; if(this.idx &gt; 0) &#123; this.progress += this.split.getLength(this.idx - 1); &#125; &#125; if(this.idx == this.split.getNumPaths()) &#123; return false; &#125; else &#123; this.context.progress(); try &#123; Configuration conf = this.context.getConfiguration(); conf.set("mapreduce.map.input.file", this.split.getPath(this.idx).toString()); conf.setLong("mapreduce.map.input.start", this.split.getOffset(this.idx)); conf.setLong("mapreduce.map.input.length", this.split.getLength(this.idx)); this.curReader = (RecordReader)this.rrConstructor.newInstance(new Object[]&#123;this.split, this.context, Integer.valueOf(this.idx)&#125;); if(this.idx &gt; 0) &#123; this.curReader.initialize(this.split, this.context); &#125; &#125; catch (Exception var2) &#123; throw new RuntimeException(var2); &#125; ++this.idx; return true; &#125; public boolean nextKeyValue() throws IOException, InterruptedException &#123; do &#123; if(this.curReader != null &amp;&amp; this.curReader.nextKeyValue()) &#123; return true; &#125; &#125; while(this.initNextRecordReader()); return false; &#125; public K getCurrentKey() throws IOException, InterruptedException &#123; return this.curReader.getCurrentKey(); &#125; public V getCurrentValue() throws IOException, InterruptedException &#123; return this.curReader.getCurrentValue(); &#125; ...&#125; 在nextKeyValue方法中通过自定义的RecordReader的nextKeyValue读取当前文件的对象，当读完当前文件中的信息，后会通过initNextRecordReader返回初始化的下一个文件的RecordReader，所以我们只需实现相应的读取一个文件的RecordReader即可。 MyCombineFileRecordReader的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class MyCombineFileRecordReader extends RecordReader&lt;LongWritable, Text&gt; &#123; private CombineFileSplit combineFileSplit; private int currentIndex; private LineRecordReader reader = new LineRecordReader(); private int totalNum; public MyCombineFileRecordReader(CombineFileSplit combineFileSplit, TaskAttemptContext context, Integer index)&#123; super(); this.combineFileSplit = combineFileSplit; this.currentIndex = index; this.totalNum = combineFileSplit.getNumPaths(); &#125; @Override public void initialize(InputSplit inputSplit, TaskAttemptContext context) throws IOException, InterruptedException &#123; FileSplit fileSplit = new FileSplit(combineFileSplit.getPath(currentIndex), combineFileSplit.getOffset(currentIndex), combineFileSplit.getLength(currentIndex), combineFileSplit.getLocations()); context.getConfiguration().set("mapreduce.map.input.file.name", fileSplit.getPath().getName()); this.reader.initialize(fileSplit, context); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if(currentIndex &gt;= 0 &amp;&amp; currentIndex &lt; totalNum)&#123; return reader.nextKeyValue(); &#125;else &#123; return false; &#125; &#125; @Override public LongWritable getCurrentKey() throws IOException, InterruptedException &#123; return reader.getCurrentKey(); &#125; @Override public Text getCurrentValue() throws IOException, InterruptedException &#123; return reader.getCurrentValue(); &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; if(currentIndex &gt;= 0 &amp;&amp; currentIndex &lt; totalNum)&#123; return (float)currentIndex/totalNum; &#125; return 0; &#125; @Override public void close() throws IOException &#123; reader.close(); &#125;&#125; MyCombineFileRecordReader中通过LineRecordReader按行来读取文本记录，在initialize方法中通过CombineFileSplit和index（CombineFileSplit中文件信息的索引位置）来得到相应文件的信息，创建对应的FileSplit，接着创建LineRecordReader对象，在nextKeyValue中委托给LineRecordReader为mapper产生键-值对象。 最后入口函数和map类的实现，将InputFormatClass替换成自定义的MyCombineInputFormat类1234567891011121314151617181920212223242526272829public class CombineInputFromatMain extends Configured implements Tool&#123; public static class CombineInputFormatMap extends Mapper&lt;Object, Text, Text, Text&gt;&#123; private Text outKey = new Text(); @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; outKey.set(context.getConfiguration().get("mapreduce.map.input.file.name")); context.write(outKey, value); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; //设定默认job和设置输入输出路径的函数 Job job = JobDefaultInit.getClusterDefaultJob(this, getConf(), args); job.setJobName("CombineInputFormat Text"); job.setJarByClass(CombineInputFromatMain.class); job.setMapperClass(CombineInputFormatMap.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); job.setNumReduceTasks(0); job.setInputFormatClass(MyCombineInputFormat.class); return job.waitForCompletion(true) ? 0:1; &#125; public static void main(String[] args) throws Exception &#123; System.exit(ToolRunner.run(new CombineInputFromatMain(), args)); &#125;&#125; 在这例子中将三个word*.txt文件打包进一个分片，实际只产生了一个map。 Sequence filesequence file由一系列的二进制key/value组成，如果为key小文件名，value为文件内容，则可以将大批小文件合并成一个大文件。 顺序文件由文件头和随后的记录内容组成，顺序文件的前三个字节为SEQ(顺序文件代码)，紧接着一个字节表示文件的版本号，文件头还包括键和值的类型，数据是否压缩的标志位，是否进行快压缩的标志位， 数据的压缩形式，用户自定义的数据以及同步标识。顺序文件读取内容只能从同步标识开始读取。同步标识位于记录和记录之间，也就是说无法从记录中间开始读取顺序文件的内容。 Sequence file的格式主要有三种，分为未压缩，记录压缩和块压缩。主要格式的存储方式可以查看官方给出的api:http://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/SequenceFile.html 将小文件合并成一个sequence file的实现(代码参考hadoop 权威指南)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117public class SmallFilesToSequenceFileConverter extends Configured implements Tool &#123; public static class WholeFileInputFormat extends FileInputFormat&lt;LongWritable, Text&gt;&#123; /** * 不切分文件，一个split读入整个文件 * @param context * @param filename * @return */ @Override protected boolean isSplitable(JobContext context, Path filename) &#123; return false; &#125; @Override public RecordReader&lt;LongWritable, Text&gt; createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; RecordReader reader = new WholeFileRecordReader(); reader.initialize(inputSplit, taskAttemptContext); return reader; &#125; &#125; /** * 自定义RecordReader，读取整个小文件内容 */ public static class WholeFileRecordReader extends RecordReader&lt;LongWritable, Text&gt;&#123; private FileSplit fileSplit; private Configuration conf; private LongWritable key = new LongWritable(); private Text value = new Text(); private boolean process = false; @Override public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; this.fileSplit = (FileSplit)inputSplit; this.conf = taskAttemptContext.getConfiguration(); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if(!process)&#123; FileSystem fs = fileSplit.getPath().getFileSystem(conf); FSDataInputStream in = null; try &#123; in = new FSDataInputStream(fs.open(fileSplit.getPath())); byte[] contextByte = new byte[(int)fileSplit.getLength()]; IOUtils.readFully(in, contextByte, 0, contextByte.length); //等同于 in.read(contextByte, 0, contextByte.length); String context = new String(contextByte, "utf-8"); key.set(fileSplit.getStart()); value.set(context); &#125;finally &#123; IOUtils.closeStream(in); &#125; process = true; return true; &#125; return false; &#125; @Override public LongWritable getCurrentKey() throws IOException, InterruptedException &#123; return key; &#125; @Override public Text getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return process? 1.0f:1.0f; &#125; @Override public void close() throws IOException &#123; &#125; &#125; public static class SmallFilesToSequenceFileMap extends Mapper&lt;Object, Text, Text, Text&gt;&#123; private Text outKey = new Text(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; outKey.set(((FileSplit)context.getInputSplit()).getPath().toString()); &#125; @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; context.write(outKey, value); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; //设定默认job和设置输入输出路径的函数 Job job = JobDefaultInit.getClusterDefaultJob(this, getConf(), args); job.setJobName("SmallFiles To SequenceFile"); job.setMapperClass(SmallFilesToSequenceFileMap.class); job.setInputFormatClass(WholeFileInputFormat.class); job.setOutputFormatClass(SequenceFileOutputFormat.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); return job.waitForCompletion(true)? 0:1; &#125; public static void main(String[] args) throws Exception &#123; System.exit(ToolRunner.run(new SmallFilesToSequenceFileConverter(), args)); &#125;&#125; hdfs可以通过命令行hadoop fs -text来显示以文本的方式显示顺序文件 读取SequenceFile简单实现1234567891011121314151617181920212223242526272829public class SequenceFileReadMain extends Configured implements Tool&#123; public static class SequenceFileReadMap extends Mapper&lt;Text, Text, Text, Text&gt;&#123; private Text outKey = new Text(); private Text outValue = new Text(); @Override protected void map(Text key, Text value, Context context) throws IOException, InterruptedException &#123; outKey.set("key : " + key.toString()); outValue.set("value : " + value.toString()); context.write(outKey, outValue); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; Job job = JobDefaultInit.getClusterDefaultJob(this, getConf(), args); job.setJobName("Sequence File Read"); job.setMapperClass(SequenceFileReadMap.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); job.setInputFormatClass(SequenceFileInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); return job.waitForCompletion(true)?0:1; &#125; public static void main(String[] args) throws Exception&#123; System.exit(ToolRunner.run(new SequenceFileReadMain(), args)); &#125;&#125; 这时候读取SequenceFile的时候，key对应的是小文件的名字，value是一个小文件的所有内容，所以需要在map编写处理整个小文件内容的代码 参考资料：https://blog.csdn.net/u011007180/article/details/52333387 https://www.cnblogs.com/staryea/p/8603112.html http://dongxicheng.org/mapreduce/hdfs-small-files-solution/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mapreduce 排序]]></title>
    <url>%2F2018%2F11%2F20%2Fmapreduce-%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[mapreduce的排序主要分部分排序、全排序和辅助排序（二次排序） 可以直接在reduce中在对数据进行排序，但是这对于reduce的负担太重，数据处理的时间消耗也会大大增加 mapreduce机制中排序只会针对键进行排序，所以如果想对某个数据进行排序，一定要将其设置为map输出的键，排序主要发生在map的spill和合并spill file阶段和reduce拉取复制map端的数据后合并成reduce文件时。 排序的设置和调用的顺序排序类及其方法调用主要遵循以下顺序： 如果设置mapreduce.job.output.key.comparator.class或设置了Job类的setSortComparatorClass(),则会使用设置的类的实例进行排序。该类需要继承WritableComparator 如果1中没有设置，键的类型是WritableComparable的子类，就会调用该键的compareTo（)的方法 如果上述都RawComparator将字节流反序列化为WritableComparable对象，再调用其compareTo（)进行比较 从以上可以看出，如果重写RawComparator的compare方法，在字节流的时候就进行所需的键的比较是性能最好的，因为这样无需在排序进行反序列化，但编写的难度相对较高。 部分排序mapreduce根据键的排序使得每个reduce输出的文件都是排序好的 全排序对于全排序最简单的方法是将所有的方法是将reduce的个数设置为1，但是如果数据量太大会对reduce造成过大的负担。 解决的方法可以通过改写partition来调整使得每个reduce之间数据保持有序，即保证一段连续区间内的数据都落在一个分区里，这样只要对reduce的输出文件排好序就可以达到全局排序。但是如果靠人为的划分分区，需要知道键的范围，并且很容易造成数据的倾斜，造成有的reduce分到数据特别多，而有的却很少，理想情况下是个分区记录数大致相等。解决方法是通过多数据进行采样，通过采样一小部分数据得到排序数据的大致分布，从而制定出如何划分分区。 hadoop内置了若干采样器，用于需要实现自定义采样的话需要继承InputSampler类(该类实现了Sampler接口)重写getSample方法，返回采样的键。InputSampler实现了静态方法writePartitionFile()方法来在致指定的hdfs路径下创建一个顺序文件来存储定义分区的键。将顺序文件加入分布式缓存，由TotalOrderPartitioner使用并未排序作业创建分区。 例子，使用采样分区的方式对一系列随机生成的服从正态分布的数据进行全排序。 原始输入数据，满足正态分布，随机生成 1234567891011hadoop fs -cat /user/test/yhj/random/input/random.txt | head12816584491071195471-4229131970634916817-557165-14358191924747-262067 重写InputFormat和RecordReader使直接读入的值是IntWritable类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public static class MyFileIntputFormat extends FileInputFormat&lt;LongWritable, IntWritable&gt;&#123; @Override public RecordReader&lt;LongWritable, IntWritable&gt; createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; RecordReader&lt;LongWritable, IntWritable&gt; reader = new MyRecordReader(); reader.initialize(inputSplit, taskAttemptContext); return reader; &#125; &#125; public static class MyRecordReader extends RecordReader&lt;LongWritable, IntWritable&gt;&#123; private LineRecordReader lineRecordReader; public MyRecordReader()&#123; lineRecordReader = new LineRecordReader(); &#125; @Override public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; lineRecordReader.initialize(inputSplit, taskAttemptContext); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; return lineRecordReader.nextKeyValue(); &#125; @Override public LongWritable getCurrentKey() throws IOException, InterruptedException &#123; return lineRecordReader.getCurrentKey(); &#125; @Override public IntWritable getCurrentValue() throws IOException, InterruptedException &#123; IntWritable value = new IntWritable(Integer.parseInt(lineRecordReader.getCurrentValue().toString())); return value; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return lineRecordReader.getProgress(); &#125; @Override public void close() throws IOException &#123; lineRecordReader.close(); &#125; &#125; 由于默认的采样器都是对键进行采样，而我们读入的键是偏移量，真正排序采样的是值，所以需要重写采样器，这里继承了RandomSampler，对值进行随机采样，RandomSampler的初始化需要三个参数RandomSampler(double freq, int numSamples, int maxSplitsSampled) freq代表采样频率，numSamples代表样本最大样本数，maxSplitsSampled代表最大分区数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public static class MySample extends InputSampler.RandomSampler&lt;Object, IntWritable&gt;&#123; public MySample(double freq, int numSamples) &#123; super(freq, numSamples); &#125; public MySample(double freq, int numSamples, int maxSplitsSampled)&#123; super(freq, numSamples, maxSplitsSampled); &#125; @Override public IntWritable[] getSample(InputFormat&lt;Object, IntWritable&gt; inf, Job job) throws IOException, InterruptedException &#123; List&lt;InputSplit&gt; splits = inf.getSplits(job); List&lt;IntWritable&gt; samples = new ArrayList&lt;&gt;(); int splitToSample = Math.min(this.numSamples, splits.size()); Random random = new Random(); long seed = random.nextLong(); random.setSeed(seed); //随机交换split for(int i = 0; i &lt; splits.size(); i++)&#123; InputSplit tmp = splits.get(i); int index = random.nextInt(splits.size()); splits.set(i, splits.get(index)); splits.set(index, tmp); &#125; //采样 for(int i = 0; i &lt; splitToSample || i &lt; this.numSamples &amp;&amp; samples.size() &lt; this.numSamples ; i++)&#123; TaskAttemptContext sampleContext = new TaskAttemptContextImpl(job.getConfiguration(), new TaskAttemptID()); RecordReader&lt;Object, IntWritable&gt; reader = inf.createRecordReader(splits.get(i), sampleContext); while (reader.nextKeyValue()) &#123; //根据采样频率采样 if (random.nextDouble() &lt; this.freq) &#123; if (samples.size() &lt; this.numSamples) &#123; IntWritable value = new IntWritable(); samples.add(ReflectionUtils.copy(job.getConfiguration(), reader.getCurrentValue(), value)); &#125; else &#123; int index = random.nextInt(this.numSamples); if (index != this.numSamples) &#123; IntWritable value = new IntWritable(); samples.set(index, ReflectionUtils.copy(job.getConfiguration(), reader.getCurrentValue(), value)); &#125; this.freq *= (double) (this.numSamples - 1) / (double) this.numSamples; &#125; &#125; &#125; reader.close(); &#125; IntWritable[] result = new IntWritable[samples.size()]; samples.toArray(result); return result; &#125; &#125; 入口函数和map函数，reduce函数采用默认实现 12345678910111213141516171819202122232425262728293031323334public class SortByTotalPartitioner extends Configured implements Tool &#123; public static class SortByTotalPartitionerMap extends Mapper&lt;LongWritable, IntWritable, IntWritable, NullWritable&gt;&#123; @Override protected void map(LongWritable key, IntWritable value, Context context) throws IOException, InterruptedException &#123; context.write(value, NullWritable.get()); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; Job job = JobDefaultInit.getSubmintDefaultJob(this, getConf(), "E:\\JavaProjects\\hdpWork\\target\\hdpWork.jar", args);// InputSampler.Sampler&lt;&gt; job.setMapperClass(SortByTotalPartitionerMap.class); job.setInputFormatClass(MyFileIntputFormat.class); job.setOutputKeyClass(IntWritable.class); job.setOutputValueClass(NullWritable.class); job.setPartitionerClass(TotalOrderPartitioner.class); job.setNumReduceTasks(3); InputSampler.Sampler&lt;Object, IntWritable&gt; sampler = new MySample(0.1, 1000, 10); //构造采样器 TotalOrderPartitioner.setPartitionFile(job.getConfiguration(), new Path("/partitionFile")); //设置共享分区文件路径 InputSampler.writePartitionFile(job, sampler); //写入分区文件,其中调用了 TotalOrderPartitioner.getPartitionFile获取文件路径 //将共享分区文件加入到分布式缓存中 String partitionFile = TotalOrderPartitioner.getPartitionFile(job.getConfiguration()); URI partitionUri = new URI(partitionFile); job.addCacheFile(partitionUri); return job.waitForCompletion(true) ? 0:1; &#125; public static void main(String[] args) throws Exception&#123; System.exit(ToolRunner.run(new SortByTotalPartitioner(), args)); &#125;&#125; 辅助排序有时候我们需要对键和值一起参与排序或者排序收到两个数据的共同影响，由于mapreduce只能支持对键排序，所以只能自定义Writable类型，支持存储多种数据，并重写compareTo()方法或者通过setSortComparatorClass()设置二次排序的方式。由于reduce的输入是key和value的迭代器，默认reduce是将键相同的值放在一个迭代器中，而由于二次排序修改了键的比较方法，因此有时候需要修改分组排序，job.setGroupingComparatorClass设置分组排序，通过分组比较器判断为相同即返回为0的代表在同一组中，这时候的分组的键将会取组中第一个的键，也就是排好序后组中第一个的键。 例子，选取下面数据中，每一年的最高温度 1234567891011121314151617181920212223242526271990 281995 341992 -121994 21995 121993 341992 321993 11994 81995 231993 241993 421994 321992 301991 221993 261990 281992 111990 181994 161992 151994 41990 281995 51990 281992 31990 28 我们需要将年份和温度都参与排序，因此需要自定义Writable类型，同时存储、序列化年份和温度 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;public class IntPairWritable implements WritableComparable&lt;IntPairWritable&gt; &#123; int year; int tempeture; public IntPairWritable()&#123; year = 0; tempeture = 0; &#125; public IntPairWritable(IntWritable year, IntWritable tempeture)&#123; this.year = year.get(); this.tempeture = tempeture.get(); &#125; public IntPairWritable(int year, int tempeture)&#123; this.year = year; this.tempeture = tempeture; &#125; public int getYear() &#123; return year; &#125; public int getTempeture() &#123; return tempeture; &#125; public void setYear(int year) &#123; this.year = year; &#125; public void setTempeture(int tempeture) &#123; this.tempeture = tempeture; &#125; @Override public boolean equals(Object obj) &#123; if(!(obj instanceof IntPairWritable))&#123; return false; &#125; IntPairWritable other = (IntPairWritable)obj; return other.year == year &amp;&amp; other.tempeture == tempeture; &#125; @Override public int compareTo(IntPairWritable o) &#123; int cmp = year - o.getYear(); if(cmp != 0)&#123; return cmp; &#125; return tempeture - o.getTempeture(); &#125; /** * 序列化到输出流中 * @param dataOutput * @throws IOException */ @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeInt(year); dataOutput.writeInt(tempeture); &#125; /** * 从输入流中反序列化读入 * @param dataInput * @throws IOException */ @Override public void readFields(DataInput dataInput) throws IOException &#123; year = dataInput.readInt(); tempeture = dataInput.readInt(); &#125; @Override public int hashCode() &#123; return new Integer(year).hashCode()*163 + new Integer(tempeture).hashCode(); &#125; @Override public String toString() &#123; return year + "\t" + tempeture; &#125; /** * 自定义Writable类型的排序比较器 */ public static class Comparator extends WritableComparator&#123; public Comparator()&#123; super(IntPairWritable.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; int tmp = ((IntPairWritable)a).getYear() - ((IntPairWritable)b).getYear(); if(tmp != 0)&#123; return tmp; &#125; return - (((IntPairWritable)a).getTempeture() - ((IntPairWritable)b).getTempeture()); &#125; &#125;&#125; 按照年份来分区和分组 123456789/** * 只按照年份来分区 */public static class YearPartition extends Partitioner&lt;IntPairWritable, NullWritable&gt;&#123; @Override public int getPartition(IntPairWritable intPairWritable, NullWritable nullWritable, int i) &#123; return Math.abs(intPairWritable.getYear()) % i; &#125;&#125; 12345678910111213/** * 分组只按照年份来比较 */public static class GroupComparator extends WritableComparator&#123; public GroupComparator()&#123; super(IntPairWritable.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; return ((IntPairWritable)a).getYear() - ((IntPairWritable)b).getYear(); &#125;&#125; 这样同年份的数据会在同个分区中，也就是在一个reduce下，reduce的分组是按照年份来比较的，那么每个分组中的数据的年份是相同的，在年份相同的情况下按照温度从大到下排序，因此每个组的键取的是组中的第一个，也就是该年份下最大温度所在的IntPairWritable map、reduce及其入口函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class MaxTemperatureUsingSecondSort extends Configured implements Tool &#123; public static class MaxTemperatureUsingSecondSortMap extends Mapper&lt;LongWritable, Text, IntPairWritable, NullWritable&gt;&#123; IntPairWritable outKey = new IntPairWritable(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] infos = value.toString().split("\t"); outKey.setYear(Integer.parseInt(infos[0])); outKey.setTempeture(Integer.parseInt(infos[1])); context.write(outKey, NullWritable.get()); &#125; &#125; /** * reduce的key存储着该年份下的最高气味 */ public static class MaxTemperatureUsingSecondSortReduce extends Reducer&lt;IntPairWritable, NullWritable, IntPairWritable, NullWritable&gt;&#123; @Override protected void reduce(IntPairWritable key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, NullWritable.get()); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; Job job = JobDefaultInit.getSubmintDefaultJob(this, getConf(), "E:\\JavaProjects\\hdpWork\\target\\hdpWork.jar", args); job.setJobName("Max Temperature Using Second Sort"); job.setMapperClass(MaxTemperatureUsingSecondSortMap.class); job.setReducerClass(MaxTemperatureUsingSecondSortReduce.class); job.setPartitionerClass(YearPartition.class); job.setSortComparatorClass(IntPairWritable.Comparator.class); job.setGroupingComparatorClass(GroupComparator.class); job.setOutputKeyClass(IntPairWritable.class); job.setOutputValueClass(NullWritable.class); return job.waitForCompletion(true) ? 1:0; &#125; public static void main(String[] args) throws Exception &#123; System.exit(ToolRunner.run(new MaxTemperatureUsingSecondSort(), args)); &#125;&#125; 程序结果,由于只有一个reduce，因此年份按照排序从小到大，如果想要在多个reduce中保存年份的全局排序，应该按照全区排序编写合适的分区方法 1234561990 281991 221992 321993 421994 321995 34]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS扩容]]></title>
    <url>%2F2018%2F11%2F20%2Fhdfd%E6%89%A9%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[hdfs的存储容量不足，需要放入新磁盘扩容扩容有两种方式，一种是linux层面的，一种是hdfs层面的hdfs的datanode存储的目录可以查看hdfs-site.xml的dfs.datanode.data.dir的值 linux层面linux层面的就是将hdfs的datanode所挂载的分区或逻辑卷扩容 如果是直接挂在固定分区上，而磁盘还有余量可以进行分区扩容 参考：http://blog.51cto.com/wutou/1782931这种直接挂载在分区上的磁盘空间管理方式十分不方便，扩容也很容易出错，而且扩容只有在磁盘还有余量的情况下，无法将两块磁盘的空间结合使用，一块磁盘空间占满，即使加入一块新磁盘也无法对原来磁盘上的分区扩容。所以才会有逻辑卷管理LVM的出现 如果是lvm对磁盘分区进行管理，那么可以方便的创建物理卷，扩容datanode所挂载的逻辑卷所属的卷组接着扩容该逻辑卷，具体操作方式参考: https://linux.cn/article-3218-1.html hdfs层面这种不需要对原来的空间扩容，是修改hdfs-site.xml的dfs.datanode.data.dir的值添加新目录，如 1234567&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt; /hadoop/hdfs/data, /data/hadoop/hdfs &lt;/value&gt;&lt;/property 将多个目录用逗号隔开，所以只要将新磁盘分区挂载在新目录上（分区方式可以自己决定,最好永久挂载），将需要添加的新目录设置用户权限1chown -R hdfs:hadoop dirPath 修改dfs.datanode.data.dir，重启datanode即可1./bin/hdfs dfsadmin -report 可以查看各个datanode的存储容量]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[非整数0-1背包问题]]></title>
    <url>%2F2018%2F11%2F20%2F%E9%9D%9E%E6%95%B4%E6%95%B00-1%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[0-1背包问题通常情况下物品的重量是整数的，采用动态规划可以解决，在解决物品重量非整数情况下的背包问题之前，我们先来回顾整数背包问题，并从中寻找解决非整数背包问题的方法。 问题定义：有n种物品和一个容量为c的背包，第i件物品的重量为_wi_，价格为_vi_,求出哪种物品组合放入背包使物品价值总和最大。 整数0-1背包问题设$p(i,j)$表示在容量为j情况下，将物品$i，i+1…n$组合的放入背包的最优解的值 则其转移方程为 如果 $j&gt;=w_i$ ，$p(i,j) = max(p(i+1,j), p(i+1,j-w_i )+v_i )$ 如果 $j&lt;w_i$ , $p(i,j) = p(i+1,j)$ 可以理解为当$j&lt;w_i$ ,背包无法放下第$i$件物品，所以其最优解和考虑放$i+1$到$n$的物品到容量为j的背包的最优解相同。当$j&gt;=w_i$时，可以选择放和不放第$i$件物品，当不放时背包价值和$p(i+1,j)$相同，当放时背包价值为$p(i+1,j-w_i )$再加上第$i$件物品的价值。 所以整数背包问题可以采用动态规划解决，开辟$n*c$的数组，从下往上不断更新数组的值，得到$p(i,j)$的值，最优解就是$p(0,c)$的值 123456789101112131415161718192021public static int solution(int c, int[] v, int[] w)&#123; if(v.length != w.length)&#123; throw new IllegalArgumentException(); &#125; int n = v.length;int m = c+1; int[][] result = new int[n][m]; for(int i = 0;i &lt; m;i++)&#123; result[n-1][i] = i&gt;=w[n-1]? v[n-1]:0; &#125; for(int i = n-2;i&gt;=0;i--)&#123; for(int j = 0;j &lt; m;j++)&#123; if(j &lt; w[i])&#123; result[i][j] = result[i+1][j]; &#125;else &#123; result[i][j] = Math.max(result[i+1][j], result[i+1][j-w[i]]+v[i]); &#125; &#125; &#125; traceBack(c, v, w, result); return result[0][c];&#125; 路径回溯，找到最优组合 123456789101112131415private static void traceBack(int c, int[]v, int[] w, int[][] p)&#123; int k = c; List&lt;Integer&gt; trace = new ArrayList&lt;&gt;(); int i; for(i = 0;i &lt; p.length-1;i++)&#123; if(p[i][k] == p[i+1][k-w[i]]+v[i])&#123; k = k - w[i]; trace.add(i+1); &#125; &#125; if(p[i][k] == v[i])&#123; trace.add(i+1); &#125; System.out.println(trace);&#125; 整数0-1背包问题的改进例如背包的容量为10，5件物品的重量分别为2，2，6，5，4 ，对应价值分别是6，3，5，4，6，则$p(i,j)​$数组的更新情况如下 weight value 1 2 3 4 5 6 7 8 9 10 2 6 0 6 6 9 9 12 12 15 15 15 2 3 0 3 3 6 6 9 9 9 10 11 6 5 0 0 0 6 6 6 6 6 10 11 5 4 0 0 0 6 6 6 6 6 10 10 4 6 0 0 0 6 6 6 6 6 6 6 当背包的容量很大（即c的值特别大），则这个数组将会异常的庞大，并且数组的每一个数都需要更新，算法需要的计算时间较多。 所以可以进行适当的改进，从上面的表格来看，我们无需记录数组种的每一个数，只需要记录下每行的跳跃点即可，比如最后一行从第4列开始跳跃，我们只需记录{(0,0),(4,6)}即可,倒数第二行只需记录{(0,0),(4,6),(9,10)}即可。接下来的问题是如何更新每行的跳跃点。 p[5] = {(0,0),(4,6)} 将p[5]的跳跃点加上下一个需要放入的物品的重量和价值，则可以得到q[5] = p[5]+(5,4) = {(5,4),(9,10)}, 将p[5]和q[5]合并得到{(0,0),(4,6),(5,4),(9,10)}，之后去除重量大却价值小的点，如(5,4)点，放入背包的物品总重量大于4，价值却只有4小于6，所以通过比较(5,4)和(4,6)需要去掉(5,4)得到p[4] = {(0,0),(4,6),(9,10)} 非整数0-1背包问题非整数0-1背包问题可以转化为整数0-1背包问题，如果非整数可以用保留三位小数来表示的话，那么可以将非整数背包问题的所有值乘上1000,全部转为整数，采用整数背包问题解决，但是这样必须牺牲一定的精度，而且会增加开辟数组的大小（乘上1000，数组的列肯定超过1000以上），这对计算时间也有很大影响，因为需要更新每一个数组中的元素。 有效的解决方法和上述对于整数0-1背包问题的改进是一样的，而且发现上述的跳跃点并不要求是整数，对于实数一样适用，因此可以采用更新跳跃点的动态规划的方式解决非整数0-1背包问题。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public static double solution(double c, double[] v, double[] w)&#123; if(v.length != w.length)&#123; throw new IllegalArgumentException(); &#125; double[][] p = new double[10000][2]; int n = v.length; p[0][0] = 0;p[0][1] = 0; int left = 0, right = 0,next = 1; int[] head = new int[n+2]; head[n+1] = 0; head[n] = 1; for(int i = n-1; i &gt;= 0;i--)&#123; int k = left; for(int j = left; j &lt;= right;j++)&#123; if(p[j][0] + w[i] &gt; c)&#123; break; &#125; double nw = p[j][0] + w[i]; double nv = p[j][1] + v[i]; //放入比nw小的跳跃点，因为重量小的价值无论大小 for(;k&lt;=right &amp;&amp; p[k][0] &lt; nw;k++,next++)&#123; p[next][0] = p[k][0]; p[next][1] = p[k][1]; &#125; //如果重量相等，取价值大的跳跃点 if(k &lt;= right &amp;&amp; p[k][0] == nw)&#123; if(p[k][1] &gt; nv)&#123; nv = p[k][1]; &#125; k++; &#125; //放入更新的跳跃点 if(nv &gt; p[next-1][1])&#123; p[next][0] = nw; p[next][1] = nv; next++; &#125; /*去除比更新的跳跃点重量大却价值小的点， 由于是每一次更新完之后结果都是重量和价值都是递增的跳跃点排列 一旦出现价值超过当前的点，那后续的点一定都是超过的*/ for(;k &lt;= right &amp;&amp; p[k][1] &lt;= nv;k++); &#125; //将后续的点放入 for (;k &lt;= right; k++,next++)&#123; p[next][0] = p[k][0]; p[next][1] = p[k][1]; &#125; left = right+1;right = next - 1;head[i] = next; &#125; traceBack(v, w, p, head); return p[next-1][1];&#125; 路径回溯，找到最优组合 1234567891011121314151617private static void traceBack(double[] v, double[] w, double[][] p, int[] head)&#123; List&lt;Integer&gt; trace = new ArrayList&lt;&gt;(); int k = head[0]-1; int n = w.length; for(int i = 1;i &lt;= n;i++)&#123; int left = head[i+1]; int right = head[i]-1; for(int j = left;j&lt;=right;j++)&#123; if(p[j][0] + w[i-1] == p[k][0] &amp;&amp; p[j][1] + v[i-1] == p[k][1])&#123; k = j; trace.add(i); break; &#125; &#125; &#125; System.out.println(trace);&#125; 如果不考虑放入哪些物品（即不考虑路径回溯），之关心最优解的值，可以只存放当前的跳跃点集和下一步的跳跃点集，无需记录每一步的跳跃点集 12345678910111213141516171819202122232425262728293031323334353637public static double solution2(double c, double[] v, double[] w)&#123; if(v.length != w.length)&#123; throw new IllegalArgumentException(); &#125; int n = v.length; List&lt;double[]&gt; p = new ArrayList&lt;&gt;(); p.add(new double[]&#123;0, 0&#125;); for(int i = n - 1; i &gt;= 0;i--)&#123; int k = 0; List&lt;double[]&gt; q = new ArrayList&lt;&gt;(); for(double[] element: p)&#123; if(w[i] + element[0] &gt; c)&#123; break; &#125; double nw = w[i] + element[0]; double nv = v[i] + element[1]; for(;k &lt; p.size() &amp;&amp; p.get(k)[0] &lt; nw;k++)&#123; q.add(p.get(k)); &#125; if(k &lt; p.size() &amp;&amp; p.get(k)[0] == nw)&#123; if(p.get(k)[1] &gt; nv)&#123; nv = p.get(k)[1]; &#125; k++; &#125; if(nv &gt; q.get(q.size()-1)[1])&#123; q.add(new double[]&#123;nw, nv&#125;); &#125; for(;k &lt; p.size() &amp;&amp; p.get(k)[1] &lt; nv;k++); &#125; for(;k &lt; p.size();k++)&#123; q.add(p.get(k)); &#125; p = q; &#125; return p.get(p.size()-1)[1];&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>背包问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop的RPC分析]]></title>
    <url>%2F2018%2F11%2F20%2FHadoop%E7%9A%84RPC%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[RPCRPC就是远程过程调用,具体什么是RPC，看一个例子就会明白。比如客户端有一个RPC协议类Protocol。 123interfce Protocol&#123; int add(int a, int b);&#125; 但是客户端没有其实现的具体类，该类在服务端12345Class ProtocolImpl implenets Protocol&#123; int add(int a, int b)&#123; return a + b; &#125;&#125; 则客户端需要调用ProtocolImpl的add方法，需要将调用的方法及其参数等信息发送给服务端，服务端解析信息，调用ProtocolImpl的add方法，将结果在传输给客户端，而RPC的目的就是使客户端仿佛在调用自身的方法来得到该方法的结果。在这其中，Protocol就是一个RPC的协议，这个协议其实就是一个接口类，接口中的方法就是对外提供的远程调用方法。 RPC由四个模块组成：1、通信模块。两个相互协作的通信模块实现请求-应答协议,它们在客户和服务器之间传递请求和应答消息,一般不会对数据包进行任何处理。请求–应答协议的实现方式有同步方式和异步方式两种。同步模式下客户端程序一直阻塞到服务器端发送的应答请求到达本地; 而异步模式不同,客户端将请求发送到服务器端后,不必等待应答返回,可以做其他事情,待服务器端处理完请求后,主动通知客户端。在高并发应用场景中,一般采用异步模式以降低访问延迟和提高带宽利用率。2、Stub 程序（代理程序）。客户端和服务器端均包含Stub程序,可将之看做代理程序。它使得远程函数调用表现得跟本地调用一样,对用户程序完全透明。在客户端,它表现得就像一个本地程序,但不直接执行本地调用,而是将请求信息通过网络模块发送给服务器端。此外,当服务器发送应答后,它会解码对应结果。在服务器端,Stub程序依次进行解码请求消息中的参数、调用相应的服务过程和编码应答结果的返回值等处理。3、调度程序。调度程序接收来自通信模块的请求消息,并根据其中的标识选择一个Stub程序进行处理。通常客户端并发请求量比较大时,会采用线程池提高处理效率。4、客户程序/服务过程。请求的发出者和请求的处理者。 Hadoop RPCHadoop RPC主要分为四个部分,分别是序列化层、函数调用层、网络传输层和服务器端处理框架,具体实现机制如下: 序列化层。序列化主要作用是将结构化对象转为字节流以便于通过网络进行传输或写入持久存储,在RPC框架中,它主要用于将用户请求中的参数或者应答转化成字节流以便跨机器传输。Hadoop2.0之后，主要用Protocol Buffers和Apache Avro，Hadoop本身也提供了一套序列化框架，一个类只要实现Writable接口即可支持对象序列化与反序列化。 函数调用层。函数调用层主要功能是定位要调用的函数并执行该函数，Hadoop RPC采用了Java反射机制（服务器端）与动态代理（客户端）实现了函数调用。 网络传输层。网络传输层描述了Client与Server之间消息传输的方式，Hadoop RPC采用了基于TCP/IP的Socket机制。 服务器端处理框架。服务器端处理框架可被抽象为网络I/O模型，它描述了客户端与服务器端间信息交互方式,它的设计直接决定着服务器端的并发处理能力,常见的网络 I/O 模型有阻塞式 I/O、非阻塞式 I/O、事件驱动 I/O 等,而Hadoop RPC采用了基于Reactor设计模式的事件驱动 I/O 模型（NIO）。 Hadoop RPC的简单使用首先需要定义一个PRC协议，该接口必须继承VersionedProtocol接口1234public interface IProxyProtocol extends VersionedProtocol&#123; long versionID = 1234L; int add(int a, int b);&#125; 接着需要一个类实现PRC的接口用于服务端的调用123456789101112131415161718public class MyProxyProtocol implements IProxyProtocol &#123; @Override public int add(int a, int b) &#123; System.out.println("I am adding"); return a+b; &#125; @Override public long getProtocolVersion(String s, long l) throws IOException &#123; System.out.println("MyProxy.ProtocolVersion = " + IProxyProtocol.versionID ); return IProxyProtocol.versionID ; &#125; @Override public ProtocolSignature getProtocolSignature(String s, long l, int i) throws IOException &#123; return new ProtocolSignature(IProxyProtocol.versionID , null); &#125;&#125; 最后是实现客户端和服务端1234567891011121314public class MyRPCClient &#123; public final static int PORT = 8888; public final static String ADDRESS = "localhost"; public static void main(String[] args) throws Exception&#123; Configuration conf = new Configuration(); IProxyProtocol proxy; proxy = RPC.getProxy(IProxyProtocol.class, 111 , new InetSocketAddress(ADDRESS, PORT), conf); int result = proxy.add(2, 3); System.out.println(result); RPC.stopProxy(proxy); &#125;&#125; 1234567891011121314public class MyRPCServer &#123; public final static int PORT = 8888; public final static String ADDRESS = "localhost"; public static void main(String[] args) throws IOException&#123; RPC.Server server = new RPC.Builder(new Configuration()) .setProtocol(IProxyProtocol.class) .setInstance(new MyProxyProtocol()) .setBindAddress(ADDRESS) .setPort(PORT) .setNumHandlers(10) .build(); server.start(); &#125;&#125; 上述的Hadoop RPC的实现，客户端是调用了 RPC.getProxy方法，生成了IProxyProtocol的动态代理，在调用协议的方法时，代理类会将方法和参数出给服务端，服务端使用具体的实现类来执行方法并返回结果给客户端。需要注意的是RPC协议必须声明versionID这个变量或者定义ProtocolInfo注解（包含看协议的名字和versionID） 客户端的实现首先我们来看Hadoop RPC的客户端代码，调用RPC.getProxy方法，追踪这个方法，可以看到其内部实现1234public static &lt;T&gt; T getProxy(Class&lt;T&gt; protocol,long clientVersion,InetSocketAddress addr, Configuration conf) throws IOException &#123; return getProtocolProxy(protocol, clientVersion, addr, conf).getProxy();&#125; 内部调用了getProtocolProxy，而getProtocolProxy又经过几次重载函数的调用，最后的实现是123456789101112131415161718192021222324252627282930313233/** * Get a protocol proxy that contains a proxy connection to a remote server * and a set of methods that are supported by the server * @param protocol protocol RPC协议 * @param clientVersion client's version * @param addr server address 服务端的地址 ip:port * @param ticket security ticket * @param conf configuration * @param factory socket factory * @param rpcTimeout max time for each rpc; 0 means no timeout * @param connectionRetryPolicy retry policy * @param fallbackToSimpleAuth set to true or false during calls to indicate if * a secure client falls back to simple auth * @return the proxy * @throws IOException if any error occurs */ public static &lt;T&gt; ProtocolProxy&lt;T&gt; getProtocolProxy(Class&lt;T&gt; protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth) throws IOException &#123; if (UserGroupInformation.isSecurityEnabled()) &#123; SaslRpcServer.init(conf); &#125; return getProtocolEngine(protocol, conf).getProxy(protocol, clientVersion, addr, ticket, conf, factory, rpcTimeout, connectionRetryPolicy, fallbackToSimpleAuth);&#125; 其中ProtocolProxy封装了动态代理类和PRC协议，ProtocolProxy.getProxy会返回生成的动态代理类。而ProtocolProxy类是getProtocolEngine返回的。12345678910111213141516/** * 如果缓存中存在该协议的RpcEngine，则直接调用 * 否则就从Configuration中取出rpc.engine.protocol.getName()的RpeEngine类 * 默认为WritableRpcEngine类 */static synchronized RpcEngine getProtocolEngine(Class&lt;?&gt; protocol, Configuration conf) &#123; RpcEngine engine = PROTOCOL_ENGINES.get(protocol); if (engine == null) &#123; Class&lt;?&gt; impl = conf.getClass(ENGINE_PROP+"."+protocol.getName(), WritableRpcEngine.class); engine = (RpcEngine)ReflectionUtils.newInstance(impl, conf); PROTOCOL_ENGINES.put(protocol, engine); &#125; return engine;&#125; 可以看到RpcEngine从Configuration读取，如果没有，默认设置是WritableRpcEngine。这是Hadoop RPC对序列化方式多样性的支持，目前提供了Writable(WritableRpcEngine)和Protocol Buffer(ProtocolRpcEngine)两种，用户也可以通过RPC.setProtocolEngine()设置。这里我们只分析WritableRpcEngine，跳转到WritableRpcEngine.getProxy方法123456789101112131415public &lt;T&gt; ProtocolProxy&lt;T&gt; getProxy(Class&lt;T&gt; protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth) throws IOException &#123; //默认为null if (connectionRetryPolicy != null) &#123; throw new UnsupportedOperationException( "Not supported: connectionRetryPolicy=" + connectionRetryPolicy); &#125; T proxy = (T) Proxy.newProxyInstance(protocol.getClassLoader(), new Class[] &#123; protocol &#125;, new Invoker(protocol, addr, ticket, conf, factory, rpcTimeout, fallbackToSimpleAuth)); return new ProtocolProxy&lt;T&gt;(protocol, proxy, true);&#125; 可以看到这里使用了动态代理生成的了代理类，封装进了ProtocolProxy类中，而RPC.getProxy中是返回了ProtocolProxy.getProxy的结果。则调用PRC协议的方法会触发代理类的invoke方法，客户端就是在invoke方法中实现了方法名和参数的传递和结果的接受。因此我们继续看Invoker类，这是WritableRpcEngine的内部类123456789101112131415161718192021222324252627282930313233343536373839404142434445private static class Invoker implements RpcInvocationHandler &#123; //ConnectionId中包括了服务端的地址，协议类，安全令牌等，用来唯一标识Client.Connection类 private Client.ConnectionId remoteId; private Client client; private boolean isClosed = false; private final AtomicBoolean fallbackToSimpleAuth; public Invoker(Class&lt;?&gt; protocol, InetSocketAddress address, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, AtomicBoolean fallbackToSimpleAuth) throws IOException &#123; this.remoteId = Client.ConnectionId.getConnectionId(address, protocol, ticket, rpcTimeout, conf); this.client = CLIENTS.getClient(conf, factory); this.fallbackToSimpleAuth = fallbackToSimpleAuth; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; long startTime = 0; if (LOG.isDebugEnabled()) &#123; startTime = Time.now(); &#125; TraceScope traceScope = null; if (Trace.isTracing()) &#123; traceScope = Trace.startSpan(RpcClientUtil.methodToTraceString(method)); &#125; ObjectWritable value; try &#123; value = (ObjectWritable) client.call(RPC.RpcKind.RPC_WRITABLE, new Invocation(method, args), remoteId, fallbackToSimpleAuth); &#125; finally &#123; if (traceScope != null) traceScope.close(); &#125; if (LOG.isDebugEnabled()) &#123; long callTime = Time.now() - startTime; LOG.debug("Call: " + method.getName() + " " + callTime); &#125; return value.get(); &#125; ...&#125; 可以看到invoke中是调用了Client.call方法来进行远程函数调用，Client的获得是先从CLIENTS中的缓存（其实就是内部维护了一个HashMap&lt;SocketFactory, Client&gt;）,如果没有就根据SocketFactory和序列化类型实例化一个并放入其中。call方法的一个参数new Invocation(method, args),其实是用来序列化方法类及其传递的参数的。Invocation类实现了Writable接口，Writable是hadoop用来序列化的接口。12345678910111213141516171819202122232425262728293031323334353637383940private static class Invocation implements Writable, Configurable &#123; private String methodName; //方法名 private Class&lt;?&gt;[] parameterClasses; //参数类型 private Object[] parameters; //参数实例对象 private Configuration conf; private long clientVersion; //客户端version,主要从方法所在类的ProtocolInfo注解中的version或者versionID中获得 private int clientMethodsHash; //方法所在类所有方法的形成的hash private String declaringClassProtocolName; //要从方法所在类的ProtocolInfo注解中的name ... public void readFields(DataInput in) throws IOException &#123; rpcVersion = in.readLong(); declaringClassProtocolName = UTF8.readString(in); methodName = UTF8.readString(in); clientVersion = in.readLong(); clientMethodsHash = in.readInt(); parameters = new Object[in.readInt()]; parameterClasses = new Class[parameters.length]; ObjectWritable objectWritable = new ObjectWritable(); for (int i = 0; i &lt; parameters.length; i++) &#123; parameters[i] = ObjectWritable.readObject(in, objectWritable, this.conf); parameterClasses[i] = objectWritable.getDeclaredClass(); &#125; &#125; @Override @SuppressWarnings("deprecation") public void write(DataOutput out) throws IOException &#123; out.writeLong(rpcVersion); UTF8.writeString(out, declaringClassProtocolName); UTF8.writeString(out, methodName); out.writeLong(clientVersion); out.writeInt(clientMethodsHash); out.writeInt(parameterClasses.length); for (int i = 0; i &lt; parameterClasses.length; i++) &#123; ObjectWritable.writeObject(out, parameters[i], parameterClasses[i], conf, true); &#125; &#125; ...&#125; 继续我们追踪Client.call，来了的Client类中（客户端核心的类），通过重载函数的调用，最后定位到了1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Make a call, passing &lt;code&gt;rpcRequest&lt;/code&gt;, to the IPC server defined by * &lt;code&gt;remoteId&lt;/code&gt;, returning the rpc response. * @param rpcKind * @param rpcRequest - contains serialized method and method parameters 包含序列化的方法和方法参数 * @param remoteId - the target rpc server * @param serviceClass - service class for RPC * @param fallbackToSimpleAuth - set to true or false during this method to * indicate if a secure client falls back to simple auth * @returns the rpc response */ public Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass, AtomicBoolean fallbackToSimpleAuth) throws IOException &#123; final Call call = createCall(rpcKind, rpcRequest); //将序列化的信息封装进Call中 Connection connection = getConnection(remoteId, call, serviceClass, fallbackToSimpleAuth); //获得Connection对象，其中封装了socket，连接到服务端 try &#123; connection.sendRpcRequest(call); // send the rpc &#125; catch (RejectedExecutionException e) &#123; throw new IOException("connection has been closed", e); &#125; catch (InterruptedException e) &#123; Thread.currentThread().interrupt(); LOG.warn("interrupted waiting to send rpc request to server", e); throw new IOException(e); &#125; synchronized (call) &#123; while (!call.done) &#123; try &#123; call.wait(); // wait for the result &#125; catch (InterruptedException ie) &#123; Thread.currentThread().interrupt(); throw new InterruptedIOException("Call interrupted"); &#125; &#125; if (call.error != null) &#123; if (call.error instanceof RemoteException) &#123; call.error.fillInStackTrace(); throw call.error; &#125; else &#123; // local exception InetSocketAddress address = connection.getRemoteAddress(); throw NetUtils.wrapException(address.getHostName(), address.getPort(), NetUtils.getHostname(), 0, call.error); &#125; &#125; else &#123; return call.getRpcResponse(); &#125; &#125; &#125; 从中可以看出首先是实例化一个Call对象，封装了输送的内容,Connection负责连接服务端，接受返回信息，放入对应的Call中，并唤醒Call,读出返回数据。其中一个Connection负责同个服务端地址，同个RPC协议，同个安全令牌的连接下的所有Call中内容的发送和接受，connection维护了Call的集合，通过callid知道对应返回的数据和发送的数据属于哪个Call对象，从后文可以看出。这里主要调用了createCall创造了Call对象，getConnection连接服务端，connection.sendRpcRequest发送请求并接受返回。接下来就深入这三个函数。1234567891011121314151617181920212223242526272829303132333435 Call createCall(RPC.RpcKind rpcKind, Writable rpcRequest) &#123; return new Call(rpcKind, rpcRequest); &#125; static class Call &#123; final int id; // call id final int retry; // retry count final Writable rpcRequest; // the serialized rpc request Writable rpcResponse; // null if rpc has error IOException error; // exception, null if success final RPC.RpcKind rpcKind; // Rpc EngineKind boolean done; // true when call is done private Call(RPC.RpcKind rpcKind, Writable param) &#123; this.rpcKind = rpcKind; this.rpcRequest = param; //生成callId,每一个Call实例对象都有唯一的id,为了connection接受消息后放入对应id的Call中 final Integer id = callId.get(); if (id == null) &#123; this.id = nextCallId(); &#125; else &#123; callId.set(null); this.id = id; &#125; final Integer rc = retryCount.get(); if (rc == null) &#123; this.retry = 0; &#125; else &#123; this.retry = rc; &#125; &#125; ...&#125; 12345678910111213141516171819202122232425262728293031/** Get a connection from the pool, or create a new one and add it to the * pool. Connections to a given ConnectionId are reused. */private Connection getConnection(ConnectionId remoteId, Call call, int serviceClass, AtomicBoolean fallbackToSimpleAuth) throws IOException &#123; if (!running.get()) &#123; // the client is stopped throw new IOException("The client is stopped"); &#125; Connection connection; /* we could avoid this allocation for each RPC by having a * connectionsId object and with set() method. We need to manage the * refs for keys in HashMap properly. For now its ok. */ do &#123; synchronized (connections) &#123; connection = connections.get(remoteId); if (connection == null) &#123; connection = new Connection(remoteId, serviceClass); connections.put(remoteId, connection); &#125; &#125; &#125; while (!connection.addCall(call)); //we don't invoke the method below inside "synchronized (connections)" //block above. The reason for that is if the server happens to be slow, //it will take longer to establish a connection and that will slow the //entire system down. connection.setupIOstreams(fallbackToSimpleAuth); return connection;&#125; 其中维护了一个连接池（其实就是一个HashTable，保证线程安全），remoteId作为其key，当连接池没有连接则会新建一个Connection并将其添加到连接池中，初始化Connection就是一些赋值，可以在Connection中看到Socket，实际上是调用了Socket建立了TCP连接，并且Connection继承了Thread，在建立连接后会启动线程，不断等待结果的相应，在下文中可以看到。在”synchronized (connections)”代码块中是不会开始建立连接的，因为如果在同步代码块中连接会造成阻塞，降低整个系统的效率。真正的建立连接是connection.setupIOstreams中会调用的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/** Connect to the server and set up the I/O streams. It then sends * a header to the server and starts * the connection thread that waits for responses. * 建立channle的in和outStream,启动connection的线程接受返回消息 */private synchronized void setupIOstreams( AtomicBoolean fallbackToSimpleAuth) &#123; if (socket != null || shouldCloseConnection.get()) &#123; return; &#125; try &#123; if (LOG.isDebugEnabled()) &#123; LOG.debug("Connecting to "+server); &#125; if (Trace.isTracing()) &#123; Trace.addTimelineAnnotation("IPC client connecting to " + server); &#125; short numRetries = 0; Random rand = null; while (true) &#123; setupConnection(); //建立socket连接 InputStream inStream = NetUtils.getInputStream(socket); //获得输入流 OutputStream outStream = NetUtils.getOutputStream(socket); //获得输出流 writeConnectionHeader(outStream); //传输连接头 ... if (doPing) &#123; inStream = new PingInputStream(inStream); &#125; this.in = new DataInputStream(new BufferedInputStream(inStream)); // SASL may have already buffered the stream if (!(outStream instanceof BufferedOutputStream)) &#123; outStream = new BufferedOutputStream(outStream); &#125; this.out = new DataOutputStream(outStream); writeConnectionContext(remoteId, authMethod); //传输上下文 // update last activity time touch(); if (Trace.isTracing()) &#123; Trace.addTimelineAnnotation("IPC client connected to " + server); &#125; // start the receiver thread after the socket connection has been set // up start(); return; &#125; &#125; catch (Throwable t) &#123; if (t instanceof IOException) &#123; markClosed((IOException)t); &#125; else &#123; markClosed(new IOException("Couldn't set up IO streams", t)); &#125; close(); &#125;&#125;/** * Write the connection header - this is sent when connection is established * +----------------------------------+ * | "hrpc" 4 bytes | * +----------------------------------+ * | Version (1 byte) | * +----------------------------------+ * | Service Class (1 byte) | * +----------------------------------+ * | AuthProtocol (1 byte) | * +----------------------------------+ */private void writeConnectionHeader(OutputStream outStream) throws IOException &#123; DataOutputStream out = new DataOutputStream(new BufferedOutputStream(outStream)); // Write out the header, version and authentication method out.write(RpcConstants.HEADER.array()); out.write(RpcConstants.CURRENT_VERSION); out.write(serviceClass); out.write(authProtocol.callId); out.flush();&#125; 这里的整个过程就是setupConnection方法建立连接，得到输入输出流，再传输Hadoop PRC的连接头和上下文（Configuration中设置的一些RPC的参数）。连接头从注释中可以看到，下文可以看到server端接受解析连接头。最后启动线程，不断等待接收相应结果。主要连接方法是setupConnection方法。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253private synchronized void setupConnection() throws IOException &#123; short ioFailures = 0; short timeoutFailures = 0; while (true) &#123; try &#123; this.socket = socketFactory.createSocket(); this.socket.setTcpNoDelay(tcpNoDelay); this.socket.setKeepAlive(true); /* * Bind the socket to the host specified in the principal name of the * client, to ensure Server matching address of the client connection * to host name in principal passed. */ UserGroupInformation ticket = remoteId.getTicket(); if (ticket != null &amp;&amp; ticket.hasKerberosCredentials()) &#123; KerberosInfo krbInfo = remoteId.getProtocol().getAnnotation(KerberosInfo.class); if (krbInfo != null &amp;&amp; krbInfo.clientPrincipal() != null) &#123; String host = SecurityUtil.getHostFromPrincipal(remoteId.getTicket().getUserName()); // If host name is a valid local address then bind socket to it InetAddress localAddr = NetUtils.getLocalInetAddress(host); if (localAddr != null) &#123; this.socket.bind(new InetSocketAddress(localAddr, 0)); &#125; &#125; &#125; NetUtils.connect(this.socket, server, connectionTimeout); if (rpcTimeout &gt; 0) &#123; pingInterval = rpcTimeout; // rpcTimeout overwrites pingInterval &#125; this.socket.setSoTimeout(pingInterval); return; &#125; catch (ConnectTimeoutException toe) &#123; /* Check for an address change and update the local reference. * Reset the failure counter if the address was changed */ if (updateAddress()) &#123; timeoutFailures = ioFailures = 0; &#125; handleConnectionTimeout(timeoutFailures++, maxRetriesOnSocketTimeouts, toe); &#125; catch (IOException ie) &#123; if (updateAddress()) &#123; timeoutFailures = ioFailures = 0; &#125; handleConnectionFailure(ioFailures++, ie); &#125; &#125;&#125; 12345public class StandardSocketFactory extends SocketFactory &#123; public Socket createSocket() throws IOException &#123; return SocketChannel.open().socket(); &#125;&#125; setupConnection中调用了socketFactory.createSocket()根据具体不同实现大SocketFactory来创建Socket，默认是StandardSocketFactory实现，这里通过SocketChannel来得到socket,socket在setupConnection中设置，建立长连接和超时时间（默认为一分钟，socket.setSoTimeout设置）。最后通过NetUtils.connect建立连接。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103public static void connect(Socket socket, SocketAddress address, int timeout) throws IOException &#123; connect(socket, address, null, timeout); &#125; public static void connect(Socket socket, SocketAddress endpoint, SocketAddress localAddr, int timeout) throws IOException &#123; if (socket == null || endpoint == null || timeout &lt; 0) &#123; throw new IllegalArgumentException("Illegal argument for connect()"); &#125; SocketChannel ch = socket.getChannel(); if (localAddr != null) &#123; Class localClass = localAddr.getClass(); Class remoteClass = endpoint.getClass(); Preconditions.checkArgument(localClass.equals(remoteClass), "Local address %s must be of same family as remote address %s.", localAddr, endpoint); socket.bind(localAddr); &#125; try &#123; if (ch == null) &#123; // let the default implementation handle it. socket.connect(endpoint, timeout); &#125; else &#123; SocketIOWithTimeout.connect(ch, endpoint, timeout); &#125; &#125; catch (SocketTimeoutException ste) &#123; throw new ConnectTimeoutException(ste.getMessage()); &#125; // There is a very rare case allowed by the TCP specification, such that // if we are trying to connect to an endpoint on the local machine, // and we end up choosing an ephemeral port equal to the destination port, // we will actually end up getting connected to ourself (ie any data we // send just comes right back). This is only possible if the target // daemon is down, so we'll treat it like connection refused. if (socket.getLocalPort() == socket.getPort() &amp;&amp; socket.getLocalAddress().equals(socket.getInetAddress())) &#123; LOG.info("Detected a loopback TCP socket, disconnecting it"); socket.close(); throw new ConnectException( "Localhost targeted connection resulted in a loopback. " + "No daemon is listening on the target port."); &#125; &#125; /** * SocketIOWithTimeout.connect方法 */ static void connect(SocketChannel channel, SocketAddress endpoint, int timeout) throws IOException &#123; boolean blockingOn = channel.isBlocking(); if (blockingOn) &#123; channel.configureBlocking(false); &#125; try &#123; if (channel.connect(endpoint)) &#123; return; &#125; long timeoutLeft = timeout; long endTime = (timeout &gt; 0) ? (Time.now() + timeout): 0; while (true) &#123; // we might have to call finishConnect() more than once // for some channels (with user level protocols) int ret = selector.select((SelectableChannel)channel, SelectionKey.OP_CONNECT, timeoutLeft); if (ret &gt; 0 &amp;&amp; channel.finishConnect()) &#123; return; &#125; if (ret == 0 || (timeout &gt; 0 &amp;&amp; (timeoutLeft = (endTime - Time.now())) &lt;= 0)) &#123; throw new SocketTimeoutException( timeoutExceptionString(channel, timeout, SelectionKey.OP_CONNECT)); &#125; &#125; &#125; catch (IOException e) &#123; // javadoc for SocketChannel.connect() says channel should be closed. try &#123; channel.close(); &#125; catch (IOException ignored) &#123;&#125; throw e; &#125; finally &#123; if (blockingOn &amp;&amp; channel.isOpen()) &#123; channel.configureBlocking(true); &#125; &#125; &#125; 可以看到实际上就是socket或socketChannel（设置为不阻塞）的连接。和服务端的连接建立连接之后就是发送Call中的内容到服务端，并接收其相应结果，返回到了Client中的call方法，通过Connection.sendRpcRequest来发送方法信息和实际参数，并在其中启动Connection对象的线程，等待接收服务端响应消息。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public void sendRpcRequest(final Call call) throws InterruptedException, IOException &#123; if (shouldCloseConnection.get()) &#123; return; &#125; // Serialize the call to be sent. This is done from the actual // caller thread, rather than the sendParamsExecutor thread, // so that if the serialization throws an error, it is reported // properly. This also parallelizes the serialization. // // Format of a call on the wire: // 0) Length of rest below (1 + 2) // 1) RpcRequestHeader - is serialized Delimited hence contains length // 2) RpcRequest // // Items '1' and '2' are prepared here. final DataOutputBuffer d = new DataOutputBuffer(); RpcRequestHeaderProto header = ProtoUtil.makeRpcRequestHeader( call.rpcKind, OperationProto.RPC_FINAL_PACKET, call.id, call.retry, clientId); header.writeDelimitedTo(d); call.rpcRequest.write(d); synchronized (sendRpcRequestLock) &#123; Future&lt;?&gt; senderFuture = sendParamsExecutor.submit(new Runnable() &#123; @Override public void run() &#123; try &#123; synchronized (Connection.this.out) &#123; if (shouldCloseConnection.get()) &#123; return; &#125; if (LOG.isDebugEnabled()) LOG.debug(getName() + " sending #" + call.id); byte[] data = d.getData(); int totalLength = d.getLength(); out.writeInt(totalLength); // Total Length out.write(data, 0, totalLength);// RpcRequestHeader + RpcRequest out.flush(); &#125; &#125; catch (IOException e) &#123; // exception at this point would leave the connection in an // unrecoverable state (eg half a call left on the wire). // So, close the connection, killing any outstanding calls markClosed(e); &#125; finally &#123; //the buffer is just an in-memory buffer, but it is still polite to // close early IOUtils.closeStream(d); &#125; &#125; &#125;); try &#123; senderFuture.get(); &#125; catch (ExecutionException e) &#123; Throwable cause = e.getCause(); // cause should only be a RuntimeException as the Runnable above // catches IOException if (cause instanceof RuntimeException) &#123; throw (RuntimeException) cause; &#125; else &#123; throw new RuntimeException("unexpected checked exception", cause); &#125; &#125; &#125;&#125; 将call中的消息写入到输出流中，写入的格式可以从注释中看到，首先是报文的长度，接着是报文头，最后是发送的请求内容。其中先将消息写入到临时的DataOutputBuffer，最后将其放入线程池中发送，避免了阻塞。前文讲到我们在建立连接之后启动了Connection的线程，不断从服务端接受响应消息。接下来我们看如何接受服务端返回的函数结果，即Connection的run方法。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public void run() &#123; if (LOG.isDebugEnabled()) LOG.debug(getName() + ": starting, having connections " + connections.size()); try &#123; while (waitForWork()) &#123;//wait here for work - read or close connection receiveRpcResponse(); &#125; &#125; catch (Throwable t) &#123; ...&#125;/* Receive a response. * Because only one receiver, so no synchronization on in. */private void receiveRpcResponse() &#123; if (shouldCloseConnection.get()) &#123; return; &#125; touch(); try &#123; int totalLen = in.readInt(); RpcResponseHeaderProto header = RpcResponseHeaderProto.parseDelimitedFrom(in); checkResponse(header); int headerLen = header.getSerializedSize(); headerLen += CodedOutputStream.computeRawVarint32Size(headerLen); int callId = header.getCallId(); if (LOG.isDebugEnabled()) LOG.debug(getName() + " got value #" + callId); Call call = calls.get(callId); RpcStatusProto status = header.getStatus(); if (status == RpcStatusProto.SUCCESS) &#123; Writable value = ReflectionUtils.newInstance(valueClass, conf); value.readFields(in); // read value calls.remove(callId); call.setRpcResponse(value); // verify that length was correct // only for ProtobufEngine where len can be verified easily if (call.getRpcResponse() instanceof ProtobufRpcEngine.RpcWrapper) &#123; ProtobufRpcEngine.RpcWrapper resWrapper = (ProtobufRpcEngine.RpcWrapper) call.getRpcResponse(); if (totalLen != headerLen + resWrapper.getLength()) &#123; throw new RpcClientException( "RPC response length mismatch on rpc success"); &#125; &#125; &#125; else &#123; // Rpc Request failed // Verify that length was correct if (totalLen != headerLen) &#123; throw new RpcClientException( "RPC response length mismatch on rpc error"); &#125; final String exceptionClassName = header.hasExceptionClassName() ? header.getExceptionClassName() : "ServerDidNotSetExceptionClassName"; final String errorMsg = header.hasErrorMsg() ? header.getErrorMsg() : "ServerDidNotSetErrorMsg" ; final RpcErrorCodeProto erCode = (header.hasErrorDetail() ? header.getErrorDetail() : null); if (erCode == null) &#123; LOG.warn("Detailed error code not set by server on rpc error"); &#125; RemoteException re = ( (erCode == null) ? new RemoteException(exceptionClassName, errorMsg) : new RemoteException(exceptionClassName, errorMsg, erCode)); if (status == RpcStatusProto.ERROR) &#123; calls.remove(callId); call.setException(re); &#125; else if (status == RpcStatusProto.FATAL) &#123; // Close the connection markClosed(re); &#125; &#125; &#125; catch (IOException e) &#123; markClosed(e); &#125;&#125; 可以看到首先是获得消息的长度，再是解析消息头，根据callId将发消息内容放入对应的Call中，将call.done赋值为true，并唤醒对应的Call。最后回到Client.call方法，返回call.getRpcResponse();到此hadoop 客户端的代码分析完毕。 服务端的实现服务端首先是使用build模式创建一个RPC.Server对象 12345678910111213141516public Server build() throws IOException, HadoopIllegalArgumentException &#123; if (this.conf == null) &#123; throw new HadoopIllegalArgumentException("conf is not set"); &#125; if (this.protocol == null) &#123; throw new HadoopIllegalArgumentException("protocol is not set"); &#125; if (this.instance == null) &#123; throw new HadoopIllegalArgumentException("instance is not set"); &#125; return getProtocolEngine(this.protocol, this.conf).getServer( this.protocol, this.instance, this.bindAddress, this.port, this.numHandlers, this.numReaders, this.queueSizePerHandler, this.verbose, this.conf, this.secretManager, this.portRangeConfig);&#125; 可以看到和客户端一样是通过getProtocolEngine得到不同的ProtocolEngine类来生成不同的Server对象，同样我们来看默认的ProtocolEngine类WritableRpcEngine。调用了WritableRpcEngine的getServer的方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103public RPC.Server getServer(Class&lt;?&gt; protocolClass, Object protocolImpl, String bindAddress, int port, int numHandlers, int numReaders, int queueSizePerHandler, boolean verbose, Configuration conf, SecretManager&lt;? extends TokenIdentifier&gt; secretManager, String portRangeConfig) throws IOException &#123;return new Server(protocolClass, protocolImpl, conf, bindAddress, port, numHandlers, numReaders, queueSizePerHandler, verbose, secretManager, portRangeConfig);&#125;public static class Server extends RPC.Server &#123; ... /** * Construct an RPC server. * @param protocolClass - the protocol being registered * can be null for compatibility with old usage (see below for details) * @param protocolImpl the protocol impl that will be called * @param conf the configuration to use * @param bindAddress the address to bind on to listen for connection * @param port the port to listen for connections on * @param numHandlers the number of method handler threads to run * @param verbose whether each call should be logged */ public Server(Class&lt;?&gt; protocolClass, Object protocolImpl, Configuration conf, String bindAddress, int port, int numHandlers, int numReaders, int queueSizePerHandler, boolean verbose, SecretManager&lt;? extends TokenIdentifier&gt; secretManager, String portRangeConfig) throws IOException &#123; super(bindAddress, port, null, numHandlers, numReaders, queueSizePerHandler, conf, classNameBase(protocolImpl.getClass().getName()), secretManager, portRangeConfig); this.verbose = verbose; Class&lt;?&gt;[] protocols; if (protocolClass == null) &#123; // derive protocol from impl /* * In order to remain compatible with the old usage where a single * target protocolImpl is suppled for all protocol interfaces, and * the protocolImpl is derived from the protocolClass(es) * we register all interfaces extended by the protocolImpl */ protocols = RPC.getProtocolInterfaces(protocolImpl.getClass()); &#125; else &#123; if (!protocolClass.isAssignableFrom(protocolImpl.getClass())) &#123; throw new IOException("protocolClass "+ protocolClass + " is not implemented by protocolImpl which is of class " + protocolImpl.getClass()); &#125; // register protocol class and its super interfaces registerProtocolAndImpl(RPC.RpcKind.RPC_WRITABLE, protocolClass, protocolImpl); protocols = RPC.getProtocolInterfaces(protocolClass); &#125; for (Class&lt;?&gt; p : protocols) &#123; if (!p.equals(VersionedProtocol.class)) &#123; registerProtocolAndImpl(RPC.RpcKind.RPC_WRITABLE, p, protocolImpl); &#125; &#125; &#125;&#125; // Register protocol and its impl for rpc calls void registerProtocolAndImpl(RpcKind rpcKind, Class&lt;?&gt; protocolClass, Object protocolImpl) &#123; String protocolName = RPC.getProtocolName(protocolClass); long version; try &#123; version = RPC.getProtocolVersion(protocolClass); &#125; catch (Exception ex) &#123; LOG.warn("Protocol " + protocolClass + " NOT registered as cannot get protocol version "); return; &#125; getProtocolImplMap(rpcKind).put(new ProtoNameVer(protocolName, version), new ProtoClassProtoImpl(protocolClass, protocolImpl)); LOG.debug("RpcKind = " + rpcKind + " Protocol Name = " + protocolName + " version=" + version + " ProtocolImpl=" + protocolImpl.getClass().getName() + " protocolClass=" + protocolClass.getName()); &#125; Map&lt;ProtoNameVer, ProtoClassProtoImpl&gt; getProtocolImplMap(RPC.RpcKind rpcKind) &#123; if (protocolImplMapArray.size() == 0) &#123;// initialize for all rpc kinds for (int i=0; i &lt;= RpcKind.MAX_INDEX; ++i) &#123; protocolImplMapArray.add( new HashMap&lt;ProtoNameVer, ProtoClassProtoImpl&gt;(10)); &#125; &#125; return protocolImplMapArray.get(rpcKind.ordinal()); &#125; WritableRpcEngine是继承自RPC.Server，而RPC.Server是继承ipc的Server的，getServer直接初始化了WritableRpcEngine.Server对象，可以看到在初始化的过程中，首先是调用了父类的构造函数，接着将协议的具体实现类根据RPC_WRITABLE和协议接口名放入到缓存中。接着我们来看父类的构造函数。 12345678910111213141516171819202122232425public abstract static class Server extends org.apache.hadoop.ipc.Server &#123; ... protected Server(String bindAddress, int port, Class&lt;? extends Writable&gt; paramClass, int handlerCount, int numReaders, int queueSizePerHandler, Configuration conf, String serverName, SecretManager&lt;? extends TokenIdentifier&gt; secretManager, String portRangeConfig) throws IOException &#123; super(bindAddress, port, paramClass, handlerCount, numReaders, queueSizePerHandler, conf, serverName, secretManager, portRangeConfig); initProtocolMetaInfo(conf); &#125; private void initProtocolMetaInfo(Configuration conf) &#123; RPC.setProtocolEngine(conf, ProtocolMetaInfoPB.class, ProtobufRpcEngine.class); ProtocolMetaInfoServerSideTranslatorPB xlator = new ProtocolMetaInfoServerSideTranslatorPB(this); BlockingService protocolInfoBlockingService = ProtocolInfoService .newReflectiveBlockingService(xlator); addProtocol(RpcKind.RPC_PROTOCOL_BUFFER, ProtocolMetaInfoPB.class, protocolInfoBlockingService); &#125; ...&#125; 123456789101112131415161718192021222324252627282930313233public abstract class Server &#123; protected Server(String bindAddress, int port, Class&lt;? extends Writable&gt; rpcRequestClass, int handlerCount, int numReaders, int queueSizePerHandler, Configuration conf, String serverName, SecretManager&lt;? extends TokenIdentifier&gt; secretManager, String portRangeConfig) throws IOException &#123; this.bindAddress = bindAddress; this.conf = conf; this.portRangeConfig = portRangeConfig; this.port = port; this.rpcRequestClass = rpcRequestClass; this.handlerCount = handlerCount; this.socketSendBufferSize = 0; ... listener = new Listener(); this.port = listener.getAddress().getPort(); connectionManager = new ConnectionManager(); this.rpcMetrics = RpcMetrics.create(this, conf); this.rpcDetailedMetrics = RpcDetailedMetrics.create(this.port); this.tcpNoDelay = conf.getBoolean( CommonConfigurationKeysPublic.IPC_SERVER_TCPNODELAY_KEY, CommonConfigurationKeysPublic.IPC_SERVER_TCPNODELAY_DEFAULT); this.setLogSlowRPC(conf.getBoolean( CommonConfigurationKeysPublic.IPC_SERVER_LOG_SLOW_RPC, CommonConfigurationKeysPublic.IPC_SERVER_LOG_SLOW_RPC_DEFAULT)); // Create the responder here responder = new Responder(); ... &#125;&#125; 可以看到父类的初始化主要是对一些变量的赋值，最主要的是初始化了listener和responder对象。 123456789101112131415161718192021222324252627282930313233343536373839/** Listens on the socket. Creates jobs for the handler threads*/ private class Listener extends Thread &#123; private ServerSocketChannel acceptChannel = null; //the accept channel private Selector selector = null; //the selector that we use for the server private Reader[] readers = null; private int currentReader = 0; private InetSocketAddress address; //the address we bind at private int backlogLength = conf.getInt( CommonConfigurationKeysPublic.IPC_SERVER_LISTEN_QUEUE_SIZE_KEY, CommonConfigurationKeysPublic.IPC_SERVER_LISTEN_QUEUE_SIZE_DEFAULT); public Listener() throws IOException &#123; address = new InetSocketAddress(bindAddress, port); // Create a new server socket and set to non blocking mode acceptChannel = ServerSocketChannel.open(); acceptChannel.configureBlocking(false); // Bind the server socket to the local host and port bind(acceptChannel.socket(), address, backlogLength, conf, portRangeConfig); port = acceptChannel.socket().getLocalPort(); //Could be an ephemeral port // create a selector; selector= Selector.open(); //如果没有指定值，readThreads默认为1 readers = new Reader[readThreads]; for (int i = 0; i &lt; readThreads; i++) &#123; Reader reader = new Reader( "Socket Reader #" + (i + 1) + " for port " + port); readers[i] = reader; reader.start(); &#125; // Register accepts on the server socket with the selector. acceptChannel.register(selector, SelectionKey.OP_ACCEPT); this.setName("IPC Server listener on " + port); this.setDaemon(true); &#125; ... &#125; Listener初始化了ServerSocketChannel，绑定了指定的一个或者一串中的一个端口，初始化了Selector,向其中注册了”连接就绪“的事件。并定义启动了几个读线程（Reader类）。Reader类主要用于读取客户端发送的信息，范序列化，生成相应的Call对象。每个Reader类中都有一个Selector，用于监听“读就绪”事件。在Listener的选择器中收到连接就绪的事件就会socketChannel封装进Connection中，将读事件注册给Reader的选择器，有相应的reader来负责读取信息。（下文会详细看到）现在先大致看一下responder类。 12345678910111213141516171819202122232425262728293031 // Sends responses of RPC back to clients.private class Responder extends Thread &#123; private final Selector writeSelector; private int pending; // connections waiting to register final static int PURGE_INTERVAL = 900000; // 15mins Responder() throws IOException &#123; this.setName("IPC Server Responder"); this.setDaemon(true); writeSelector = Selector.open(); // create a selector pending = 0; &#125; @Override public void run() &#123; LOG.info(Thread.currentThread().getName() + ": starting"); SERVER.set(Server.this); try &#123; doRunLoop(); &#125; finally &#123; LOG.info("Stopping " + Thread.currentThread().getName()); try &#123; writeSelector.close(); &#125; catch (IOException ioe) &#123; LOG.error("Couldn't close write selector in " + Thread.currentThread().getName(), ioe); &#125; &#125; &#125; ... &#125; 可以看到responder也是一个线程类，这个线程类主要是将RPC的相应信息传给客户端，可以看到其内部也有一个selector对象，主要是一些写事件会注册在其中。在初始化Listener和Responder对象，生成Server实例之后，就需要调用Server.start()来启动服务端。 12345678910public synchronized void start() &#123; responder.start(); listener.start(); handlers = new Handler[handlerCount]; for (int i = 0; i &lt; handlerCount; i++) &#123; handlers[i] = new Handler(i); handlers[i].start(); &#125;&#125; 分别启动了listener、responder和handlers的线程。接下来看listener的run方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public void run() &#123; LOG.info(Thread.currentThread().getName() + ": starting"); SERVER.set(Server.this); connectionManager.startIdleScan(); while (running) &#123; SelectionKey key = null; try &#123; getSelector().select(); //阻塞直到注册的socket管道有事件触发为止 Iterator&lt;SelectionKey&gt; iter = getSelector().selectedKeys().iterator(); while (iter.hasNext()) &#123; key = iter.next(); iter.remove(); try &#123; if (key.isValid()) &#123; if (key.isAcceptable()) doAccept(key); &#125; &#125; catch (IOException e) &#123; &#125; key = null; &#125; &#125; catch (OutOfMemoryError e) &#123; // we can run out of memory if we have too many threads // log the event and sleep for a minute and give // some thread(s) a chance to finish LOG.warn("Out of Memory in server select", e); closeCurrentConnection(key, e); connectionManager.closeIdle(true); try &#123; Thread.sleep(60000); &#125; catch (Exception ie) &#123;&#125; &#125; catch (Exception e) &#123; closeCurrentConnection(key, e); &#125; &#125; LOG.info("Stopping " + Thread.currentThread().getName()); synchronized (this) &#123; try &#123; acceptChannel.close(); selector.close(); &#125; catch (IOException e) &#123; &#125; selector= null; acceptChannel= null; // close all connections connectionManager.stopIdleScan(); connectionManager.closeAll(); &#125;&#125;void doAccept(SelectionKey key) throws InterruptedException, IOException, OutOfMemoryError &#123; ServerSocketChannel server = (ServerSocketChannel) key.channel(); SocketChannel channel; while ((channel = server.accept()) != null) &#123; channel.configureBlocking(false); channel.socket().setTcpNoDelay(tcpNoDelay); channel.socket().setKeepAlive(true); Reader reader = getReader(); //数组递增，获取下一个reader Connection c = connectionManager.register(channel); //构造Connection对象并放入connectionManager管理，如果connectionManager的最大连接数将无法建立连接 // If the connectionManager can't take it, close the connection. if (c == null) &#123; if (channel.isOpen()) &#123; IOUtils.cleanup(null, channel); &#125; connectionManager.droppedConnections.getAndIncrement(); continue; &#125; //将其添加到SelectionKey中，当发生错误时可以从SelectionKey中获得Connection将其关闭 key.attach(c); // so closeCurrentConnection can get the object reader.addConnection(c); &#125;&#125; listener线程等待注册的连接事件触发，调用doAccept函数，doAccept中获得连接的socket,构造一个Connection对象放入connectionManager的缓存中（内部有一个Set connections变量），这个Connection类是ipc.Server的内部类，和上述Client中的内部类不同。接着在递增的取出一个reader将connection放入reader内部的阻塞队列中。 123456public void addConnection(Connection conn) throws InterruptedException &#123; //pendingConnections为阻塞队列 BlockingQueue&lt;Connection&gt; pendingConnections.put(conn); //唤醒readSelector readSelector.wakeup();&#125; 接着我们来看Reader的run方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public void run() &#123; LOG.info("Starting " + Thread.currentThread().getName()); try &#123; doRunLoop(); &#125; finally &#123; try &#123; readSelector.close(); &#125; catch (IOException ioe) &#123; LOG.error("Error closing read selector in " + Thread.currentThread().getName(), ioe); &#125; &#125;&#125;private synchronized void doRunLoop() &#123; while (running) &#123; SelectionKey key = null; try &#123; // consume as many connections as currently queued to avoid // unbridled acceptance of connections that starves the select int size = pendingConnections.size(); for (int i=size; i&gt;0; i--) &#123; Connection conn = pendingConnections.take(); conn.channel.register(readSelector, SelectionKey.OP_READ, conn); &#125; readSelector.select(); //不用经过判断，如果没有channel触发事件，阻塞直到有一个注册的事件就绪为止或者调用weakup() Iterator&lt;SelectionKey&gt; iter = readSelector.selectedKeys().iterator(); while (iter.hasNext()) &#123; key = iter.next(); iter.remove(); try &#123; if (key.isReadable()) &#123; doRead(key); &#125; &#125; catch (CancelledKeyException cke) &#123; // something else closed the connection, ex. responder or // the listener doing an idle scan. ignore it and let them // clean up. LOG.info(Thread.currentThread().getName() + ": connection aborted from " + key.attachment()); &#125; key = null; &#125; &#125; catch (InterruptedException e) &#123; if (running) &#123; // unexpected -- log it LOG.info(Thread.currentThread().getName() + " unexpectedly interrupted", e); &#125; &#125; catch (IOException ex) &#123; LOG.error("Error in Reader", ex); &#125; catch (Throwable re) &#123; LOG.fatal("Bug in read selector!", re); ExitUtil.terminate(1, "Bug in read selector!"); &#125; &#125;&#125; 从阻塞队列中取出connection，在readSelector注册读事件，选择器响应，调用doRead方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227void doRead(SelectionKey key) throws InterruptedException &#123; int count = 0; Connection c = (Connection)key.attachment(); if (c == null) &#123; return; &#125; c.setLastContact(Time.now()); try &#123; count = c.readAndProcess(); &#125; catch (InterruptedException ieo) &#123; ... &#125;&#125; public int readAndProcess() throws WrappedRpcServerException, IOException, InterruptedException &#123; while (true) &#123; /* Read at most one RPC. If the header is not read completely yet * then iterate until we read first RPC or until there is no data left. */ int count = -1; //dataLengthBuffer为四个字节，可能是接受报头，也可能是数据的长度 if (dataLengthBuffer.remaining() &gt; 0) &#123; count = channelRead(channel, dataLengthBuffer); if (count &lt; 0 || dataLengthBuffer.remaining() &gt; 0) return count; &#125; /** * 如果connectionHeaderRead是true, * 表示已经读过连接头，那么不需要读取头部数据，dataLengthBuffer存储的数据的长度。 * 如果connectionHeaderRead是false, * 表示dataLengthBuffer读取的是hrpc * +----------------------------------+ * | "hrpc" 4 bytes | * +----------------------------------+ * | Version (1 byte) | * +----------------------------------+ * | Service Class (1 byte) | * +----------------------------------+ * | AuthProtocol (1 byte) | * +----------------------------------+ */ if (!connectionHeaderRead) &#123; //Every connection is expected to send the header. //connectionHeaderBuf主要存放后三个字节 if (connectionHeaderBuf == null) &#123; connectionHeaderBuf = ByteBuffer.allocate(3); &#125; count = channelRead(channel, connectionHeaderBuf); if (count &lt; 0 || connectionHeaderBuf.remaining() &gt; 0) &#123; return count; &#125; int version = connectionHeaderBuf.get(0); // TODO we should add handler for service class later this.setServiceClass(connectionHeaderBuf.get(1)); dataLengthBuffer.flip(); // Check if it looks like the user is hitting an IPC port // with an HTTP GET - this is a common error, so we can // send back a simple string indicating as much. if (HTTP_GET_BYTES.equals(dataLengthBuffer)) &#123; setupHttpRequestOnIpcPortResponse(); return -1; &#125; //RpcConstants.HEADER为“hrpc” if (!RpcConstants.HEADER.equals(dataLengthBuffer) || version != CURRENT_VERSION) &#123; //Warning is ok since this is not supposed to happen. LOG.warn("Incorrect header or version mismatch from " + hostAddress + ":" + remotePort + " got version " + version + " expected version " + CURRENT_VERSION); setupBadVersionResponse(version); return -1; &#125; // this may switch us into SIMPLE authProtocol = initializeAuthContext(connectionHeaderBuf.get(2)); dataLengthBuffer.clear(); connectionHeaderBuf = null; connectionHeaderRead = true; continue; &#125; //分配空间为dataLength的长度 if (data == null) &#123; dataLengthBuffer.flip(); dataLength = dataLengthBuffer.getInt(); checkDataLength(dataLength); data = ByteBuffer.allocate(dataLength); &#125; //读取数据 count = channelRead(channel, data); //第一次是读取PRC的context，connectionContextRead为false,读取完context后为true if (data.remaining() == 0) &#123; dataLengthBuffer.clear(); data.flip(); boolean isHeaderRead = connectionContextRead; //如果是context,则读取context,否则读取其数据 processOneRpc(data.array()); data = null; if (!isHeaderRead) &#123; continue; &#125; &#125; return count; &#125;&#125;/** * Process an RPC Request - handle connection setup and decoding of * request into a Call * @param buf - contains the RPC request header and the rpc request * @throws IOException - internal error that should not be returned to * client, typically failure to respond to client * @throws WrappedRpcServerException - an exception to be sent back to * the client that does not require verbose logging by the * Listener thread * @throws InterruptedException */ private void processOneRpc(byte[] buf) throws IOException, WrappedRpcServerException, InterruptedException &#123; int callId = -1; int retry = RpcConstants.INVALID_RETRY_COUNT; try &#123; final DataInputStream dis = new DataInputStream(new ByteArrayInputStream(buf)); final RpcRequestHeaderProto header = decodeProtobufFromStream(RpcRequestHeaderProto.newBuilder(), dis); callId = header.getCallId(); retry = header.getRetryCount(); if (LOG.isDebugEnabled()) &#123; LOG.debug(" got #" + callId); &#125; checkRpcHeaders(header); //这里通过callId来判数据是context还是实际数据，如果是context,callId为CONNECTION_CONTEXT_CALL_ID = -3，否则callId &gt; 0 if (callId &lt; 0) &#123; // callIds typically used during connection setup //connectionContextRead在该方法中赋值为true processRpcOutOfBandRequest(header, dis); &#125; else if (!connectionContextRead) &#123; throw new WrappedRpcServerException( RpcErrorCodeProto.FATAL_INVALID_RPC_HEADER, "Connection context not established"); &#125; else &#123; //读取数据 processRpcRequest(header, dis); &#125; &#125; catch (WrappedRpcServerException wrse) &#123; // inform client of error Throwable ioe = wrse.getCause(); final Call call = new Call(callId, retry, null, this); setupResponse(authFailedResponse, call, RpcStatusProto.FATAL, wrse.getRpcErrorCodeProto(), null, ioe.getClass().getName(), ioe.getMessage()); call.sendResponse(); throw wrse; &#125;&#125;/** * Process an RPC Request - the connection headers and context must * have been already read * @param header - RPC request header * @param dis - stream to request payload * @throws WrappedRpcServerException - due to fatal rpc layer issues such * as invalid header or deserialization error. In this case a RPC fatal * status response will later be sent back to client. * @throws InterruptedException */private void processRpcRequest(RpcRequestHeaderProto header, DataInputStream dis) throws WrappedRpcServerException, InterruptedException &#123; Class&lt;? extends Writable&gt; rpcRequestClass = getRpcRequestWrapper(header.getRpcKind()); if (rpcRequestClass == null) &#123; LOG.warn("Unknown rpc kind " + header.getRpcKind() + " from client " + getHostAddress()); final String err = "Unknown rpc kind in rpc header" + header.getRpcKind(); throw new WrappedRpcServerException( RpcErrorCodeProto.FATAL_INVALID_RPC_HEADER, err); &#125; Writable rpcRequest; try &#123; //Read the rpc request //利用反射实例化对象，并读取信息，得到调用方法和参数的数据。如果是WritableRpcEngine，对应的是Invocation rpcRequest = ReflectionUtils.newInstance(rpcRequestClass, conf); rpcRequest.readFields(dis); &#125; catch (Throwable t) &#123; // includes runtime exception from newInstance LOG.warn("Unable to read call parameters for client " + getHostAddress() + "on connection protocol " + this.protocolName + " for rpcKind " + header.getRpcKind(), t); String err = "IPC server unable to read call parameters: "+ t.getMessage(); throw new WrappedRpcServerException( RpcErrorCodeProto.FATAL_DESERIALIZING_REQUEST, err); &#125; Span traceSpan = null; if (header.hasTraceInfo()) &#123; // If the incoming RPC included tracing info, always continue the trace TraceInfo parentSpan = new TraceInfo(header.getTraceInfo().getTraceId(), header.getTraceInfo().getParentId()); traceSpan = Trace.startSpan(rpcRequest.toString(), parentSpan).detach(); &#125; //将信息封装成Call对象，包括callId,调用对象信息的等，和Client的Call相对应 Call call = new Call(header.getCallId(), header.getRetryCount(), rpcRequest, this, ProtoUtil.convert(header.getRpcKind()), header.getClientId().toByteArray(), traceSpan); if (callQueue.isClientBackoffEnabled()) &#123; // if RPC queue is full, we will ask the RPC client to back off by // throwing RetriableException. Whether RPC client will honor // RetriableException and retry depends on client ipc retry policy. // For example, FailoverOnNetworkExceptionRetry handles // RetriableException. queueRequestOrAskClientToBackOff(call); &#125; else &#123; callQueue.put(call); // queue the call; maybe blocked here &#125; incRpcCount(); // Increment the rpc count&#125; 内部调用了Connection.readAndProcess方法，dataLengthBuffer可能读取到连接头，如果是连接头则dataLengthBuffer里的内容是“hrpc”,否则dataLengthBuffer读取的是数据的长度，这里有分context和实际的数据，这个都由processOneRpc处理，这里通过callId是否小于0来判断是否是context（context的callId为-3），如果是context，则会将connectionContextRead设置true，最后通过processRpcRequest方法读取实际数据。通过反射机制创建具体的Writable子类，通过ReadFile的发序列化得到内容，最后将CallId，Wriable等封装进Call中（和Client的Call相对应），放入callQueue这个队列值，而callQueue中的Call的处理是交给Handler处理。我们来看Handler的run方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384private class Handler extends Thread &#123; public Handler(int instanceNumber) &#123; this.setDaemon(true); this.setName("IPC Server handler "+ instanceNumber + " on " + port); &#125; @Override public void run() &#123; LOG.debug(Thread.currentThread().getName() + ": starting"); SERVER.set(Server.this); ByteArrayOutputStream buf = new ByteArrayOutputStream(INITIAL_RESP_BUF_SIZE); while (running) &#123; TraceScope traceScope = null; try &#123; //从阻塞队列中取出一个Call对象 final Call call = callQueue.take(); // pop the queue; maybe blocked here if (LOG.isDebugEnabled()) &#123; LOG.debug(Thread.currentThread().getName() + ": " + call + " for RpcKind " + call.rpcKind); &#125; if (!call.connection.channel.isOpen()) &#123; LOG.info(Thread.currentThread().getName() + ": skipped " + call); continue; &#125; String errorClass = null; String error = null; RpcStatusProto returnStatus = RpcStatusProto.SUCCESS; RpcErrorCodeProto detailedErr = null; Writable value = null; //放入ThreadLocal，以便获取当前处理的call等信息 CurCall.set(call); if (call.traceSpan != null) &#123; traceScope = Trace.continueSpan(call.traceSpan); &#125; try &#123; // Make the call as the user via Subject.doAs, thus associating // the call with the Subject //调用call调用对应的方法 if (call.connection.user == null) &#123; value = call(call.rpcKind, call.connection.protocolName, call.rpcRequest, call.timestamp); &#125; else &#123; value = call.connection.user.doAs (new PrivilegedExceptionAction&lt;Writable&gt;() &#123; @Override public Writable run() throws Exception &#123; // make the call return call(call.rpcKind, call.connection.protocolName, call.rpcRequest, call.timestamp); &#125; &#125; ); &#125; &#125; catch (Throwable e) &#123; ... &#125; CurCall.set(null); synchronized (call.connection.responseQueue) &#123; setupResponse(buf, call, returnStatus, detailedErr, value, errorClass, error); // Discard the large buf and reset it back to smaller size // to free up heap. if (buf.size() &gt; maxRespSize) &#123; LOG.warn("Large response size " + buf.size() + " for call " + call.toString()); buf = new ByteArrayOutputStream(INITIAL_RESP_BUF_SIZE); &#125; call.sendResponse(); &#125; &#125; catch (InterruptedException e) &#123; ... &#125; finally &#123; ... &#125; &#125; LOG.debug(Thread.currentThread().getName() + ": exiting"); &#125;&#125; 可以看到handler的run方法，从callQueue阻塞队列中取出Call对象，接着调用call方法来执行指定的方法，我们接着来看call方法。 123456789101112131415161718/** Called for each call. * call的接口方法 */public abstract Writable call(RPC.RpcKind rpcKind, String protocol, Writable param, long receiveTime) throws Exception;//具体实现@Overridepublic Writable call(RPC.RpcKind rpcKind, String protocol, Writable rpcRequest, long receiveTime) throws Exception &#123; return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest, receiveTime);&#125;public static RpcInvoker getRpcInvoker(RPC.RpcKind rpcKind) &#123; RpcKindMapValue val = rpcKindMap.get(rpcKind); return (val == null) ? null : val.rpcInvoker; &#125; 可以看到实际到最好是调用看RpcInvoker的具体实现类的call方法，而该类是从rpcKindMap中通过rpcKind取出的，那么这个RpcInvoker的具体实现类是什么时候放进去的，具体是什么。追踪rpcKindMap使用的地方可以看到。 12345678910111213141516171819202122232425public class WritableRpcEngine implements RpcEngine &#123; /** * Register the rpcRequest deserializer for WritableRpcEngine */ private static synchronized void initialize() &#123; org.apache.hadoop.ipc.Server.registerProtocolEngine(RPC.RpcKind.RPC_WRITABLE, Invocation.class, new Server.WritableRpcInvoker()); isInitialized = true; &#125;&#125;public static void registerProtocolEngine(RPC.RpcKind rpcKind, Class&lt;? extends Writable&gt; rpcRequestWrapperClass, RpcInvoker rpcInvoker) &#123; RpcKindMapValue old = rpcKindMap.put(rpcKind, new RpcKindMapValue(rpcRequestWrapperClass, rpcInvoker)); if (old != null) &#123; rpcKindMap.put(rpcKind, old); throw new IllegalArgumentException("ReRegistration of rpcKind: " + rpcKind); &#125; LOG.debug("rpcKind=" + rpcKind + ", rpcRequestWrapperClass=" + rpcRequestWrapperClass + ", rpcInvoker=" + rpcInvoker);&#125; 可以看出在当初WritableRpcEngine类加载时就已经将对应的初始化的Server.WritableRpcInvoker类放入rpcKindMap中，所以如果是采用WritableRpcEngine类的，那现在从rpcKindMap取出的就是Server.WritableRpcInvoker类。具体看该类下的call方法如果通过反射技术调用过程得到具体的值。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120static class WritableRpcInvoker implements RpcInvoker &#123; @Override public Writable call(org.apache.hadoop.ipc.RPC.Server server, String protocolName, Writable rpcRequest, long receivedTime) throws IOException, RPC.VersionMismatch &#123; //得到当初客户端序列化的Invocation对象 Invocation call = (Invocation)rpcRequest; if (server.verbose) log("Call: " + call); // Verify writable rpc version if (call.getRpcVersion() != writableRpcVersion) &#123; // Client is using a different version of WritableRpc throw new RpcServerException( "WritableRpc version mismatch, client side version=" + call.getRpcVersion() + ", server side version=" + writableRpcVersion); &#125; long clientVersion = call.getProtocolVersion(); final String protoName; ProtoClassProtoImpl protocolImpl; if (call.declaringClassProtocolName.equals(VersionedProtocol.class.getName())) &#123; // VersionProtocol methods are often used by client to figure out // which version of protocol to use. // // Versioned protocol methods should go the protocolName protocol // rather than the declaring class of the method since the // the declaring class is VersionedProtocol which is not // registered directly. // Send the call to the highest protocol version VerProtocolImpl highest = server.getHighestSupportedProtocol( RPC.RpcKind.RPC_WRITABLE, protocolName); if (highest == null) &#123; throw new RpcServerException("Unknown protocol: " + protocolName); &#125; protocolImpl = highest.protocolTarget; &#125; else &#123; protoName = call.declaringClassProtocolName; // Find the right impl for the protocol based on client version. //根据协议名和客户端的版本号，RpcKind来找到具体实现类 ProtoNameVer pv = new ProtoNameVer(call.declaringClassProtocolName, clientVersion); protocolImpl = server.getProtocolImplMap(RPC.RpcKind.RPC_WRITABLE).get(pv); if (protocolImpl == null) &#123; // no match for Protocol AND Version VerProtocolImpl highest = server.getHighestSupportedProtocol(RPC.RpcKind.RPC_WRITABLE, protoName); if (highest == null) &#123; throw new RpcServerException("Unknown protocol: " + protoName); &#125; else &#123; // protocol supported but not the version that client wants throw new RPC.VersionMismatch(protoName, clientVersion, highest.version); &#125; &#125; &#125; // Invoke the protocol method long startTime = Time.now(); int qTime = (int) (startTime-receivedTime); Exception exception = null; try &#123; //根据方法名来找到方法，利用反射来调用该方法得到结果 Method method = protocolImpl.protocolClass.getMethod(call.getMethodName(), call.getParameterClasses()); method.setAccessible(true); server.rpcDetailedMetrics.init(protocolImpl.protocolClass); Object value = method.invoke(protocolImpl.protocolImpl, call.getParameters()); if (server.verbose) log("Return: "+value); //封装进ObjectWritable类中返回 return new ObjectWritable(method.getReturnType(), value); &#125; catch (InvocationTargetException e) &#123; Throwable target = e.getTargetException(); if (target instanceof IOException) &#123; exception = (IOException)target; throw (IOException)target; &#125; else &#123; IOException ioe = new IOException(target.toString()); ioe.setStackTrace(target.getStackTrace()); exception = ioe; throw ioe; &#125; &#125; catch (Throwable e) &#123; if (!(e instanceof IOException)) &#123; LOG.error("Unexpected throwable object ", e); &#125; IOException ioe = new IOException(e.toString()); ioe.setStackTrace(e.getStackTrace()); exception = ioe; throw ioe; &#125; finally &#123; int processingTime = (int) (Time.now() - startTime); if (LOG.isDebugEnabled()) &#123; String msg = "Served: " + call.getMethodName() + " queueTime= " + qTime + " procesingTime= " + processingTime; if (exception != null) &#123; msg += " exception= " + exception.getClass().getSimpleName(); &#125; LOG.debug(msg); &#125; String detailedMetricsName = (exception == null) ? call.getMethodName() : exception.getClass().getSimpleName(); server.rpcMetrics.addRpcQueueTime(qTime); server.rpcMetrics.addRpcProcessingTime(processingTime); server.rpcDetailedMetrics.addProcessingTime(detailedMetricsName, processingTime); if (server.isLogSlowRPC()) &#123; server.logSlowRpcCalls(call.getMethodName(), processingTime); &#125; &#125; &#125;&#125; WritableRpcInvoker的call方法中将rpcRequest转型为Invocation对象，取出其中的协议名和版本号，根据其从server.getProtocolImplMap中得到协议具体实现类对象，从而通过方法名利用反射调用相应的方法，返回结果封装进ObjectWritable中。至于是如何得到协议具体实现类对象的，可以从以下方法中看出。 12345678910 Map&lt;ProtoNameVer, ProtoClassProtoImpl&gt; getProtocolImplMap(RPC.RpcKind rpcKind) &#123; if (protocolImplMapArray.size() == 0) &#123;// initialize for all rpc kinds for (int i=0; i &lt;= RpcKind.MAX_INDEX; ++i) &#123; protocolImplMapArray.add( new HashMap&lt;ProtoNameVer, ProtoClassProtoImpl&gt;(10)); &#125; &#125; return protocolImplMapArray.get(rpcKind.ordinal()); &#125; 根据rpcKind从protocolImplMapArray取出Map&lt;ProtoNameVer, ProtoClassProtoImpl&gt;对象，再根据ProtoNameVer取出具体实例对象。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public Server(Class&lt;?&gt; protocolClass, Object protocolImpl, Configuration conf, String bindAddress, int port, int numHandlers, int numReaders, int queueSizePerHandler, boolean verbose, SecretManager&lt;? extends TokenIdentifier&gt; secretManager, String portRangeConfig) throws IOException &#123; super(bindAddress, port, null, numHandlers, numReaders, queueSizePerHandler, conf, classNameBase(protocolImpl.getClass().getName()), secretManager, portRangeConfig); this.verbose = verbose; Class&lt;?&gt;[] protocols; if (protocolClass == null) &#123; // derive protocol from impl /* * In order to remain compatible with the old usage where a single * target protocolImpl is suppled for all protocol interfaces, and * the protocolImpl is derived from the protocolClass(es) * we register all interfaces extended by the protocolImpl */ protocols = RPC.getProtocolInterfaces(protocolImpl.getClass()); &#125; else &#123; if (!protocolClass.isAssignableFrom(protocolImpl.getClass())) &#123; throw new IOException("protocolClass "+ protocolClass + " is not implemented by protocolImpl which is of class " + protocolImpl.getClass()); &#125; // register protocol class and its super interfaces //将protocolClass和protocolImpl放入map中 registerProtocolAndImpl(RPC.RpcKind.RPC_WRITABLE, protocolClass, protocolImpl); //得到protocolClass的所有接口 protocols = RPC.getProtocolInterfaces(protocolClass); &#125; //将protocolClass的所有接口和protocolImpl放入map中 for (Class&lt;?&gt; p : protocols) &#123; if (!p.equals(VersionedProtocol.class)) &#123; // registerProtocolAndImpl(RPC.RpcKind.RPC_WRITABLE, p, protocolImpl); &#125; &#125;&#125;void registerProtocolAndImpl(RpcKind rpcKind, Class&lt;?&gt; protocolClass, Object protocolImpl) &#123; String protocolName = RPC.getProtocolName(protocolClass); long version; try &#123; version = RPC.getProtocolVersion(protocolClass); &#125; catch (Exception ex) &#123; LOG.warn("Protocol " + protocolClass + " NOT registered as cannot get protocol version "); return; &#125; //放入map中 getProtocolImplMap(rpcKind).put(new ProtoNameVer(protocolName, version), new ProtoClassProtoImpl(protocolClass, protocolImpl)); LOG.debug("RpcKind = " + rpcKind + " Protocol Name = " + protocolName + " version=" + version + " ProtocolImpl=" + protocolImpl.getClass().getName() + " protocolClass=" + protocolClass.getName()); &#125; 可以从上面看出，在初始化Server对象时，就已经将协议具体类和其所有接口类放入一个Map中，而这个map根据rpcKind放在了列表的rpcKind位置处。 现在回到Handler的run方法中，在调用call方法获得结果值后，调用setupResponse和call.sendResponse()方法，我们来具体看看这两个方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/*** Setup response for the IPC Call.* * @param responseBuf buffer to serialize the response into* @param call &#123;@link Call&#125; to which we are setting up the response* @param status of the IPC call* @param rv return value for the IPC Call, if the call was successful* @param errorClass error class, if the the call failed* @param error error message, if the call failed* @throws IOException*/private static void setupResponse(ByteArrayOutputStream responseBuf, Call call, RpcStatusProto status, RpcErrorCodeProto erCode, Writable rv, String errorClass, String error) throws IOException &#123; responseBuf.reset(); DataOutputStream out = new DataOutputStream(responseBuf); //构造响应数据头 RpcResponseHeaderProto.Builder headerBuilder = RpcResponseHeaderProto.newBuilder(); headerBuilder.setClientId(ByteString.copyFrom(call.clientId)); headerBuilder.setCallId(call.callId); headerBuilder.setRetryCount(call.retryCount); headerBuilder.setStatus(status); headerBuilder.setServerIpcVersionNum(CURRENT_VERSION); if (status == RpcStatusProto.SUCCESS) &#123; RpcResponseHeaderProto header = headerBuilder.build(); final int headerLen = header.getSerializedSize(); int fullLength = CodedOutputStream.computeRawVarint32Size(headerLen) + headerLen; try &#123; if (rv instanceof ProtobufRpcEngine.RpcWrapper) &#123; ProtobufRpcEngine.RpcWrapper resWrapper = (ProtobufRpcEngine.RpcWrapper) rv; fullLength += resWrapper.getLength(); out.writeInt(fullLength); header.writeDelimitedTo(out); rv.write(out); &#125; else &#123; // Have to serialize to buffer to get len //写入数据长度，数据头，数据内容（序列化响应返回的内容） final DataOutputBuffer buf = new DataOutputBuffer(); rv.write(buf); byte[] data = buf.getData(); fullLength += buf.getLength(); out.writeInt(fullLength); header.writeDelimitedTo(out); out.write(data, 0, buf.getLength()); &#125; &#125; catch (Throwable t) &#123; LOG.warn("Error serializing call response for call " + call, t); // Call back to same function - this is OK since the // buffer is reset at the top, and since status is changed // to ERROR it won't infinite loop. setupResponse(responseBuf, call, RpcStatusProto.ERROR, RpcErrorCodeProto.ERROR_SERIALIZING_RESPONSE, null, t.getClass().getName(), StringUtils.stringifyException(t)); return; &#125; &#125; else &#123; // Rpc Failure headerBuilder.setExceptionClassName(errorClass); headerBuilder.setErrorMsg(error); headerBuilder.setErrorDetail(erCode); RpcResponseHeaderProto header = headerBuilder.build(); int headerLen = header.getSerializedSize(); final int fullLength = CodedOutputStream.computeRawVarint32Size(headerLen) + headerLen; out.writeInt(fullLength); header.writeDelimitedTo(out); &#125; //将其设置在Call对象中 call.setResponse(ByteBuffer.wrap(responseBuf.toByteArray()));&#125; setupResponse中主要是序列化响应的对象，并将其放入Call对象的rpcResponse中。接着就需要将Call传递给Responder来处理了。可以从Call.sendResponse()看到。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131//Call.sendResponse()@InterfaceStability.Unstable@InterfaceAudience.LimitedPrivate(&#123;"HDFS"&#125;)public void sendResponse() throws IOException &#123; int count = responseWaitCount.decrementAndGet(); assert count &gt;= 0 : "response has already been sent"; if (count == 0) &#123; connection.sendResponse(this); &#125;&#125;//Connection.sendResponse(Call call)private void sendResponse(Call call) throws IOException &#123; responder.doRespond(call);&#125;//// Enqueue a response from the application.//void doRespond(Call call) throws IOException &#123; synchronized (call.connection.responseQueue) &#123; // must only wrap before adding to the responseQueue to prevent // postponed responses from being encrypted and sent out of order. if (call.connection.useWrap) &#123; ByteArrayOutputStream response = new ByteArrayOutputStream(); wrapWithSasl(response, call); call.setResponse(ByteBuffer.wrap(response.toByteArray())); &#125; //放入responseQueue中 call.connection.responseQueue.addLast(call); if (call.connection.responseQueue.size() == 1) &#123; processResponse(call.connection.responseQueue, true); &#125; &#125;&#125;// Processes one response. Returns true if there are no more pending// data for this channel.//private boolean processResponse(LinkedList&lt;Call&gt; responseQueue, boolean inHandler) throws IOException &#123; boolean error = true; boolean done = false; // there is more data for this channel. int numElements = 0; Call call = null; try &#123; synchronized (responseQueue) &#123; // // If there are no items for this channel, then we are done // numElements = responseQueue.size(); if (numElements == 0) &#123; error = false; return true; // no more data for this channel. &#125; // // Extract the first call // 从列表中取出第一个元素 // call = responseQueue.removeFirst(); SocketChannel channel = call.connection.channel; if (LOG.isDebugEnabled()) &#123; LOG.debug(Thread.currentThread().getName() + ": responding to " + call); &#125; // // Send as much data as we can in the non-blocking fashion // 尝试一次性发送结果 // int numBytes = channelWrite(channel, call.rpcResponse); if (numBytes &lt; 0) &#123; return true; &#125; //如果没有余留，则表示发送成功 if (!call.rpcResponse.hasRemaining()) &#123; //Clear out the response buffer so it can be collected call.rpcResponse = null; call.connection.decRpcCount(); if (numElements == 1) &#123; // last call fully processes. done = true; // no more data for this channel. &#125; else &#123; done = false; // more calls pending to be sent. &#125; if (LOG.isDebugEnabled()) &#123; LOG.debug(Thread.currentThread().getName() + ": responding to " + call + " Wrote " + numBytes + " bytes."); &#125; &#125; else &#123; // // If we were unable to write the entire response out, then // insert in Selector queue. // 否则需要放入列表中，向responser的writeSelector注册写事件 // call.connection.responseQueue.addFirst(call); if (inHandler) &#123; // set the serve time when the response has to be sent later call.timestamp = Time.now(); incPending(); try &#123; // Wakeup the thread blocked on select, only then can the call // to channel.register() complete. writeSelector.wakeup(); //向选择器注册 channel.register(writeSelector, SelectionKey.OP_WRITE, call); &#125; catch (ClosedChannelException e) &#123; //Its ok. channel might be closed else where. done = true; &#125; finally &#123; decPending(); &#125; &#125; if (LOG.isDebugEnabled()) &#123; LOG.debug(Thread.currentThread().getName() + ": responding to " + call + " Wrote partial " + numBytes + " bytes."); &#125; &#125; error = false; // everything went off well &#125; &#125; finally &#123; if (error &amp;&amp; call != null) &#123; LOG.warn(Thread.currentThread().getName()+", call " + call + ": output error"); done = true; // error. no more data for this channel. closeConnection(call.connection); &#125; &#125; return done;&#125; 可以看到，最后放到了responseQueue中，responseQueue是一个列表，不是一个阻塞对列，每个Call封装了从客户端反序列化的对象信息，发送回客户端的数据还有Connection(每一个客户端和服务端维持一个Connection，因此call.connection.responseQueue代表着连接着同个客户端的响应消息发送队列，发送到同个客户端的call存放在同一个responseQueue中)。当responseQueue的长度为1时会调用processResponse，但如果存在一些特殊情况（返回的结果数量太大或者网络缓慢），没能一次性将结果发送，则会向Responser的selector注册写事件，由Responser将响应结果采用异步的方式继续发送未发送完的数据。来看Responser的run方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117@Overridepublic void run() &#123; LOG.info(Thread.currentThread().getName() + ": starting"); SERVER.set(Server.this); try &#123; doRunLoop(); &#125; finally &#123; LOG.info("Stopping " + Thread.currentThread().getName()); try &#123; writeSelector.close(); &#125; catch (IOException ioe) &#123; LOG.error("Couldn't close write selector in " + Thread.currentThread().getName(), ioe); &#125; &#125;&#125;private void doRunLoop() &#123; long lastPurgeTime = 0; // last check for old calls. while (running) &#123; try &#123; waitPending(); // If a channel is being registered, wait. writeSelector.select(PURGE_INTERVAL); Iterator&lt;SelectionKey&gt; iter = writeSelector.selectedKeys().iterator(); while (iter.hasNext()) &#123; SelectionKey key = iter.next(); iter.remove(); try &#123; if (key.isWritable()) &#123; doAsyncWrite(key); &#125; &#125; catch (CancelledKeyException cke) &#123; // something else closed the connection, ex. reader or the // listener doing an idle scan. ignore it and let them clean // up Call call = (Call)key.attachment(); if (call != null) &#123; LOG.info(Thread.currentThread().getName() + ": connection aborted from " + call.connection); &#125; &#125; catch (IOException e) &#123; LOG.info(Thread.currentThread().getName() + ": doAsyncWrite threw exception " + e); &#125; &#125; //等待时间设置，如果消息超过该时间还没有发送成功，则会执行后面的代码，关闭管道 long now = Time.now(); if (now &lt; lastPurgeTime + PURGE_INTERVAL) &#123; continue; &#125; lastPurgeTime = now; // // If there were some calls that have not been sent out for a // long time, discard them. // if(LOG.isDebugEnabled()) &#123; LOG.debug("Checking for old call responses."); &#125; ArrayList&lt;Call&gt; calls; // get the list of channels from list of keys. // 搜集超过时间还未发送的call synchronized (writeSelector.keys()) &#123; calls = new ArrayList&lt;Call&gt;(writeSelector.keys().size()); iter = writeSelector.keys().iterator(); while (iter.hasNext()) &#123; SelectionKey key = iter.next(); Call call = (Call)key.attachment(); if (call != null &amp;&amp; key.channel() == call.connection.channel) &#123; calls.add(call); &#125; &#125; &#125; //逐个关闭 for(Call call : calls) &#123; doPurge(call, now); &#125; &#125; catch (OutOfMemoryError e) &#123; // // we can run out of memory if we have too many threads // log the event and sleep for a minute and give // some thread(s) a chance to finish // LOG.warn("Out of Memory in server select", e); try &#123; Thread.sleep(60000); &#125; catch (Exception ie) &#123;&#125; &#125; catch (Exception e) &#123; LOG.warn("Exception in Responder", e); &#125; &#125;&#125;private void doAsyncWrite(SelectionKey key) throws IOException &#123; Call call = (Call)key.attachment(); if (call == null) &#123; return; &#125; if (key.channel() != call.connection.channel) &#123; throw new IOException("doAsyncWrite: bad channel"); &#125; synchronized(call.connection.responseQueue) &#123; if (processResponse(call.connection.responseQueue, false)) &#123; try &#123; key.interestOps(0); &#125; catch (CancelledKeyException e) &#123; /* The Listener/reader might have closed the socket. * We don't explicitly cancel the key, so not sure if this will * ever fire. * This warning could be removed. */ LOG.warn("Exception while changing ops : " + e); &#125; &#125; &#125;&#125; Responser异步继续发送在选择器中注册的写事件中的响应消息（还是调用），并设定超时时间，超过时间还未发送则对其进行关闭。 总体流程客户端客户端实际时使用了动态代理在在Invoker方法中发送了远程过程调用的请求到服务端，并等待接受结果。 服务端服务端采用reactor基于事件驱动的设计模式，利用JDK自带的socket通信机制和线程池完成。 参考http://bigdatadecode.club/Hadoop%20RPC%20%E8%A7%A3%E6%9E%90.html 《Hadoop技术内幕：深入解析YARN架构设计与实现原理》,图片也来自此。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F11%2F20%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[字符串编辑距离]]></title>
    <url>%2F2018%2F11%2F20%2F%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[编辑距离（Edit Distance），这里指的是Levenshtein距离，也就是字符串S1通过插入、修改、删除三种操作最少能变换成字符串S2的次数。接下来介绍利用动态规划来求解字符串的编辑距离。 定义：$s_1$和$s_2$表示两字符串，$dist(i, j)$表示字符串$s_1$的前$i$个字符串和$s_2$的前$j$个字符串的编辑距离，$s_1(i)$和$s_2(j)$分别表示$s1$的第$i$个字符和$s2$的第$j$字符。 若$s_1(i) = s_2(j)$,则$dist(i, j)$就等于$s_1$的前$i-1$个字符串和$s_2$的前$j-1$个字符串的编辑距离即 $dist(i, j) = dist(i-1,j-1) $ 若$s_1(i) \neq s_2(j)$,则为了使$s_1(i) = s_2(j)$,可以通过在$s_1$的第$i$个字符处插入$s_2$的第$j$个字符，或替换$s_1$的第$i$个字符为$s_2$的第$j$个字符,或删除$s_1(s_2)$的第$i(j)$个字符(即使删除之后还是会出现不同),但是上述都是在进行了一次字符的操作之后,将转化为字问题求解，如上述的替换使$s_1(i) = s_2(j)$，则 $dist(i, j) = dist(i-1,j-1) + 1$ ，插入和删除使 $dist(i, j) = dist(i-1,j) +1$或者 $dist(i, j) = dist(i,j-1) +1$ 。 基于上述的情况可以得出递推公式：$$dist(i,j) = \begin{equation}\left\{ \begin{array}{lr} i, &amp; j = 0 \\ j, &amp; i = 0 \\ dist(i-1,j-1), &amp; s_1(i) = s_2(j) \\ min(d(i-1,j), d(i,j-1),d(i-1,j-1)), &amp; s_1(i) \neq s_2(j) \\ \end{array}\right.\end{equation}$$例如：$s_1 = “abcd”$,$s_2 = “abfce”$,则利用动态规划求解的矩阵为 a b f c e 0 1 2 3 4 5 a 1 0 1 2 3 4 b 2 1 0 1 2 3 c 3 2 1 1 1 2 d 4 3 2 2 2 2 代码实现1234567891011121314151617181920212223242526272829public static int levenshtein(String s1, String s2)&#123; if(s1 == null)&#123; return s2 == null? 0:s2.length(); &#125; if(s2 == null)&#123; return s1 == null? 0:s1.length(); &#125; int n = s1.length(); int m = s2.length(); int[][] matrix = new int[n+1][m+1]; for(int i = 0;i &lt;= n;i++)&#123; matrix[i][0] = i; &#125; for(int j = 0;j &lt;= m;j++)&#123; matrix[0][j] = j; &#125; for(int i = 1;i &lt;= n;i++)&#123; for(int j = 1;j &lt;= m;j++)&#123; if(s1.charAt(i-1) == s2.charAt(j-1))&#123; matrix[i][j] = matrix[i-1][j-1]; &#125;else &#123; matrix[i][j] = Math.min(Math.min(matrix[i-1][j], matrix[i][j-1]), matrix[i-1][j-1]) + 1; &#125; &#125; &#125; return matrix[n][m];&#125; 上述的方法其中有一个不足之处是当两个字符串太长时，则对应申请的数组占用的空间也变大。而上述的算法中，在更新$dist(i,j)$的过程中只用到了$dist(i-1,j)$、$dist(i,j-1)$、$dist(i-1,j-1)$这三个数，因此我们可以只存储更新的上一行的数据，这样就可以得到$dist(i-1,j)$，$dist(i-1,j-1)$这两个数，而$d(i,0) = i$,可以在此基础上根据上一行的$d(i-1,0)$和$d(i-1,1)$推出$d(i-1,1)$,在结合上一行的$d(i-1,1)$和$d(i-1,2)$推出$d(i-1,2)$…以此类推得到第$i$行的数据。接下来是改进后的代码实现 1234567891011121314151617181920212223242526public static int levenshteinImprove(String s1, String s2)&#123; if(s1 == null) return s2 == null? 0:s2.length(); if(s2 == null) return s1 == null? 0:s1.length(); int n = s1.length(),m = s2.length(); int[] matrix = new int[m+1]; for(int i = 0;i &lt;= m;i++)&#123; matrix[i] = i; &#125; for(int i = 1;i &lt;= n;i++)&#123; int pre = matrix[0]; matrix[0] = i; for(int j = 1;j &lt;= m;j++)&#123; int tmp = matrix[j]; if(s1.charAt(i-1) == s2.charAt(j-1))&#123; matrix[j] = pre; &#125;else &#123; matrix[j] = Math.min(Math.min(pre, matrix[j-1]), matrix[j]) + 1; &#125; pre = tmp; &#125; &#125; return matrix[m];&#125; 上述代码还可以改进一个小细节，当比较的字符串长短一个很长，一个很短时，我们可以比较其长短，取较短的字符串长度作为开辟的数组的长度。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>字符串</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
</search>
