<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[关于动态规划的想法]]></title>
    <url>%2F%E5%85%B3%E4%BA%8E%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%9A%84%E6%83%B3%E6%B3%95.html</url>
    <content type="text"><![CDATA[前言动态规划是经常用到的算法，一般是通过递推，将一个复杂的问题分解为简单的最小问题求解，即存在着最优子结构。从求解子问题一步步推出原始问题的解。我将目前遇到的动态规划的问题按照开辟数组的维度分为一维和二维两类，背包问题不属于这类，因为背包问题相关的问题也是通过动态规划，但是比较复杂，单独放在外面。以上是我个人对于遇到的动态规范的分类，纯属个人想法，还有树形动态规划，后续遇到会加入进去。 一维这里的一维也可以理解为递推，即一个问题的求解一般根据其前一个子问题或前两子问题来的。 比如Fibonacci数列，LeetCode 70等。 以Fibonacci数列为例，$f(n) = f(n-1) + f(n-2)$,其中$f(1) = f(2) = 1$。这种问题一般通过递归自顶向下或者循环自底向上求解。如求Fibonacci数列的$f(n)$ 12345678910111213141516171819202122// 自顶向下，递归public long fibonacci(int n)&#123; if(n &lt;= 2)&#123; return 1; &#125;else&#123; return fibonacci(n-1) + fibonacci(n-2) &#125;&#125;// 自底向上public long fibonacci(int n)&#123; if(n &lt;= 2)&#123; return 1; &#125; long p = 1;q = 1; for(int i = 3;i &lt;= n;i++)&#123; int tmp = q; q = p + q; p = tmp; &#125; return q;&#125; 当然在递归时存在着重复计算，如$f(n-1)$需要计算0到n-2的所有子结果，而$f(n-2)$需要计算0到n-3的所有子结果，存在着0到n-3的子问题的重复计算。因此可以使用备忘录的形式，即开辟数组记录已经计算过的$f(n)$值。 12345678910111213141516171819// 修改之后的递归public long fibonacci(int n)&#123; long[] memo = new long[n+1]; for(int i = 0;i &lt;= n;i++)&#123; memo[i] = -1; &#125; return recursion(n);&#125;public long recursion(int n)&#123; if(n &lt;= 2)&#123; return 1; &#125; if(memo[n] != -1)&#123; return memo[n]; &#125; memo[n] = fibonacci(n-1) + fibonacci(n-2); return memo[n];&#125; 二维二维下的动态规划，也可以看做是区间dp，一般是将问题从最小的区间开始，不断扩展，从已知的几个相邻区间推到出现区间的值。 数组从上向下或从左往右扩展如字符串的编辑距离的求解,一开始得到各自空字符串的编辑距离（二维数组第0行和第0列），接着根据当前比较两个字符的情况和已知的区间（数组中的左元素、上元素和左上元素）得到当前区间的值。 如简单正则表达式匹配,假设$d[i][j]​$表示p字符模式从0到i的子串和s字符串从0到j的子串的匹配结果。 如果$p[i]=’*’ \&amp;\&amp;p[i-1] = s[j]$，$d[i][j]=d[i-1][j]||d[i-2][j] || d[i][j-1]$ 如果$p[i]=’*’ \&amp;\&amp;p[i-1]\neq s[j]​$，$d[i][j]=d[i-1][j]||d[i-2][j]​$ 如果$p[i]=’.’ ||p[i] ==s[j]$，$d[i][j]=true$ 不满足上述情况的，$d[i][j]=false$ 实现代码如下：s 123456789101112131415161718192021222324252627282930313233343536373839public boolean isMatch(String s, String p) &#123; boolean[][] isMacth = new boolean[p.length()+1][s.length()+1]; isMacth[0][0] = true; for(int i = 1; i &lt;= s.length();i++)&#123; isMacth[0][i] = false; &#125; for(int i = 1; i &lt;= p.length();i++)&#123; if(p.charAt(i-1) == '*')&#123; isMacth[i][0] = isMacth[i-2][0]; &#125;else &#123; isMacth[i][0] = false; &#125; &#125; for(int i = 1;i &lt;= p.length();i++)&#123; if(p.charAt(i-1) == '*')&#123; for(int j = 1; j &lt;= s.length();j++)&#123; if(compare(p.charAt(i-2), s.charAt(j-1)))&#123; isMacth[i][j] = isMacth[i][j-1]; &#125; isMacth[i][j] = isMacth[i-1][j] || isMacth[i-2][j] || isMacth[i][j]; &#125; &#125;else &#123; for(int j = 1; j &lt;= s.length(); j++)&#123; if (compare(p.charAt(i-1), s.charAt(j-1)))&#123; isMacth[i][j] = isMacth[i-1][j-1]; &#125;else &#123; isMacth[i][j] = false; &#125; &#125; &#125; &#125; return isMacth[p.length()][s.length()];&#125;public boolean compare(char a, char b)&#123; if(a == '.')&#123; return true; &#125;else return a == b;&#125; 数组从中间向左上或右下扩展如最长回文子串,假设$p[i][j]$表示字符串第$i$个字符到第$j$个字符之间的子串是否为回文子串，其递推公式为：$$p[i][j] = \begin{cases}true \quad(s[i] == s[j]\quad and\quad p[i+1][j-1] = true)\quad or \quad j-i == 1\\false \quad other\\\end{cases}$$如果$p[i][j]$为真且长度大于当前最大长度则记录长度和起始位置。 1234567891011121314151617181920212223242526public String longestPalindrome(String s) &#123; if(s == null || s.length() &lt;= 1)&#123; return s; &#125; boolean[][] p = new boolean[s.length()][s.length()]; p[s.length()-1][s.length()-1] = true; int l = 0, start = 0; for(int i = s.length()-2;i &gt;= 0;i--)&#123; p[i][i] = true; for(int j = i+1; j &lt; s.length();j++)&#123; if(s.charAt(i) == s.charAt(j))&#123; if((j-i) == 1)&#123; p[i][j] = true; &#125; if(p[i+1][j-1])&#123; p[i][j] = true; &#125; &#125; if(p[i][j] &amp;&amp; j - i &gt; l)&#123; l = j - i; start = i; &#125; &#125; &#125; return s.substring(start, start+l+1);&#125; 背包问题0-1背包问题及其相关的问题可以看大牛的《背包九讲》，里面写的很详细。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>背包问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读HashMap源码]]></title>
    <url>%2F%E8%AF%BBHashMap%E6%BA%90%E7%A0%81.html</url>
    <content type="text"><![CDATA[前言HashMap是Java中常用的数据结构，是集合类中的重要存在，其中包含了散列表、链表和红黑树。散列表解决冲突的方法是链地址法，即将散列值相同的元素存放在一个链表中。当冲突过多，链表过长会造成查找效率降低，因此java8中在HashMap中引入红黑树进行优化，当某个散列值下的链表长度过长（长度大于8），会将其转变为红黑树存储，优化查询。涉及到红黑树的操作不会再这里细讲 源码解读构造函数123456789101112131415161718192021222324252627282930313233343536373839404142434445// 默认初始化容量static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4;// 最大容量static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;// 负载因子static final float DEFAULT_LOAD_FACTOR = 0.75f;final float loadFactor;//容量阈值int threshold;public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR;&#125;public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);&#125;public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125;// 返回刚好大于cap的2的n次方的值static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; initialCapacity: 初始容量 loadFactor：负载因子 threshold：容量阈值，当HashMap的存储的元素达到该值时，就会触发扩容操作 capatcity在源码中没有定义变量记录是因为其大小为table数组的长度，阈值threshold的大小由capatcity和loadFactor来决定，threshold=capatcity*loadFactor HashMap的构造函数主要是设置初始化容量和负载因子，如果采用无参数构造函数，则使用默认的初始化容量16和负载因子0.75。当采用设置了初始化容量的构造函数时，就会调用tableSizeFor方法，该方法通过位运算的方法得到一个2的n次方，该值刚好大于cap,即当用户指定一个初始化容量为2的n次方的值，实际HashMap的阈值赋值为大于该值的一个2的n次方的值，在初始化的时候，其首先按照threshold的大小开辟数组，接着经过一次赋值threshold=threshold*loadFactor（后文resize方法中可以看到）改变threshold的大小，符合上述的说法。 基本存储单元HashMap的散列表是一个数组table，其中的元素是Node类，该类继承了Map.Entry接口，其中包含了key,value,hashCode和next(存储链表中下一跳)。在java8之前是内置的Entry类，而在java8中Entry类变成了Node类。 123456789101112131415161718192021222324252627282930313233343536373839static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; 红黑树中节点TreeNode 123456789101112131415161718192021222324252627282930static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125; /** * Returns root of tree containing this node. */ final TreeNode&lt;K,V&gt; root() &#123; for (TreeNode&lt;K,V&gt; r = this, p;;) &#123; if ((p = r.parent) == null) return r; r = p; &#125; &#125; // 省略后面红黑树增删，调整等方法 ...&#125;static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125;&#125; 可以看到TreeNode继承LinkedHashMap.Entry，而LinkedHashMap.Entry是继承HashMap.Node,所以TheeNode是HashMap.Node的子类，对于HashMap的元素普遍操作（如遍历）都可以转为对Node的操作，无需判断table中存储从是链表还是红黑树。 查找123456789101112131415161718192021222324252627282930313233343536373839404142public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;/* * 这里将高16位和低16位进行异或运算，使产生的低16位的产生受到高16位和低16位共同 * 影响，避免当HashMap的capacity过小时，产生的散列值只受到低16位影响，同时也会增加 * hash的复杂度，当HashMap中放入hashCode分布不佳的元素，可以通过这种hash计算方法，降低 * 散列表的冲突率。 */static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;/** * HashMap中判断元素是相等，必须hash值和元素equals()方法都判断为相等 * 因为hash值不仅在查找中使用，判断元素相等时也作为一个判断依据 */final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; // （n-1）&amp; hash取的是余数即：hash%(n-1)，位运算取余的效率更高 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; //如果是红黑树，则用红黑树递归方法寻找 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //否则用链表的方式寻找 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; (n-1)&amp; hash是用位运算计算hash%(n-1),确定key在哈希表中的位置。其hash值的运算不是直接采用HashCode，而是通过HashCode重新计算，因为当table表长度小，其散列值的产生只有低位有关，与高位无关，若加入的元素产生的hashCode低位比较相似，高位不同，则会造成大量冲突。因此将高16位与低16位做异或运算，使hash值低位受HashCode影响，使散列值分布均匀。 遍历HashMap的遍历经常是通过遍历keySet和entrySet方法产生的Set来实现的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public Set&lt;K&gt; keySet() &#123; Set&lt;K&gt; ks = keySet; if (ks == null) &#123; ks = new KeySet(); keySet = ks; &#125; return ks;&#125;final class KeySet extends AbstractSet&lt;K&gt; &#123; public final int size() &#123; return size; &#125; public final void clear() &#123; HashMap.this.clear(); &#125; public final Iterator&lt;K&gt; iterator() &#123; return new KeyIterator(); &#125; public final boolean contains(Object o) &#123; return containsKey(o); &#125; public final boolean remove(Object key) &#123; return removeNode(hash(key), key, null, false, true) != null; &#125; public final Spliterator&lt;K&gt; spliterator() &#123; return new KeySpliterator&lt;&gt;(HashMap.this, 0, -1, 0, 0); &#125; public final void forEach(Consumer&lt;? super K&gt; action) &#123; Node&lt;K,V&gt;[] tab; if (action == null) throw new NullPointerException(); if (size &gt; 0 &amp;&amp; (tab = table) != null) &#123; int mc = modCount; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) action.accept(e.key); &#125; if (modCount != mc) throw new ConcurrentModificationException(); &#125; &#125;&#125;final class KeyIterator extends HashIterator implements Iterator&lt;K&gt; &#123; public final K next() &#123; return nextNode().key; &#125;&#125;abstract class HashIterator &#123; Node&lt;K,V&gt; next; // next entry to return Node&lt;K,V&gt; current; // current entry int expectedModCount; // for fast-fail int index; // current slot HashIterator() &#123; expectedModCount = modCount; Node&lt;K,V&gt;[] t = table; current = next = null; index = 0; // next指到第一个不为空的数组元素 if (t != null &amp;&amp; size &gt; 0) &#123; // advance to first entry do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; &#125; public final boolean hasNext() &#123; return next != null; &#125; final Node&lt;K,V&gt; nextNode() &#123; Node&lt;K,V&gt;[] t; Node&lt;K,V&gt; e = next; if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (e == null) throw new NoSuchElementException(); if ((next = (current = e).next) == null &amp;&amp; (t = table) != null) &#123; do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; return e; &#125; public final void remove() &#123; Node&lt;K,V&gt; p = current; if (p == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); current = null; K key = p.key; removeNode(hash(key), key, null, false, false); expectedModCount = modCount; &#125;&#125; 可以看到keySet的迭代器继承HashIterator，其中KeyIterator的next方法直接调用了HashIterator的nextNode方法，nextNode是通过遍历table找到不为null的元素，接着再调用Node的next，不断从链表中遍历元素，但是其中冲突可能会用红黑树存储，对于红黑树的遍历并没有描写。这是因为上文讲到红黑树的TreeNode继承了Node，TreeNode的next存储着红黑树遍历下一跳的元素，所以只要调用Node类的next就可以遍历链表和红黑树，下文会看到ThreeNode的next的建立。 插入除去红黑树的操作，插入是HashMap较为复杂的方法，因为其设计到table的扩容，链表转为红黑树等问题，先来看代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657static final int TREEIFY_THRESHOLD = 8;public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //如果table为空，则初始化table,延迟到插入元素时进行初始化 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //如果不存在冲突，直接放入将节点放入数组中 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 如果头结点key相等，将头结点赋值给e if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) // 如果是红黑树，调用树的插入操作 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 否则为链表，遍历 for (int binCount = 0; ; ++binCount) &#123; // 遍历到尾部，插入到链表的最后一个元素 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 当链表长度大于8时，变为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // 存在相同的key，则将其赋值给e if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // e不为空说明是替换掉相同key下的值，返回 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 如果容量大于阈值，则需进行扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 可以看到table是在插入的时候进行判断有无进行初始化并对其初始化，初始化和扩容都是调用resize方法，整个插入过程是首先检查是否初始化，无则进行初始化，接着根据插入元素的hash的散列值找到数组中的位置，如果不存在冲突则直接存入，如果是链表，则遍历链表，如果是红黑树，则遍历树，对其已存在的key，替换value，不存在则插入到链表尾部或者红黑树中。其中在插入到链表只若容量大于8，则需要将链表转为红黑树。 12345678910111213141516171819202122232425static final int MIN_TREEIFY_CAPACITY = 64;/* * 将链表转为红黑树，只有当链表长度大于8&amp;&amp;table的容量大于64时才会触发树化 */final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); &#125;&#125; 转为红黑树过程中，建立了next和prev的指向，即建立好的红黑树后，结点之间通过next和prev可以遍历整颗树，这也是遍历过程值中不需要考虑红黑树的原因，如何建立红黑树，这里不细讲，下次分析红黑树中再讲。 接着来看resiz方法怎么来初始化和扩展table。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; // 判断长度是否大于0，长度为0的表示为被初始化 if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; // 初始化指定的初始化容量 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 当table为空时，需要初始化table，将如果指定了初始化容量，则其threshold做初始化长度，接着将threshold赋值为threshold*loadFactor，否则直接按照默认table长度为16，负载因子为0.75，阈值为16*0.75。如果已经初始化，则要对其进行扩容，将容量扩充为原来的两倍，扩充之后需要对散列表中的元素重新分配，在重新分配在java7是直接遍历元素进行重新计算散列值，但是在java8中进行了优化，将需要分配的链表或者红黑树分成两个链表，分别放入各自在新的表位置中。可以看到在代码中loHead和loTail、hiHead和hiTail分别是两个链表的头结点和尾结点，这里称为lo链表和hi链表，旧散列表中同一个链表中的元素按照hash值和旧表容量进行与运算，判断是否为0分为两类，放入两个链表中，最后两个链表分别放入新表的位置中。 其中的原理可以通过一个例子说明，假设一个HashMap，table容量为8，在余数为2的链上有key的hash值为2、10、18、26的元素。一开始元素确定在table中的位置是通过与n-1（例子中为7）取余的方式。$$n-1:00000111\\2\qquad:0000 0001\\result:00000001\\\quad\\n-1:00000111\\10\quad:0001 0001\\result:00000001$$扩展容量后为16，则其确定在散列表的中的位置同样与容量与运算（n-1变为15），如果重新与运算的话$$n-1:00001111\\2\qquad:0000 0001\\result:00000001\\\quad\\n-1:00001111\\10\quad:0001 0001\\result:00010001$$仔细观察的话，其实后四位的取与结果是一样的，不同的只有第五位上计算不同，而这取决于hash在第五位是0还是1，因此其实只要得到hash值的第五位是0还是1，就可以决定放入表中的位置。如果是0的话还是旧的位置，如果是1的话，则新位置为旧的位置加上8（2的3次方）。而确定第五位的值，只需要将hash值与旧容量值（8：0001 0000）取与运算就能得到 因此只需与oldCap取与运算，判断是否为0分两个链表，0的链表维持原来的位置，1的则加上oldCap的值作为新位置 对于红黑树的分裂放入方法也差不多 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950static final int UNTREEIFY_THRESHOLD = 6;sfinal void split(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, int index, int bit) &#123; TreeNode&lt;K,V&gt; b = this; // Relink into lo and hi lists, preserving order TreeNode&lt;K,V&gt; loHead = null, loTail = null; TreeNode&lt;K,V&gt; hiHead = null, hiTail = null; int lc = 0, hc = 0; for (TreeNode&lt;K,V&gt; e = b, next; e != null; e = next) &#123; next = (TreeNode&lt;K,V&gt;)e.next; e.next = null; if ((e.hash &amp; bit) == 0) &#123; if ((e.prev = loTail) == null) loHead = e; else loTail.next = e; loTail = e; ++lc; &#125; else &#123; if ((e.prev = hiTail) == null) hiHead = e; else hiTail.next = e; hiTail = e; ++hc; &#125; &#125; if (loHead != null) &#123; // 当链表长度小于等于6时，变成链表 if (lc &lt;= UNTREEIFY_THRESHOLD) tab[index] = loHead.untreeify(map); else &#123; tab[index] = loHead; // 当loHead为空的时候，表明所有元素全部在hiHead链表上，已经是一棵调整好的红黑树 if (hiHead != null) // (else is already treeified) loHead.treeify(tab); &#125; &#125; if (hiHead != null) &#123; if (hc &lt;= UNTREEIFY_THRESHOLD) tab[index + bit] = hiHead.untreeify(map); else &#123; tab[index + bit] = hiHead; if (loHead != null) hiHead.treeify(tab); &#125; &#125;&#125; 删除删除操作比较简单，相当于在查找的基础上删掉该元素。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null;&#125; 在jdk8和jdk7中的区别 jdk8中引入了红黑树，将链表长度超过8的转变为红黑树，jdk7中则只使用链表 jdk8中元素插入链表在链表的尾部，而jdk7中是插入到头部，jdk8将其插入到尾部并加入红黑树，解决了jdk7的HashMap在多并发下扩容时会陷入链表成环的问题 Jdk8在扩容重新分配时使用了新的方法，与就容量取与运算结果分为两个链表或树，而jdk7是重新计算hash取余的结果 非默认序列化HashMap 并没有使用默认的序列化机制，table变量被transient修饰，无法序列化，而是通过实现readObject/writeObject两个方法自定义了序列化的内容，因为序列化 talbe 存在着两个问题： table 多数情况下是无法被存满的，序列化未使用的部分，浪费空间 同一个键值对在不同 JVM 下，所处的桶位置可能是不同的，在不同的 JVM 下反序列化 table 可能会发生错误。 在不同的jvm下会有不同的HasCode，直接序列化生产对于后续加入元素会产生错误。 参考HashMap 源码详细分析(JDK1.8) - 个人文章 - SegmentFault 思否 HashMap为何从头插入改为尾插入 - 掘金]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ambaria安装HDP]]></title>
    <url>%2Fambaria%E5%AE%89%E8%A3%85HDP.html</url>
    <content type="text"><![CDATA[前言Hadoop的生态非常的庞大，如果要对其一一搭建，并配置好相应的配置文件，需要耗费很大的精力和时间。利用Ambari + HDP能帮助我们快速搭建Hadoop平台，同时Ambari提供了监控集群的功能，能在其上面方便的添加Hadoop生态的组件以及相关的分布式框架，如spark,kafka等等。 按照环境 五台Centos7的机器 java 8 python 2.7 安装HDP 3.1和ambari 2.7.3 官方的安装文档 如果之前有ambari和hdp的环境需要重装，则重装之前需要先清理环境，参考网上的一遍博客：完全卸载HDP和Ambari 首先机器之间需要免密登陆，免密的登陆操作这里就不细说了，网上有很多教程，在安装之前最好保证机器之间时间同步，可以使用ntp服务使机器之间的时间同步，安装使用教程可以查看之前的博客：简单集群时间同步 本地仓库由于ambari和HDP的yum下载太慢，因此直接下载配置本地资源 如果没有http服务，需要先安装http服务 12yum install httpdservice httpd start 下载ambari和HDP仓库地址:Ambari Repositories、HDP 3.1.0 Repositories,下载ambari、HDP、HDP-UTILS、HDP-GPL的tat.gz的压缩包，解压文件到/var/www/html/下 在/etc/yum.repos.d/下，新建ambari.repo，添加内容 12345678[AMBARI]name=AMBARIbaseurl=http://10.1.18.221/ambari/centos7/2.7.3.0-139path=/enabled=1gpgcheck=1gpgkey=http://10.1.18.221/ambari/centos7/2.7.3.0-139/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinspriority=1 新建HDP.repo，内容： 12345678[HDP-3.1]name=HDP-3.1baseurl=http://10.1.18.221/HDP/centos7/3.1.0.0-78path=/enabled=1gpgcheck=1gpgkey=http://10.1.18.221/HDP/centos7/3.1.0.0-78/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinspriority=1 新建HDP-UTILS.repo，内容： 12345678[HDP-UTILS-1.1.0.22]name=HDP-UTILS-1.1.0.22baseurl=http://10.1.18.221/HDP-UTILS/centos7/1.1.0.22path=/enabled=1gpgcheck=1gpgkey=http://10.1.18.221/HDP-UTILS/centos7/1.1.0.22/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinspriority=1 新建HDP-GPL.repo，内容： 1234567name=HDP-GPL-3.1.0.0baseurl=http://10.1.18.221/HDP-GPL/centos7/3.1.0.0-78path=/enabled=1gpgcheck=1gpgkey=http://10.1.18.221/HDP-GPL/centos7/3.1.0.0-78/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinspriority=1 新建yum的缓存 12yum clean allyum makecache 安装Ambari首先需要关闭SELinux 1234567# 查看SELinux状态getenforce# 临时关闭（不用重启机器）setenforce 0# 修改配置文件永久关闭，需要重启机器vim /etc/selinux/config# 将SELINUX=enforcing改为SELINUX=disabled 关闭iptables 1234#停止firewallsystemctl stop firewalld.service#禁止firewall开机启动systemctl disable firewalld.service 安装ambari 1yum install -y ambari-server 配置初始化ambari 1ambari-server setup 配置安装的详细内容可以查看官方文档，以下的回应安装提示差不多是官方文档的翻译。 回应安装提示： 如果您尚未暂时禁用SELinux，则可能会收到警告。接受默认值（y），然后继续。 默认情况下，Ambari Server运行在root。(n)在Customize user account for ambari-server daemon提示符下接受默认值，继续root。如果要创建另一个用户来运行Ambari服务器，或者要分配以前创建的用户，请y在Customize user account for ambari-server daemon提示符处选择，然后提供用户名。 如果您尚未暂时禁用iptables，则可能会收到警告。输入y继续。 选择要下载的JDK版本。 输入1以下载Oracle JDK 1.8。默认情况下，Ambari Server安装程序会下载并安装Oracle JDK 1.8和随附的Java Cryptography Extension（JCE）策略文件 输入2需要提供自己安装的jdk的java_home路径 出现提示时，请查看GPL许可协议。要明确启用Ambari下载和安装LZO数据压缩库，不明白LZO的作用可以直接y，反正多安装不是什么坏处 选择n在 Enter advanced database configuration以使用Ambari的默认嵌入式PostgreSQL数据库。默认的PostgreSQL数据库名称是ambari。默认用户名和密码是ambari/bigdata。否则，要使用现有的PostgreSQL，MySQL / MariaDB或Oracle数据库与Ambari，请选择y。 启动登陆ambari 如果安装顺利，可以直接启动ambari 1ambari-server start 接着可以访问http://&lt;your.ambari.server&gt;:8080来登陆ambari,初始用户名和密码都是admin 点击Launch Install Wizard开始集群配置 集群配置填写集群名字 选择HDP的版本，在这一步由于搭建了本地源，所以使用本地源的URI 填入集群的各个节点的主机名，不是IP,要保证/etc/hosts能正常解析到各个节点,可以保持一份正确的配置，所有节点都使用相同的hosts文件 123456789cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain610.1.18.221 master.hdu.edu.cn amdnode810.1.18.222 slave1.hdu.edu.cn node110.1.18.223 slave2.hdu.edu.cn node210.1.18.224 slave3.hdu.edu.cn node310.1.18.225 slave4.hdu.edu.cn node4 因此主机名填写如下 接着填好ambari所在主机的秘钥，进入下一步安装 接着选择安装的组件，配置组件，配置参数基本按照默认走，除了填写几个用户名和密码，就不细讲了。 建议：一开始最好不要安装大量组件，只把zookeeper,hdfs，yarn,mr2这些必要的组件安装，后期慢慢添加更多组件，如果一开始安装大量组件，需要一股脑解决大量问题，别问我怎么知道的。 安装过程中的错误 安装Hive Client报错“Failed to download file from http://master.hdu.edu.cn:8080/resources/mysql-connector-java.jar due to HTTP error: HTTP Error 404: Not Found” 解决方法：在master.hdu.edu.cn对应的节点下载关联好jdbc 1234yum install mysql-connector-java*ls -al /usr/share/java/mysql-connector-java.jarcd /var/lib/ambari-server/resources/ln -s /usr/share/java/mysql-connector-java.jar mysql-connector-java.jar 报错“ExecutionFailed: Execution of ‘ambari-python-wrap /usr/bin/hdp-select set xxx-client 3.1.0.0-78’ returned 1. symlink target /usr/hdp/current/xxx-client for tez already exists and it is not a symlink.” 解决方法，把软连接建立上去： 12rm -rf /usr/hdp/current/xxx-clientln -s /usr/hdp/3.1.0.0-78/xxx /usr/hdp/current/xxx-client 按照过程中最好联网，虽然建立了HDP的本地源，但是有些还是通过yum从远程仓库获取，如mariadb-server等 按照过程中会有很多错误，基本是查看错误的日志，排除错误原因，如yum安装不上，可以采用手动安装；缺少文件，可以从其他成功安装的集群获取文件；文件权限不对，无法读取，手动设置好需要的权限等。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Ambari</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop的搭建]]></title>
    <url>%2FHadoop%E7%9A%84%E6%90%AD%E5%BB%BA.html</url>
    <content type="text"><![CDATA[前言Hadoop的搭建此次分为伪分布式和分布式，伪分布式分为windows和mac Os、Linux 伪分布式Hadoop的伪分布式搭建需要提前安装好jdk1.8，选用hadoop3.0.0版本，官方提供的二进制和源码下载网址：https://archive.apache.org/dist/hadoop/common/hadoop-3.0.0/ ，此次的搭建使用二进制包安装，不涉及源码的编译，所以下载的文件为hadoop-3.0.0.tar.gz。 Windows首先将下载好hadoop-3.0.0.tar.gz压缩包，解压到合适的目录下（目录位置用于配置环境变量，用户可以自行选择），为了接下来方便配置的讲解，假设本次例子解压目录为：E:\hadoop-3.0.0，后文的hadoop的xx目录都是该目录下的子目录。 windows启动脚本在windows下安装hadoop,需要额外添加hadoop的window启动脚本，下载位置为：https://github.com/steveloughran/winutils ，选择相应的hadoop大版本，如本次安装为hadoop3.0.0。则需将winutils中hadoop-3.0.0\bin下的所有文件复制，粘贴到hadoop的bin目录下，即例子中的 E:\hadoop-3.0.0\bin中。 环境变量配置 修改hadoop-env.cmd文件 修改hadoop的etc\hadoop目录下hadoop-env.cmd文件，将文件中的set JAVA_HOME=%JAVA_HOME% 替换成 set JAVA_HOME=C:/Progra~1/Java/jdk1.8.0_144，用系统的JAVA_HOME的路径代替文件中JAVA_HOME的路径。 注意：本文只的jdk安装目录实际上为默认的C:/Program Files/Java/jdk1.8.0_144,但是该文件中参数的配置是不能出现空格的，如果含有空格，必须按照Windows 8.3 Pathname的路径规范书写，所以建议可以适当改变jdk的安装路径尽量不要包括空格等特殊符号。 添加环境变量 我的电脑 –&gt; 属性 –&gt; 高级系统设置中添加系统变量HADOOP_HOME，变量值为E:\hadoop-3.0.0，在path变量中添加%HADOOP_HOME%\bin; %HADOOP_HOME%\sbin; Hadoop配置文件 修改hadoop的etc\hadoop目录下core-site.xml文件，添加以下配置 123456789101112131415&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置Hadoop临时目录文件 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:///E:/hadoop-3.0.0/data/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///E:/hadoop-3.0.0/data/name&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hadoop的etc\hadoop目录下hdfs-site.xml文件，添加以下配置 1234567891011121314151617181920&lt;configuration&gt; &lt;property&gt; &lt;!-- hdfs的文件的默认副本数 --&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///E:/hadoop-3.0.0/data/dfs/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///E:/hadoop-3.0.0/data/dfs/datanode&lt;/value&gt; &lt;/property&gt; &lt;!-- 关闭权限检查--&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hadoop的etc\hadoop目录下mapred-site.xml文件，添加以下配置 1234567&lt;configuration&gt; &lt;property&gt; &lt;!-- mapreduce 运行在yarn上 --&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hadoop的etc\hadoop目录下yarn-site.xml文件，添加以下配置 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;127.0.0.1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动hadoop第一次需要格式化hdfs,使用命令hadoop namenode -format 打开cmd,输入start-dfs和start-yarn命令分别启动hdfs和yarn。 访问http://localhost:8088 和http://localhost:9870 可以看到hdfs和yarn的web界面 运行wordcount运行hadoop自带的wordcount例子，首先可以通过web界面向hdfs上传一个example.txt文件在新建的/input目录下,内容如下: 12345A Grain of SandTo see a world in a grain of sandAnd a heaven in a wild fllowerHold infinity in the palm of your handAnd eternity in an hour 运行wordcount的jar（在hadoop的share\hadoop\mapreduce目录下的hadoop-mapreduce-examples-3.0.0.jar）来统计本件中单词的数量 1E:\java\hadoop-3.0.0\share\hadoop\mapreduce&gt;hadoop jar hadoop-mapreduce-examples-3.0.0.jar wordcount /input/example.txt /out 最后可以看到hdfs上/out目录下part-r-00000的文件，内容为example文件的单词的统计 1234A 1And 2Grain 1... Mac和Linux下载解压安装包tar -zxvf hadoop-3.0.0.tar.gz。 环境变量配置 修改hadoop-env.cmd文件 修改hadoop的etc\hadoop目录下hadoop-env.cmd文件，将文件中的set JAVA_HOME=%JAVA_HOME% 替换成 set JAVA_HOME=，用系统的JAVA_HOME的路径代替文件中JAVA_HOME的路径。 添加环境变量 修改~/.bash_profile文件,在最后添加hadoop的环境变量 123vim ~/.bash_profileexport HADOOP_HOME=/Users/Cyan/coding/hadoop/hadoop-2.8.2export PATH=$PATH:$HADOOP_HOME/bin Hadoop配置文件配置文件几乎和windows的一样，除了路径的书写，根据mac和linux的路径书写来 启动Hadoop和运行wordcount和windows的一样，不重复了 遇到问题 针对mac,启动HDFS时，start-dfs.sh,报错“connection refused”，则需要在计算机系统设置中打开远程登录许可。 点击 Sharing（共享）： 勾选 Remote Login（远程登录），然后添加当前用户： 运行mapreduce程序。如wordcound,报错“错误:“找不到或无法加载主类 org.apache.hadoop.mapreduce.v2.app.MRAppMaster” 在yarn-site.xml中添加名为yarn.application.classpath的属性，值为hadoop classpath命令的输出结果 12hadoop classpath/Users/zhongyue/hadoop-3.0.0/etc/hadoop:/Users/zhongyue/hadoop-3.0.0/share/hadoop/common/lib/*:/Users/zhongyue/hadoop-3.0.0/share/hadoop/common/*:/Users/zhongyue/hadoop-3.0.0/share/hadoop/hdfs:/Users/zhongyue/hadoop-3.0.0/share/hadoop/hdfs/lib/*:/Users/zhongyue/hadoop-3.0.0/share/hadoop/hdfs/*:/Users/zhongyue/hadoop-3.0.0/share/hadoop/mapreduce/*:/Users/zhongyue/hadoop-3.0.0/share/hadoop/yarn:/Users/zhongyue/hadoop-3.0.0/share/hadoop/yarn/lib/*:/Users/zhongyue/hadoop-3.0.0/share/hadoop/yarn/* 123456vim yarn-site.xml&lt;property&gt; &lt;name&gt;yarn.application.classpath&lt;/name&gt; &lt;value&gt;/Users/zhongyue/hadoop-3.0.0/etc/hadoop:/Users/zhongyue/hadoop-3.0.0/share/hadoop/common/lib/*:/Users/zhongyue/hadoop-3.0.0/share/hadoop/common/*:/Users/zhongyue/hadoop-3.0.0/share/hadoop/hdfs:/Users/zhongyue/hadoop-3.0.0/share/hadoop/hdfs/lib/*:/Users/zhongyue/hadoop-3.0.0/share/hadoop/hdfs/*:/Users/zhongyue/hadoop-3.0.0/share/hadoop/mapreduce/*:/Users/zhongyue/hadoop-3.0.0/share/hadoop/yarn:/Users/zhongyue/hadoop-3.0.0/share/hadoop/yarn/lib/*:/Users/zhongyue/hadoop-3.0.0/share/hadoop/yarn/* &lt;/value&gt;&lt;/property&gt; 分布式hadoop分布式运行环境搭建是使用virtual Box创建三台Ubuntu虚拟机上进行的，所以会有一些ubuntu的一些配置，方便后续Hadoop的安装 更改apt源由于apt原来的源太慢，改为国内阿里的源 1234567891011121314151617181920# 备份原来的文件sudo mv /etc/apt/sources.list /etc/apt/source.list.bak# 编辑源列表文件sudo vim /etc/apt/sources.list# 添加内容deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-proposed main restricted universe multiversedeb http://archive.canonical.com/ubuntu/ xenial partnerdeb http://extras.ubuntu.com/ubuntu/ xenial main# 更新源sudo apt-get updatesudo apt-get upgrade 固定IP地址由于虚拟机使用的是桥接网络，固定ip方便以后ssh连接 首先查看虚拟机的网卡 12345678910111213ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 08:00:27:42:48:cc brd ff:ff:ff:ff:ff:ff inet 192.168.31.112/24 brd 192.168.31.255 scope global enp0s3 valid_lft forever preferred_lft forever inet6 fe80::a00:27ff:fe42:48cc/64 scope link valid_lft forever preferred_lft forever 可以看到这里的网卡编号为enp0s3,接下来编辑文件 sudo vi /etc/network/interfaces,写入静态ip地址(address)，子网掩码(netmask)和网(gateway)，配置固定的DNS,dns-nameserver后面接DNS服务器地址，可根据不同地域和网络网上查询得到，8.8.8.8万能的dns,修改sudo vim /etc/resolv.conf可以改变dns,但是重启失效。最后重启网络服务 1234567891011sudo vim /etc/network/interfacesauto loiface lo inet loopbackauto enp0s3iface enp0s3 inet staticaddress 192.168.31.112netmask 255.255.255.0gateway 192.168.31.1dns-nameservers 8.8.8.8# 重启网络服务sudo /etc/init.d/networking restart ssh安装启动1234567sudo apt-get install openssh-serversudo /etc/init.d/ssh startsudo /etc/rc.local# /etc/rc.local中exit 0前加入/etc/init.d/ssh start设置/etc/init.d/ssh start# 生产ssh keyssh-keygen 将集群之间的公钥保存在authorized_keys中，形成集群机器之间的免密登陆 JDK配置下载jdk文件，jdk-8u191-linux-x64.tar.gz 解压该文件到指定目录 12mkdir /usr/lib/Javatar -zxf jdk-8u191-linux-x64.tar.gzz -C /usr/lib/Java/ 接下来配置java的环境变量,在/etc/profile的最后添加环境变量 123456vim /etc/profileexport JAVA_HOME=/usr/lib/Java/jdk1.8.0_191export JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$CLASSPATHexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH 最后查看是否配置成功 1234java -versionjava version "1.8.0_191"Java(TM) SE Runtime Environment (build 1.8.0_191-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode) Hadoop安装下载解压Hadoop 3.0的安装包12wget http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.1.1/hadoop-3.1.1.tar.gztar -zxvf hadoop-3.1.1.tar.gz 添加环境变量修改/etc/profile，根据hadoop解压后的位置添加HADOOPCONF_DIR、HADOOP_HOME、HADOOP_HDFS_HOME，并将$HADOOP_HOME/bin和$HADOOP_HOME/sbin`追加入到path中， 12345$ vim /etc/profileexport HADOOPCONF_DIR=/usr/hadoop3.1/hadoop-3.1.1/etc/hadoopexport HADOOP_HOME=/usr/hadoop3.1/hadoop-3.1.1export HADOOP_HDFS_HOME=/usr/hadoop3.1/hadoop-3.1.1export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH 使修改的环境变量生效 1$ source /etc/profile 修改host集群域名和ip映射，方便之后配置 1234567$ cat /etc/hosts127.0.0.1 localhost# 127.0.1.1 hdp-310.1.16.111 hdp-110.1.16.112 hdp-210.1.16.113 hdp-3 这里是三台机器组成集群，ip分别是10.1.16.111/112/113，对应域名hdp-1/hdp-2/hdp-3（分别是三台机器的hostname，也可以随意取名 ），这个需要按照实际ip和hostname 来 修改hadoop的配置文件修改hadoop-env.sh${HADOOP_HOME}/etc/hadoop/hadoop-env.sh，增加JAVA_HOME，同java环境变量中的相同 1export JAVA_HOME=/usr/lib/Java/jdk1.8.0_191 core-site.xml配置${HADOOP_HOME}/etc/hadoop/core-site.xml配置hdfs中NameNode的URI(包括协议、主机名称、端口号),从上述选取一台作为namenode的节点 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hdp-1:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml配置${HADOOP_HOME}/etc/hadoop/hdfs-site.xml，配置hafs中文件的备份数，以及namenode，datanode的存储位置，需要先创建namenode，datanode的存储位置，即下面的/data/hdp/namenode和/data/hdp/datanode 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/data/hdp/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/data/hdp/datanode&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml配置${HADOOP_HOME}/etc/yarn-site.xml,配置resourcemanager运行的机器，为了能够运行MapReduce程序，需要让各个NodeManager在启动时加载shuffle server，接着是resourcemanager和jobhistory的UI界面的地址，开启日志聚集和保存时间 12345678910111213141516171819202122232425262728293031323334&lt;configuration&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hdp-1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt; &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;hdp-1:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hdp-1:19888&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;640800&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml${HADOOP_HOME}/etc/mapred-site.xml，配置mapreduce运行在yarn上 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; workers${HADOOP_HOME}/etc/worker，将集群中的所有机器添加上去 123hdp-1hdp-2hdp-3 启动Hadoop首次启动需要格式化namenode 12345678# 格式化hdfs namenode -format# 启动hdfs,可以使用start-all.sh来启动hdfs和yarnstart-dfs.sh# 启动yarnstart-yarn.sh# 如果想看jobhistory，可以启动一下服务mr-jobhistory-daemon.sh start historyserver 可以通过jps查看启动了哪些组件 123456789101112# hdp-1，由于namenode和resourcemanager在其上运行11939 Jps10756 SecondaryNameNode11173 NodeManager11045 ResourceManager10581 DataNode10454 NameNode7148 JobHistoryServer# hdp-2,hdp-328848 NodeManager29491 Jps28719 DataNode 通过配置的yarn.resourcemanager.webapp.address和mapreduce.jobhistory.webapp.address,可以看到yarn的信息界面，hdfs的信息界面默认在运行namenode机器的9870端口。 以上配置只是能让Hadoop够运行以及能看到基本信息，实际生产中还需要进行更多配置，有兴趣可以从官方查看各个配置参数。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[InnoDB数据页结构及其与聚簇索引的关系]]></title>
    <url>%2FInnoDB%E6%95%B0%E6%8D%AE%E9%A1%B5%E7%BB%93%E6%9E%84%E5%8F%8A%E5%85%B6%E4%B8%8E%E8%81%9A%E7%B0%87%E7%B4%A2%E5%BC%95%E7%9A%84%E5%85%B3%E7%B3%BB.html</url>
    <content type="text"><![CDATA[表空间InnoDB中数据都存放在一个空间中，就是表空间。在文件系统中就是idb文件，每个idb文件都是一个表空间。它们之间通过表空间id来区分，在默认情况下，InnoDB使用的是共享表空间，所有数据存放在一个共享表空间ibdata1中。共享表空间可以通过参数innodb_data_file_path进行配置。 可以同时配置多个文件组成一个共享表空间，如 1innodb_data_file_path=/data/ibdata1:2000M;/data/ibdata2:2000M:autoextend 这里配置了/data/ibdata1和/data/ibdata2两个文件组成表空间，如果两个文件在不同磁盘上，能降低各个磁盘的负载，可以提高数据库的性能。同时，两个文件名后跟了存储大小，表示文件的最大存放空间，而autoextend表示/data/ibdata2若用完了2000M，文件可以自动增长。 实际上，除了共享表空间还有独立表空间，将参数innodb_file_pre_table设为ON可以开启独立表空空间。在独立表空间中，对于每张表内的数据都单独放在一个表空间中，但是每个表空间只会对应表的数据、索引和插入缓冲Bitmap页，其余数据，例如回滚日志，事务信息，二次缓存等还是存放在共享表空间。 表空间由段(segment)、区(extent)、页(page)组合。 数据页页是InnoDB存储引擎管理数据库的最小磁盘单位。默认一个页的大小是16K,InnoDB 1.0.x后可以通过参数KEY_BLOCK_SIZE设置默认页大小为2K,4K,8K，InnoDB 1.2.x新增参数innodb_page_size可以设置默认页大小为4K,8K。 数据页由File Header、Page Header、Infimun和Supremum Records、User Records、Free Space、Page Directory、File Trailer组合。其中File Header、Page Header和File Trailer的大小是固定的，分别占用38、56、8个字节，记录数据页的信息。而Infimun、Supremum Records、User Records、Free Space、Page Directory存储的信息和实际的行数据有关，因此大小是动态的。 File HeaderFile Header固定38个字节，用来记录页的头信息。 File Header中的FIL_PAGE_PREV和FIL_PAGE_NEXT分别是记录了上一页和下一页在表空间中的位置，因此页与页之间是通过链表的数据结构连接再一起，而且是一个双向链表。 Page HeaderPage Header接着File Header之后，固定56字节，记录数据页的状态信息。 Infimun和Supremum RecordsInfimun和Supremum Records是虚拟的两个行记录，用来限定实际数据行的边界。Infimun是比页中任何主键值都小的值，而Supremum是比任何主键值都要大的值。这两值在页被创建时就被建立，不会被删除。 User Records存储实际行记录的内容。数据库中一行数据对应文件中的一行记录格式。目前InnoDB只要有两种行记录格式：Compact和Redundant，Compacts是mysql 5.1之后的默认行记录格式，Redundant是为了兼容之前的版本而保留。接下来分别介绍这两种行记录格式。 Compact行记录格式Compact行记录格式能够高效的存储行记录，一页中存放的行数据越大，性能就越高。Compact的存储格式如下图： 变成字段长度列表记录了行记录中变长字段(varchar)的长度，其按照列的顺序逆序放置，一个变长字段的长度最大不会超过2个字节，因为varchar类型的最大长度为65535，正好是2的16次方，因此最多两个字节就能记录下该字段的长度。如果该列的长度小于255字节，则用1个字节表示，如果大于255字节，则用2个字节表示。 例如，创建一张表z,包括三个变长字段，一个固定长度字段，则其对对应的变长字段列表如下 12345678910111213CREATE TABLE test(t1 VARCHAR(10),t2 VARCHAR(10),t3 CHAR(10),t4 VARCHAR(10)ENGINE=INNODB CHARSERT=LATIN1 ROW_FORMAT=COMPAT);-- 第一行INSERT INTO test VALUES(&apos;a&apos;, &apos;bb&apos;, &apos;ccc&apos;, &apos;dddd&apos;);-- 第二行INSERT INTO test VALUES(&apos;e&apos;, NULL, &apos;ff&apos;, &apos;gggg&apos;);-- 第三行INSERT INTO test VALUES(&apos;e&apos;, NULL, NULL, &apos;gggg&apos;); 第一行对应的变长字段长度列表为04 03 01(dddd,ccc,a)，第二行04 02 01(gggg,ff,e)，第三行为04 01(gggg,e)，因为t3为固定长度字段，所以不会记录其长度，对已NULL的字段也是不会记录，比如第三行的t2字段。 NULL标志占1个字节，表示该行记录有无NULL值，有则为1，无为0。字段中的NULL值实际上不记录在compact格式中，所以NULL数据实际上除了占有NULL标志位以外不占用任何存储空间。 记录头信息固定占5个字节，详细表示记录的信息如下图 从行记录头中的next_record可以看出行与行之间同样是链表的方式存储，因此一页之间的行与行其实不需要按照顺序存储，实际上行存储顺序可能是无序的，但是通过链表将其按照主键顺序串连在一起。n_owned是和下文要讲的Page Dirextory有关，帮助在页中查找行记录。 接下来分别是事务ID和回滚指针，分别占用6字节和7字节，如果InnoDB表没有指定主键也没有非空唯一索引，则会在事务ID前增加一个自动创建的6字节的主键ID。 最后存储的就是各个字段上实际数据。 Redundant行记录格式Redundant行记录格式是为了兼容之前版本的页格式，其存储格式如下： 地段长度偏移列表表示各个字段（包括主键ID,事务ID,回滚指针，列数据，不包括记录头信息）距离记录头信息开始的偏移量。 redundant记录头信息占6个字节，格式如下： 接下来的信息格式和Compact行记录格式一样，除了对NULL值的处理，在Compact中除了NULL标志外实际上是不存储NULL值，但是redundant中NULL是记录其内容，具体内容就是用00填充其固定字段长度，对于变长字段则同样不占用任何存储空间，即不记录变长字段NULL值。 Free SpaceFree Space指的是页中的空闲空间，同样是链表的数据结构，当一行被删除之后，该空间就会被加入Free Space当中。 Page DirectoryPage Directory也叫页目录，存放的是记录在页中的相对位置。这些记录指针被称为slots（槽）。InnoDB的页中的并不是为每一条记录存放在槽，InnoDB实际上采用的是稀疏目录，即一个槽中可能存放的是多个记录。slots中的记录是按照主键索引值逆序排放的，每个槽占两个字节，通过槽中的指针找到对应行记录的位置，行记录头中的n_owned记录了该记录拥有的记录数，即槽中存放的记录数。 slots中的记录是按照主键索引值逆序排放的，通过槽使用二分查找可以快速的找到页中相应行记录的位置。例如，页中记录的主键值有{g, i, c, k, e, f, a, h, b, j, l, d}，假设一个槽存放4条记录，则按照主键排序的逆序，槽中的记录可能是{a, e, i} 通过槽使用二分查找查找到的结果只是一个粗略的结果，需要获得准确的结果，还需通过next_record顺着链表最多查看n_owned条记录是否符合要求。 实际上通过InnoDB查找记录的顺序是首先通过B+书找到相应的页，接着将页加载到内存中，在页中通过Page Directory利用二分查找找到粗略的位置，最后通过链表找到记录。只不过在内存中二分查找的过程很快，一般忽略这部分的查找时间。 File TrailerFile Trailer是为了检测页是否完整的写入磁盘而设置的，其中包含checksum值，通过相应的checknum函数进行比较，检测页的完整性。 InnoDB数据页与索引聚簇索引InnoDB针对每张表的主键都会构造一颗B+树，即聚簇索引（Clusterd Index），所以InnoDB的每张表都有聚簇索引，就是其在主键上创建的索引。索引结构同B+树的数据结构相同，但与辅助索引不同的是其叶子节点存放的是所有的数据，即聚簇索引叶子节点实际上就是数据页，叶子节点之间通过双向链表有序的连接在一起，即上文每个数据页的File Header都有FIL_PAGE_PREV和FIL_PAGE_NEXT记录上一页和下一页的位置。聚簇索引在物理存储上并不是按照顺序的存放，而是通过其双向链表和数据页中的行记录格式的行链表实现了逻辑上的顺序存放，沿着链表查看是顺序存放的。 因此通过聚簇索引查找实际上分三种查找方式： 根据B+树查找到对应的数据页 在数据页中根据Page Directory的稀疏目录找到粗略的行记录位置 z最后根据next_record在链表上顺序查找 辅助索引在InnoDB表的非主键字段上建立的B+树索引也称为辅助索引（Secondary Index）。辅助索引结构同样是B+树的结构，但是其叶子节点存放的不是数据页，而是索引的键值加上主键值。 通过辅助索引可以查找到相应的主键值，接着利用主键值在聚簇索引上找到实际数据，即我们通过辅助索引查找实际上是通过两颗B+树查找。 与非聚簇索引的比较例如MYISAM存储引擎使用的是非聚簇索引，不同于聚簇索引的是其叶子节点存放的是数据存放的地址。其主索引和辅助索引除了键值不同外并没有什么区别，叶子节点都是使用指针指向真正的数据地址。 对于离散的查找，聚簇索引要比非聚簇索引慢一些，因为聚簇索引要走两颗B+树的I/O。 在数据的更新上，特别是更新影响到物理地址的变化，聚簇索引要比非聚簇索引，因为非聚簇索引需要改变所有索引上相应叶子节点的物理地址，而聚簇索引叶子节点直接存放数据，辅助索引存放的是主键值，无需改变。 对于排序和范围查找，聚簇索引将数据按照逻辑顺序排列，能够通过一行记录的位置快速找到其相应的周围的记录，明显比非聚簇索引好。]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>数据页</tag>
        <tag>聚簇索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[布隆过滤器-Bloom Filter]]></title>
    <url>%2F%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8-Bloom-Filter.html</url>
    <content type="text"><![CDATA[简介​ Bloom Filter是由 Burton Howard Bloom在1970提出的，用来判断一个元素是否存在集合中的概率算法。经过Bloom Filter判断过不在集合中的元素就一定不在集合中，若判断结果是在集合中，则可能会误判，即该元素可能不在集合中。换句话说，可能会误判，但绝不可能漏判。元素可以添加到集合中，但不会被删除（尽管可以通过“计数”过滤器来解决）; 添加到集合中的元素越多，误判的概率就越大。Bloom Filter将元素映射到位数组中，所以极大的节省空间。 算法描述​ 通常我们判断一个元素是否在集合中，可以将集合存放在一个列表等数据结构中，通过比对查找的方式判断是否包含该元素，但是当元素的个数越来越大时，列表占用的内存空间会越来越大，可以将其存放在磁盘空间中，但是这样会大大降低查询时间。进一步改进，可以通过散列表来映射元素到二进制位数组中，真正的数据存放磁盘空间中，内存存放散列表，通过元素散列映射的位是否为1判断元素是否存在集合中。但是散列表当元素增加时，发生冲突的概率也不断增大，冲突会降低查询的效率，为了减少冲突，可以通过多个哈希函数解决，当一个元素通过多个哈希函数映射的位都为1时，那么判断元素可能在集合中，否则元素必定不在元素中。Bloom Filter就是使用这种方法。 初始化​ Bloom Filter是通过不同的哈希函数映射到位数组，初始化是一个位数组，数组元素全设置为0，代表目前没有元素在集合中，数组的初始化长度下文会介绍。 插入元素​ 插入的元素需要通过k个哈希函数得到k个对应的位数组上的位置，将这些位置上的位全都设置为1，若已经设置为1，则可以不做任何改变。 如图将集合中的x，y，z通过三个哈希函数得到的位数组上对应的位都设置为1。 判断元素是否在集合中​ 将需要判断的元素同样通过k个哈希函数得到k个对应的位数组上的位置，判断其位置上的位是否全为1，若是则元素可能存在在集合中，否则必定不存在。如上图的w通过哈希函数得到的位置上的位有一个不为1，则w必定不存在该集合中。 参数的选择Bloom Filter主要有四个参数: $k$：哈希函数的个数 $m$：位数组的大小 $n$：集合的元素个数 $p$：假阳性概率（false positive probability），即将不在集合中的元素判断为在集合中，误判的概率 哈希函数的数量$k$必须是正整数。如果不考虑这个约束，则对已给定的$m$和$n$或$p$，当$k$满足以下公式时误判的概率最小：$$k = -\frac{lnp}{ln2} = \frac{m}{n}ln2$$而由$n$和$p$可以得到最佳的$m$的值：$$m = -\frac{nlnp}{(ln2)^2}$$所以每个元素的最佳位数是：$$\frac{m}{n}=-\frac{lnp}{(ln2)^2}\approx-1.44log_2p$$ 实现Bloom Filter的简单实现，参考：https://github.com/MagnusS/Java-BloomFilter 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147import java.io.Serializable;import java.nio.charset.Charset;import java.security.MessageDigest;import java.security.NoSuchAlgorithmException;import java.util.BitSet;import java.util.Collection;public class BloomFilter&lt;E&gt; implements Serializable &#123; int k; //hans函数的个数 int m; //bit的位数 int n; //存储的预期容量 int size; //实际存储的容量 BitSet bitSet; static final Charset CHARSET = Charset.forName("utf-8"); static final String hashName = "MD5"; static final MessageDigest MESSAGE_DIGEST; static &#123; MessageDigest tmp; try &#123; tmp = MessageDigest.getInstance(hashName); &#125;catch (NoSuchAlgorithmException e)&#123; tmp = null; &#125; MESSAGE_DIGEST = tmp; &#125; /** * 根据假阳性概率和预计个数计算bit数组的大小 * @param falsePositiveProbability 假阳性概率 * @param n 预期的集合的个数 * @return */ private static int calculateM(double falsePositiveProbability, int n)&#123; return (int)Math.ceil(-1.44*Math.log(falsePositiveProbability)/Math.log(2.0) * n); &#125; /** * 根据假阳性概率和预计个数计算散列函数的个数 * @param falsePositiveProbability 假阳性概率 * @param n 预期的集合的个数 * @return */ private static int calculateK(double falsePositiveProbability, int n)&#123; return (int) Math.round(calculateM(falsePositiveProbability, n)/ (double)n * Math.log(2.0)); &#125; /** * 根据bit数组的大小和预计个数计算散列函数的个数 * @param m * @param n * @return */ private static int calculateK(int m, int n)&#123; return (int) Math.round(m/(double)n * Math.log(2.0)); &#125; public BloomFilter(double falsePositiveProbability, int n)&#123; this(calculateK(falsePositiveProbability, n), calculateM(falsePositiveProbability, n), n); &#125; public BloomFilter(int m, int n)&#123; this(calculateK(m, n), m, n); &#125; public BloomFilter(int k, int m, int n)&#123; this.k = k; this.m = m; this.n = n; bitSet = new BitSet(this.m); size = 0; System.out.println("bloom filter init success, K: " + k + " m: " + m + " n： " + n); &#125; /** * 得到相应的hash * @param data * @param hashNum * @return */ public static int[] getHashs(byte[] data, int hashNum)&#123; int[] result = new int[hashNum]; int k = 0; byte salt = 0; while (k &lt; hashNum)&#123; byte[] digest; synchronized (MESSAGE_DIGEST)&#123; MESSAGE_DIGEST.update(salt); salt++; digest = MESSAGE_DIGEST.digest(data); &#125; for(int i = 0; i &lt; digest.length/4 &amp;&amp; k &lt; hashNum; i++)&#123; int h = 0; for(int j = i*4; j &lt; (i*4) + 4;j++)&#123; h &lt;&lt;= 8; h |= ((int) digest[j] &amp; 0xFF); &#125; result[k] = h; k++; &#125; &#125; return result; &#125; /** * 添加元素到过滤器中 * @param element */ public void add(E element)&#123; int[] hashCodes = getHashs(element.toString().getBytes(CHARSET), this.k); for(int hashCode: hashCodes)&#123; bitSet.set(Math.abs(hashCode % this.m), true); size ++; &#125; &#125; public void addAll(Collection&lt;E&gt; collection)&#123; for(E element: collection)&#123; add(element); &#125; &#125; /** * 判断元素是否在集合中 * @param element * @return */ public boolean contains(E element)&#123; int[] hashCodes = getHashs(element.toString().getBytes(CHARSET), this.k); for(int hashCode: hashCodes)&#123; if(! bitSet.get(Math.abs(hashCode % this.m)))&#123; return false; &#125; &#125; return true; &#125; public boolean containsAll(Collection&lt;E&gt; collection)&#123; for(E element: collection)&#123; if(! contains(element))&#123; return false; &#125; &#125; return true; &#125;&#125; 应用 爬虫中的url去重 Hbase中通过Bloom Filter来快速确定查询的数据是否在HFile中，加快查询速度]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Bloom Filter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[超平面]]></title>
    <url>%2F%E8%B6%85%E5%B9%B3%E9%9D%A2.html</url>
    <content type="text"><![CDATA[什么是超平面数学中的超平面以下内容均来自：https://www.cnblogs.com/dengdan890730/p/5554787.html 在数学中，超平面（Hyperplane）是n维欧氏空间中余维度等于1的线性子空间。这是平面中的直线、空间中的平面之推广. $n$维超平面的方程定义为： ​ $a_1x_1+…+a_nx_n=b$，其中$a_1,…,a_n$是不全为0的常熟 ​ 即$w^Tx+b=0，$ 其中，$w$与$x$都是$d$维列向量，$x=(x_1,x_2,…,x_n)$为超平面上的点，$w=(w_1,w_2,…,w_n)$为平面上的法向量,$b$是一个实数, 代表平面与原点之间的距离. 我们最常见的平面概念是在三维空间中定义的:$Ax+By+cZ+D=0$ 超平面有两个性质： 方程是线性的: 是空间点的各分量的线性组合 方程数量为1 $d$维空间中的超平面其实就是维度比所在空间低一维的平面，即$d-1$维。例如3维空间的超平面是二维平面，二维空间的超平面是一条直线，一维空间的超平面是一个点. 点到超平面的距离假设点$x′$为超平面$A:w^Tx+b=0$上的任意一点, 则点$x$到$A$的距离为$x−x′$在超平面法向量$w$上的投影长度:$$d=\frac{|w^T(x−x′)|}{||w||}=\frac{|wTx+b|}{||w||}$$ 超平面的正面与反面一个超平面可以将它所在的空间分为两半, 它的法向量指向的那一半对应的一面是它的正面, 另一面则是它的反面. 判断一个点是在超平面的正面还是反面(面向的空间里)还是要用到它的法向量$w$.仍然假设点$x′$为超平面$A:wTx+b=0A:wTx+b=0$上的任意一点, 点$x$为待判断的点.若$x−x′$与$w$的夹角小于$90^o$, 则$x$在$A$的正面, 否则在反面$$w^T(x−x′)&gt;0→w^Tx+b&gt;0$$所以判定依据为:$$x在A的=\begin{cases}正面,\quad\quad w^Tx+b&gt;0\\平面上,\quad w^Tx+b=0\\反面,\quad\quad wTx+b&lt;0x\\\end{cases}$$若将距离公式中分子的绝对值去掉, 让它可以为正为负. 那么, 它的值正得越大, 代表点在平面的正向且与平面的距离越远. 反之, 它的值负得越大, 代表点在平面的反向且与平面的距离越远. 投影到超平面对于$n$维空间上的向量$x = (x_1,x_2,…,x_n)$投影到一个法向量为$w$的超平面上的投影向量$x’$为：$$x’=x-w^Txw$$]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>超平面</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase客户端的写缓存BufferedMutator]]></title>
    <url>%2FHBase%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E5%86%99%E7%BC%93%E5%AD%98BufferedMutator.html</url>
    <content type="text"><![CDATA[客户端的写缓存HBase的每一个put操作实际上是一个RPC操作，将客户端的数据传输到服务器再返回结果，这只适用于小数据量的操作，如果数据量多的话，每次put都需要建立一次RPC的连接（TCP连接），而建立连接传输数据是需要时间的，因此减少RPC的调用可以提高数据传输的效率，减少建立连接的时间和IO消耗。 HBase的客户端API提供了写缓存区，put的数据一开始放在缓存区内，当数量到达指定的容量或者用户强制提交是才将数据一次性提交到HBase的服务器。这个缓冲区可以通过调用 HTable.setAutoFlush(false) 来开启。而新版HBbase的API中使用了BufferedMutator替换了老版的缓冲区，通过BufferedMutator对象提交的数据自动存放在缓冲区中。 BufferedMutatorBufferedMutator通过mutate方法提交数据，flush方法可以强制刷新缓冲区提交数据，在执行close方法之前也会刷新缓冲区。 BufferedMutator是通过设定BufferedMutator.ExceptionListener监听器来异步处理异常，重写onException来实现异常处理，该监听器用来监听接受服务器端发送回来的错误消息。 用户可以通过设定BufferedMutatorParams的来定制符合要求的BufferedMutator。比如缓冲区的大小通过BufferedMutatorParams中的writeBufferSize方法设置（缓冲区的大小也可以通过配置文件的 hbase.client.write.buffer设置，值为long类型，单位为byte），异常监听器也是在BufferedMutatorParams中设置。 官方例子分析官方给出了BufferedMutator的使用例子，通过分析代码可以了解到BufferedMutator的使用 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class BufferedMutatorExample extends Configured implements Tool &#123; private static final Logger LOG = LoggerFactory.getLogger(BufferedMutatorExample.class); //线程池的大小 private static final int POOL_SIZE = 10; //线程的数量 private static final int TASK_COUNT = 100; private static final TableName TABLE = TableName.valueOf("tanle_name"); private static final byte[] FAMILY = Bytes.toBytes("f"); @Override public int run(String[] args) throws InterruptedException, ExecutionException, TimeoutException &#123; //异常监听器的建立,当写入异常是触发该监听器 final BufferedMutator.ExceptionListener listener = new BufferedMutator.ExceptionListener() &#123; @Override public void onException(RetriesExhaustedWithDetailsException e, BufferedMutator mutator) &#123; for (int i = 0; i &lt; e.getNumExceptions(); i++) &#123; LOG.info("Failed to sent put " + e.getRow(i) + "."); &#125; &#125; &#125;; //创建BufferedMutatorParams对象，设置监听器 BufferedMutatorParams params = new BufferedMutatorParams(TABLE) .listener(listener); //可以改动缓冲区的大小,如下面设置缓冲区大小为4M params.writeBufferSize(4*1023*1024); /** 创建连接和根据BufferedMutatorParams创建BufferedMutator * 这里用到java 7的新特性 try-with-resources */ try (final Connection conn = ConnectionFactory.createConnection(getConf()); final BufferedMutator mutator = conn.getBufferedMutator(params)) &#123; /** 线程池的建立，运行线程不断put数据 */ final AtomicInteger count = new AtomicInteger(1); final ExecutorService workerPool = Executors.newFixedThreadPool(POOL_SIZE); List&lt;Future&lt;Void&gt;&gt; futures = new ArrayList&lt;&gt;(TASK_COUNT); for (int i = 0; i &lt; TASK_COUNT; i++) &#123; futures.add(workerPool.submit(new Callable&lt;Void&gt;() &#123; @Override public Void call() throws Exception &#123; Integer value = count.getAndIncrement(); Put p = new Put(Bytes.toBytes("task " + value)); p.addColumn(FAMILY, Bytes.toBytes("someQualifier"), Bytes.toBytes("task " + value + " info")); mutator.mutate(p); return null; &#125; &#125;)); &#125; // 结束线程和线程池 for (Future&lt;Void&gt; f : futures) &#123; f.get(5, TimeUnit.MINUTES); &#125; workerPool.shutdown(); &#125; catch (IOException e) &#123; LOG.info("exception while creating/destroying Connection or BufferedMutator", e); &#125; return 0; &#125; public static void main(String[] args) throws Exception &#123; ToolRunner.run(new BufferedMutatorExample(), args); &#125;&#125; 上面的官方例子就是启动线程不断提交数据，BufferedMutator中缓冲区可以避免频繁的调用RPC，在批处理数据时及其重要，并且BufferedMutator的mutate操作是异步的，所以不会产生阻塞，这在Map-Reduce作业有很好的使用，BufferedMutator接收来自MR作业的puts，异步的批量提交数据，不影响MR作业的运行。 错误处理当通过BufferedMutator批量提交发生错误时触发绑定的BufferedMutator.ExceptionListener监听器实例的onException方法，其中RetriesExhaustedWithDetailsException记录了发生错误的内容及其提交的错误内容等信息，而其余正确的提交的内容则会正确放入HBase表中。 对于提交内容的检查分为客户端的检查和服务器端的检查。 当客户端检查到提交的内容出错（比如Put未添加内容或者未指定列），会抛出客户端的错误，这样错误不会RetriesExhaustedWithDetailsException监听器接受，被其运行的线程会因错误而终止，则在该Put之后的内容都不会提交。 当服务端检查到提交的内容出错（比如指定的列簇不存在），会向客户端传输错误，而这错误会被RetriesExhaustedWithDetailsException监听器接受，不会对后续提交的数据产生影响。 Table的put,get,delete方法提交一个（put,get,delete）列表操作，其中有错误的内容时，也分客户端的检查和服务器端的检查（不同操作检查内容不同），在客户端检查出错会抛出异常终止程序，服务端异常时会传输会错误信息，但是其余正确的操作已将提交到服务端并被正确执行。 try-with-resources的特性不了解可以参考 https://www.jianshu.com/p/3ab87269140c]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>BufferedMutator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase 简介]]></title>
    <url>%2FHBase%20%E7%AE%80%E4%BB%8B.html</url>
    <content type="text"><![CDATA[数据模型在Hbase中，数据同样是存储在表中，有行和列。但是hbase和关系型数据库（RDBMS）有很大的区别，hbase更像是多维度的map。 hbase的同样有着表（table）, 行（row）,列（Column），根据表、行、列可以定位到一个单元格（cell）上,这样看来hbase和关系型数据库形式一样，其实不是的，hbase的列包括列簇（column family）和列限定符（column qualifier），一般将列限定符成为列，一行有固定的列簇（由表结构决定），一个列簇包含一系列的列，列没有固定的结构，每一行的列簇下的列可以不相同，列由添加数据决定，可以将hbase的表想象成一个Map&lt;row, Map&lt;column family,Map&gt;&gt;,由行、列簇寻找一个key不固定的map。而且相同单元格会存储着不同的版本值，每一次存储新值会添加时间戳Timestamp，并不会覆盖掉上次值，一个hbase表默认能存储一个版本值，若第四次设置新增，则会把第一次的值覆盖掉。因此一个单元格的定位需要row,family,qualifier和timestamp来确定。 存储在HBase表中的行是按照行健的字典顺序排列下来的。 如下为一个hbase表中存储的数据，com.cnn.www的people列簇可以不存储任何列和值，com.cnn.www和com.example.www列簇中的列可以不相同 Row Key Time Stamp ColumnFamily contents ColumnFamily anchor ColumnFamily people “com.cnn.www” t9 anchor:cnnsi.com = “CNN” “com.cnn.www” t8 anchor:my.look.ca = “CNN.com” “com.cnn.www” t6 contents:html = “…” “com.cnn.www” t5 contents:html = “…” “com.cnn.www” t3 contents:html = “…” “com.example.www” t5 contents:html = “…” people:author = “ 实际存储虽然在概念级别，表可以被视为稀疏的行集，但它们是按列族物理存储的。可以随时将新的列限定符（column_family：column_qualifier）添加到现有列族。如下anchor列簇的存储表和contents列簇的存储表。 行键 时间戳 列族 anchor “com.cnn.www” T9 anchor:cnnsi.com = &quot;CNN&quot; “com.cnn.www” T8 anchor:my.look.ca = &quot;CNN.com&quot; 行键 时间戳 列族contents: “com.cnn.www” T6 内容：html =“ ……” “com.cnn.www” T5 内容：html =“ ……” “com.cnn.www” T3 内容：html =“ ……” 概念视图中显示的空单元格根本不存储。因此，contents:html在时间戳处对列的值的请求t8将不返回任何值。同样，对anchor:my.look.ca时间戳记值的请求t9将不返回任何值。但是，如果未提供时间戳，则将返回特定列的最新值。给定多个版本，最新版本也是第一个版本，因为时间戳按降序存储。因此，com.cnn.www如果未指定时间戳，则对行中所有列的值的请求将是：contents:html来自timestamp t6的值，anchor:cnnsi.com来自timestamp t9的值，anchor:my.look.ca来自timestamp t8的值。 存储的数据类型HBase通过Put和Result支持“bytes-in / bytes-out”接口，实际存储的是字节数组， 因此任何可以转换为字节数组的都可以存储为值。输入可以是字符串，数字，复杂对象，甚至是图像，只要它们可以呈现为字节。 值的大小存在实际限制（例如，在HBase中存储10-50MB对象可能太大了）。HBase中的所有行都符合数据模型，包括版本控制。在进行设计时要考虑到这一点，以及ColumnFamily的块大小。 命名空间命名空间是表的逻辑分组，类似于关系型数据库系统中的数据库。命名空间提供了一下几个好处（这里不太理解具体的体现）： 配额管理（HBASE-8410） - 限制命名空间可以使用的资源量（即区域，表）。 命名空间安全管理（HBASE-9206） - 为租户提供另一级别的安全管理。 区域服务器组（HBASE-6721） - 可以将命名空间/表固定到RegionServers的子集上，从而保证粗略的隔离级别。 命名空间管理可以创建，删除或更改命名空间。在表创建期间通过指定表单的完全限定表名来确定命名空间成员身份。 1&lt;table namespace&gt;:&lt;table qualifier&gt; 12345678hbase&gt; #Create a namespacehbase&gt; create_namespace 'my_ns'hbase&gt; #create my_table in my_ns namespacehbase&gt; create 'my_ns:my_table', 'fam'hbase&gt; #drop namespacehbase&gt; drop_namespace 'my_ns'hbase&gt; #alter namespacehbase&gt; alter_namespace 'my_ns', &#123;METHOD =&gt; 'set', 'PROPERTY_NAME' =&gt; 'PROPERTY_VALUE'&#125; 预定义的名称空间有两个预定义的特殊命名空间： hbase - 系统命名空间，用于包含HBase内部表 default - 没有明确指定名称空间的表将自动落入此名称空间 12345hbase&gt; #namespace=foo and table qualifier=barhbase&gt; create 'foo:bar', 'fam'hbase&gt; #namespace=default and table qualifier=barhbase&gt; create 'bar', 'fam' Hbase shell的基本命令（持续更新）==hbase shell命令是对大小写敏感，写入命令要注意区分大小写。== list 命令 列出所有的表名 describe命令 查看表的信息 1describe 'table_name' create命令 例如创建一个名为t1,列簇有f1,f2,f3的表，行健是必须的所以不许指定，每个列簇的列添加数据时指定，因为每一行数据的列都不相同。 12345hbase&gt; create 't1', &#123;NAME =&gt; 'f1'&#125;, &#123;NAME =&gt; 'f2'&#125;, &#123;NAME =&gt; 'f3'&#125;hbase&gt; # The above in shorthand would be the following:hbase&gt; create 't1', 'f1', 'f2', 'f3'hbase&gt; create 't1', &#123;NAME =&gt; 'f1', VERSIONS =&gt; 1, TTL =&gt; 2592000, BLOCKCACHE =&gt; true&#125;hbase&gt; create 't1', &#123;NAME =&gt; 'f1', CONFIGURATION =&gt; &#123;'hbase.hstore.blockingStoreFiles' =&gt; '10'&#125;&#125; enable，disable、is_disabled、disable_all命令 enable，disable分别是解除禁用和禁用表，禁用表之后可以看到该表的存在，但是无法对该表的内容进行操作，is_disabled查看该表是否被禁用，(enable_all)disable_all(解除)禁用所有匹配给定正则表达式的表。 1234hbase&gt;disable 'table_name'hbase&gt;is_disable 'table_name'hbase&gt;# 禁用所有表名前缀为table的表hbase&gt;disable_all 'table*' alter命令 修改表结构 123456789101112131415161718192021222324252627282930313233343536hbase&gt; # 改变或添加列簇的最带版本存储的版本个数为5hbase&gt; alter 't1', NAME =&gt; 'f1', VERSIONS =&gt; 5hbase&gt; # 能同时修改多个列簇hbase&gt; alter 't1', 'f1', &#123;NAME =&gt; 'f2', IN_MEMORY =&gt; true&#125;, &#123;NAME =&gt; 'f3', VERSIONS =&gt; 5&#125;hbase&gt; # 删除ns1命名空间下的t1表的f1列簇hbase&gt; alter 'ns1:t1', NAME =&gt; 'f1', METHOD =&gt; 'delete'hbase&gt; alter 'ns1:t1', 'delete' =&gt; 'f1'hbase&gt; # 能修改表的MAX_FILESIZE, READONLY, MEMSTORE_FLUSHSIZE, DURABILITY等属性，比如我们修改region的最大大小至128Mhbase&gt; alter 't1', MAX_FILESIZE =&gt; '134217728'hbase&gt; # 可以通过设置表协处理器属性来添加表协处理器：hbase&gt; alter 't1','coprocessor'=&gt;'hdfs:///foo.jar|com.foo.FooRegionObserver|1001|arg1=1,arg2=2'hbase&gt; # 由于您可以为表配置多个协处理器，因此序列号将自动附加到属性名称以唯一标识它。hbase&gt; # 协处理器属性必须与下面的模式匹配，以便框架了解如何加载协处理器类：hbase&gt; # [coprocessor jar file location] | class name | [priority] | [arguments]hbase&gt; # 可以设置configuration属性在表或列簇上hbase&gt; alter 't1', CONFIGURATION =&gt; &#123;'hbase.hregion.scan.loadColumnFamiliesOnDemand' =&gt; 'true'&#125;hbase&gt; alter 't1', &#123;NAME =&gt; 'f2', CONFIGURATION =&gt; &#123;'hbase.hstore.blockingStoreFiles' =&gt; '10'&#125;&#125;hbase&gt; # 可以删除表范围属性hbase&gt; alter 't1', METHOD =&gt; 'table_att_unset', NAME =&gt; 'MAX_FILESIZE'hbase&gt; alter 't1', METHOD =&gt; 'table_att_unset', NAME =&gt; 'coprocessor$1'hbase&gt; # 可以设置REGION_REPLICATION:hbase&gt; alter 't1', &#123;REGION_REPLICATION =&gt; 2&#125;hbase&gt; # 可以同时修改多个属性hbase&gt; alter 't1', &#123; NAME =&gt; 'f1', VERSIONS =&gt; 3 &#125;,&#123; MAX_FILESIZE =&gt; '134217728' &#125;, &#123; METHOD =&gt; 'delete', NAME =&gt; 'f2' &#125;,OWNER =&gt; 'johndoe', METADATA =&gt; &#123; 'mykey' =&gt; 'myvalue' &#125; drop命令 删除表，在删除表之前要确保表已经disbale，这样可以防止删除表时对其他使用该表的用户造成的影响 12hbase&gt;disable 'table_name'hbase&gt;drop 'table_name' put命令 例如向表t1的行健为r1的列簇为f1,列名为q1的值设为value,当存在是会设置新的时间戳的值，不存在会添加新的列。（如果不指定列名，则默认列名为空字符串） 123456hbase&gt; put 'ns1:t1', 'r1', 'f1:q1', 'value'hbase&gt; put 't1', 'r1', 'f1:q1', 'value'hbase&gt; put 't1', 'r1', 'f1:q1', 'value', ts1hbase&gt; put 't1', 'r1', 'f1:q1', 'value', &#123;ATTRIBUTES=&gt;&#123;'mykey'=&gt;'myvalue'&#125;&#125;hbase&gt; put 't1', 'r1', 'f1:q1', 'value', ts1, &#123;ATTRIBUTES=&gt;&#123;'mykey'=&gt;'myvalue'&#125;&#125;hbase&gt; put 't1', 'r1', 'f1:q1', 'value', ts1, &#123;VISIBILITY=&gt;'PRIVATE|SECRET'&#125; get 命令 查看表中的数据 123456hbase&gt; # 查看t1表的r1行的相关内容hbase&gt; get 't1', 'r1'hbase&gt; # 查看t1表的r1行的列簇为c1相关内容hbase&gt; get 't1', 'r1', 'f1'hbase&gt; # 查看t1表的r1行的列簇为c1的列为cnam相关内容hbase&gt; get 't1', 'r1', 'f1:q1' scan 命令 扫描表,没有像put和get简便的写法 1234567hbase&gt; # 查看表"t1"中的所有数据hbase&gt; scan 't1'hbase&gt; # scan 命令可以指定 startrow,stoprow 来 scan 多个 rowhbase&gt; scan 'user_test',&#123;COLUMNS =&gt;'info:username',LIMIT =&gt;10, STARTROW =&gt; 'test', STOPROW=&gt;'test2'&#125;hbase&gt; # 查看表"t1"中列族"c1"的所有数据,或者列族"c1"中列"cname"的所有数据hbase&gt; scan 't1', &#123;COLUMN =&gt; 'f1'&#125;hbase&gt; scan 't1', &#123;COLUMN =&gt; 'f1:q1'&#125; delete 命令 1hbase&gt; delete 't1', 'r1', 'f1:q1' 可以将命令写入脚本文件一次性顺序执行，例如在hbase_commands.txt中写入要执行的hbase shell命令,可以使用如下命令执行,在脚本的结尾最好写入exit命令，否则执行完命令之后停留在客户端交互界面。 1hbase shell ./hbase_commands.txt HBase 的API(持续更新)HBase与表操作有关的api主要有 HBaseConfiguration（配置类）， Connection（建立数据库连接类）， Admin（操作表的创建，删除，修改）， TableName（表名类），Table（操作表内容的增删改查）， Put（对应于put命令，存储值，Table.put方法的参数）， Get（对应于get命令，查看值，Table.get方法的参数）， Append（在值后面追加内容，table.append方法的参数，若原本有内容，得到追加后的结果，否则返回null）， Result（get返回的对象）， Scan（对应于scab命令，遍历值，Table.scan方法的参数，scan在指定起始行健和终止行健时，扫描的范围是[startRow, endRow),不包括终止行）， ResultScanner(scan的结果返回ResultScanner对象，扫描不会通过一次RPC请求到所有的结果，而是按行为单位请求返回的，如果请求大量数量，一次请求完会消耗大量系统资源和时间，返回的ResultScanner对象是一个迭代器，迭代得到Result对象，每次调用next方法会进行RPC请求一行数据，即使next(int nbRows)方法内部也是调用多次next()方法) Delete（对应于delete命令，删除值，Table.delete方法的参数，在没有指定时间戳的情况下，deleteColumn方法删除列的最新版本，deleteColumns方法删除所有版本，在指定时间戳的情况下，deleteColumn删除与时间戳匹配的版本，deleteColumns方法删除与时间戳相等和更早的版本）。 Put，Get，Delete， Increment，Append等都是Row的子类，方便Tbale的batch(List&lt;? extends Row&gt; actions, Object[] results)方法批处理增删改查等操作 api的详细使用请查看官方文档http://hbase.apache.org/1.2/apidocs/，这是1.2版本的，目前对于新版本的HBase来说有些过时。 写的简单操作hbase表的类 https://github.com/Yhaij/HadoopLearn/tree/master/src/main/java/com/hbase/learn 行锁HBase提供行锁，但是在0.95版本之后客户端无法显示的得到和使用行锁，该API已经移除，只在服务端有行锁的应用。在客户端只能使用checkAndPut,checkAndDelete等行原子操作 https://issues.apache.org/jira/browse/HBASE-7315]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单集群时间同步]]></title>
    <url>%2F%E7%AE%80%E5%8D%95%E9%9B%86%E7%BE%A4%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5.html</url>
    <content type="text"><![CDATA[最近集群的Hbase的其中几个节点总是连接不上，最后发现是集群之间的系统时间不同步导致的(hbase的时间戳决定节点之间的时间必须同步)。决定使用的ntp来解决集群之间的系统时间同步问题。 安装ntp1yum install -y ntp ntp服务器配置集群之间的时间同步同样采用sever/client的方式，将其中一个节点做为ntp的服务器,其余作为客户端通过ntp服务来向ntp服务器同步时间。需要选定一台作为ntp server，修该ntp的配置文件。 1vim /etc/ntp.conf 在restrict内容下，加入一句，限制服务器的访问类型。 123456# Permit all access over the loopback interface. This could# be tightened as well, but to do so would effect some of# the administrative functions.restrict 127.0.0.1restrict -6 ::1restrict 10.1.13.0 mask 255.255.255.0 nomodify 这里加入了restrict 10.1.13.0 mask 255.255.255.0 nomodify，根据子网掩码，表示访问该ntp服务器的客户端ip必须在10.1.13.1-10.1.13.254范围内，nomodify表明客户端不能更改服务端的时间参数，但是客户端可以通过服务端进行网络校时。 restrict 相关语法为：restrict IP地址 mask 子网掩码 参数 其中IP地址也可以是default ，default 就是指所有的IP 参数有以下几个： ignore ：关闭所有的 NTP 联机服务 nomodify：客户端不能更改服务端的时间参数，但是客户端可以通过服务端进行网络校时。 notrust ：客户端除非通过认证，否则该客户端来源将被视为不信任子网 noquery ：不提供客户端的时间查询：用户端不能使用ntpq，ntpc等命令来查询ntp服务器 notrap ：不提供trap远端登陆：拒绝为匹配的主机提供模式 6 控制消息陷阱服务。陷阱服务是 ntpdq 控制消息协议的子系统，用于远程事件日志记录程序。 nopeer ：用于阻止主机尝试与服务器对等，并允许欺诈性服务器控制时钟 kod ： 访问违规时发送 KoD 包。 restrict -6 表示IPV6地址的权限设置。 接着注释掉原来的server内容，加入以下两句 12345678# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver 127.127.1.0fudge 127.127.1.0 stratum 10 server指定从哪个ntp服务器同步时间，由于这里是ntp服务器，所以只指定同步自己。 ntp客户端配置客户端的配置只需指定向ntp服务器同步时间即可。比如上面配置的ntp服务器地址为10.1.13.111，则只需要加入server 10.1.13.111即可 1234567# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver 10.1.13.111 启动服务配置好了只需启动服务端和客户端的ntp服务,设置开机启动即可 12service ntpd startchkconfig ntpd on 这里的ntp服务占用UDP的123端口，当服务启动失败可以看看是否端口被占用 1netstat -nulp | grep 123 并且要保证防护墙的UDP的123端口开放 如果不开启ntp服务自动对时的话，也可以自己设置定时任务自动对时，只启动ntp服务器，客户端可以通过crontab -e设置定时任务通过ntpdate对时。 12crontab -e* */1 * * * /usr/sbin/ntpdate 10.1.18.221 &gt;/dev/null 2&gt;&amp;1 但是这种方法不推荐，最好采用上面启动ntp服务自动对时]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>ntp</tag>
        <tag>集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce 多路径输出]]></title>
    <url>%2FMapReduce-%E5%A4%9A%E8%B7%AF%E5%BE%84%E8%BE%93%E5%87%BA.html</url>
    <content type="text"><![CDATA[mapreduce中实现多路径输出主要使用MulitipleOutputs类通过两个例子可以掌握 输入样例 mulitipleInput.txt1234567file1 001file2 002file3 003file2 004file1 005file1 006file3 007 输出：file1和file3开头的记录归到一个文件下file2和file3开头的记录归到一个文件下 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class MultipleOutputsExample extends Configured implements Tool&#123; public static class MultipleMapper extends Mapper&lt;Object, Text, Text, NullWritable&gt;&#123; private MultipleOutputs&lt;Text, NullWritable&gt; mos; @Override protected void setup(Mapper&lt;Object, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // TODO Auto-generated method stub mos = new MultipleOutputs&lt;Text, NullWritable&gt;(context); &#125; @Override protected void map(Object key, Text value, Mapper&lt;Object, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // TODO Auto-generated method stub String[] infos = value.toString().split(" "); if(infos[0].equals("file1"))&#123; mos.write("file1", value, NullWritable.get()); &#125;else if (infos[0].equals("file2")) &#123; mos.write("file2", value, NullWritable.get()); &#125; else &#123; mos.write("file1", value, NullWritable.get()); mos.write("file2", value, NullWritable.get()); &#125; &#125; @Override protected void cleanup(Mapper&lt;Object, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // TODO Auto-generated method stub mos.close(); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; // TODO Auto-generated method stub Configuration conf = new Configuration(); String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); if (otherArgs.length &lt; 2)&#123; System.err.println("Usage: Data Deduplication &lt;in&gt; &lt;out&gt;"); System.exit(2); &#125; Job job = Job.getInstance(conf); job.setJarByClass(MultipleOutputsExample.class); job.setMapperClass(MultipleMapper.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); MultipleOutputs.addNamedOutput(job, "file1", TextOutputFormat.class, Text.class, NullWritable.class); MultipleOutputs.addNamedOutput(job, "file2", TextOutputFormat.class, Text.class, NullWritable.class); FileInputFormat.addInputPath(job, new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); return job.waitForCompletion(true)? 0:1; &#125; public static void main(String[] args) throws Exception &#123; System.exit(ToolRunner.run(new MultipleOutputsExample(), args)); &#125;&#125; 结果12345678910111213$ hadoop fs -cat /user/test/wordTest/mulitipleOutput/file1-m-00000file1 001file3 003file1 005file1 006file3 007$ hadoop fs -cat /user/test/wordTest/mulitipleOutput/file2-m-00000file2 002file3 003file2 004file3 007file2 008 如果想把file1和file2的内容放入不同的目录下，可以通过指定baseOutputPath，将file1开头的文件放在同一个目录中管理。将mos.write(&quot;file1&quot;, value, NullWritable.get());和mos.write(&quot;file2&quot;, value, NullWritable.get());改为mos.write(&quot;file1&quot;, value, NullWritable.get(),&quot;file1/part&quot;);和mos.write(&quot;file2&quot;, value, NullWritable.get()，&quot;file2/part&quot;);或mos.write(value, NullWritable.get(),&quot;file1/part&quot;);和mos.write(value, NullWritable.get()，&quot;file2/part&quot;);可以看到输出结果 123456$ hadoop fs -ls /user/test/wordTest/mulitipleOutputFound 4 items-rw-r--r-- 3 hdfs hdfs 0 2018-06-30 16:18 /user/test/wordTest/mulitipleOutput/_SUCCESSdrwxr-xr-x - hdfs hdfs 0 2018-06-30 16:18 /user/test/wordTest/mulitipleOutput/file1drwxr-xr-x - hdfs hdfs 0 2018-06-30 16:18 /user/test/wordTest/mulitipleOutput/file2-rw-r--r-- 3 hdfs hdfs 0 2018-06-30 16:18 /user/test/wordTest/mulitipleOutput/part-r-00000 指定baseOutputPath输出路径和输出文件名直接按照baseOutPutPath指定，但是默认输出文件名后缀会跟上-r-00000，如果想更改可以继承FileOutputFormat重写RecordWriter实现。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 小文件的处理]]></title>
    <url>%2FHadoop-%E5%B0%8F%E6%96%87%E4%BB%B6%E7%9A%84%E5%A4%84%E7%90%86.html</url>
    <content type="text"><![CDATA[hadoop的HDFS和MapReduce本身都是用户处理大量数据的大文件，对于小文件来说，由于namenode会在记录每个block对象，如果存在大量的小文件，会占用namenode的大量内存空间，而且HDFS存储文件是按block来存储，即使一个文件的大小不足一个block的大小，文件还是会占用一个block的存储空间，所以大量的小文件会对HDFS的存储和访问都带来不利的影响。 hadoop对于小文件的处理主要有Hadoop Archive，Sequence file和CombineFileInputFormat三种方式。 Hadoop ArchiveHadoop Archive是hadoop的归档命令，可以将hdfs上的小文件打包成一个har文件，这种方式虽然不会减少小文件占用大量存储空间的问题，但是会减少namenode的内存空间。同时har文件支持hdfs命令对其的访问。 命令：hadoop archive -archiveName 归档名称 -p 父目录 [-r &lt;复制因子&gt;] 原路径（可以多个） 目的路径 -archiveNames设置归档生成文件的名字 -p 需要进行归档的文件的父目录 例子：123456789$ hadoop fs -ls /user/test/yhj/input/Found 3 items-rw-r--r-- 3 root hdfs 760 2018-07-04 11:48 /user/test/yhj/input/word1.txt-rw-r--r-- 3 root hdfs 82 2018-07-04 11:48 /user/test/yhj/input/word2.txt-rw-r--r-- 3 root hdfs 1738 2018-07-04 11:48 /user/test/yhj/input/word3.txt$ hadoop archive -archiveName word.har -p /user/test/yhj/input/ word1.txt word2.txt word3.txt /user/test/yhj/harInput/$ hadoop fs -ls /user/test/yhj/harInput/Found 1 itemsdrwxr-xr-x - hdfs hdfs 0 2018-07-05 20:18 /user/test/yhj/harInput/word.har HAR文件的生成是通过运行一个mapreduce的程序生成，所以需要集群环境中装个mapreduce HAR是在Hadoop file system之上的一个文件系统，因此所有fs shell命令对HAR文件均可用，但使用不同的URI。另外，请注意档案是不可变的。所以，重命名，删除并创建返回一个错误，例如：1234567891011$ hadoop fs -ls /user/test/yhj/harInput/word.harFound 4 items-rw-r--r-- 3 hdfs hdfs 0 2018-07-05 20:18 /user/test/yhj/harInput/word.har/_SUCCESS-rw-r--r-- 5 hdfs hdfs 255 2018-07-05 20:18 /user/test/yhj/harInput/word.har/_index-rw-r--r-- 5 hdfs hdfs 22 2018-07-05 20:18 /user/test/yhj/harInput/word.har/_masterindex-rw-r--r-- 3 hdfs hdfs 2580 2018-07-05 20:18 /user/test/yhj/harInput/word.har/part-0$ hadoop fs -ls har:/user/test/yhj/harInput/word.harFound 3 items-rw-r--r-- 3 hdfs hdfs 760 2018-07-04 11:48 har:///user/test/yhj/harInput/word.har/word1.txt-rw-r--r-- 3 hdfs hdfs 82 2018-07-04 11:48 har:///user/test/yhj/harInput/word.har/word2.txt-rw-r--r-- 3 hdfs hdfs 1738 2018-07-04 11:48 har:///user/test/yhj/harInput/word.har/word3.txt 可以看到Hadoop存档目录包含元数据（采用_index和_masterindex形式）、数据部分data（part- *）文件、归档文件的名称和部分文件中的位置（_index文件）。 HAR文件也可以被mapreduce读取，路径的URI可以使用不同的URI,比如例子中的文件输入的路径URI可以下面两种方式使用12hdfs://10.1.13.111:8020/user/test/yhj/harInput/word.harhar://hdfs-10.1.13.111:8020/user/test/yhj/harInput/word.har 但是这个例子的文件来说，两个输入路径产生map的个数是不同的，har的路径产生的map有三个，对应三个word*.txt,而hdfs的路径只有一个，对应word.har/part-0 如果是文件支持行记录切分使用mapreduce来处理数据（文件的前后数据不相互影响），建议使用hdfs的URI路径,因为存档目录的part-*可能包括多个小文件的数据，这样可以减少map的个数，不会为每个单独的小文件启动一个map。 CombineFileInputFormat将大量小文件做为mapreduce的输入是不合适的，因为FileInputFormat只会分割大文件（文件大小超过设定的分片大小，默认为HDFS的块大小），对于小于分片大小的文件，每个文件作为一个分片，如果文件大小小于一个块的大小，mapreduce会为每个小文件产生一个map，这样会产生大量小文件，而每个map只会处理少量数据，每次map操作都会产生开销。当然可以通过mapred.min.split.size和mapred.max.split.size来控制map数量。 CombineFileInputFormat是mapreduce针对小文件而设计的，CombineFileInputFormat可以将多个小文件打包进一个分片，另外，比直接设置map数量好的在于，CombineFileInputFormat在决定将那些块放入一个分片是会考虑到块所在的节点和机架的位置，避免操作分片是过多的数据传输。 CombineFileInputFormat是一个抽象类，hadoop自带的实现的有CombineTextInputFormat，我们可以通过继承CombineFileInputFormat实现createRecordReader方法，自定义RecordReader类来实现理海量小文件的MapReduce。 InputFormat主要有两个方法，getSplits（计算得到分片），createRecordReader（产生返回RecordReader，RecordReader生成输出map读入的键值对） CombineFileInputFormat中已经实现了getSplits，即将多个小文件打包进一个分片中CombineFileSplit，我们需要实现createRecordReader方法，返回一个可以读取该分片中内容的RecordReader。 MyCombineInputFormat的实现123456789101112public class MyCombineInputFormat extends CombineFileInputFormat&lt;LongWritable, Text&gt;&#123; @Override public RecordReader createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException &#123; RecordReader&lt;LongWritable, Text&gt; reader = new CombineFileRecordReader&lt;&gt;((CombineFileSplit) inputSplit, taskAttemptContext, MyCombineFileRecordReader.class); try &#123; reader.initialize(inputSplit, taskAttemptContext); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return reader; &#125;&#125; 这里实际返回了一个CombineFileRecordReader的对象，CombineFileRecordReader通过CombineFileSplit，context和Class&lt;? extends RecordReader&gt;类型构造，MyCombineFileRecordReader是我们对于CombineFileSplit中每一个文件的产生map的输入的方法。CombineFileRecordReader中的nextKeyValue方法，会为每一个打包在CombineFileSplit中的文件构造一个RecordReader方法，读取文件中的记录。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class CombineFileRecordReader&lt;K, V&gt; extends RecordReader&lt;K, V&gt; &#123; ... public CombineFileRecordReader(CombineFileSplit split, TaskAttemptContext context, Class&lt;? extends RecordReader&lt;K, V&gt;&gt; rrClass) throws IOException &#123; this.split = split; this.context = context; this.idx = 0; this.curReader = null; this.progress = 0L; try &#123; this.rrConstructor = rrClass.getDeclaredConstructor(constructorSignature); this.rrConstructor.setAccessible(true); &#125; catch (Exception var5) &#123; throw new RuntimeException(rrClass.getName() + " does not have valid constructor", var5); &#125; this.initNextRecordReader(); &#125; protected boolean initNextRecordReader() throws IOException &#123; if(this.curReader != null) &#123; this.curReader.close(); this.curReader = null; if(this.idx &gt; 0) &#123; this.progress += this.split.getLength(this.idx - 1); &#125; &#125; if(this.idx == this.split.getNumPaths()) &#123; return false; &#125; else &#123; this.context.progress(); try &#123; Configuration conf = this.context.getConfiguration(); conf.set("mapreduce.map.input.file", this.split.getPath(this.idx).toString()); conf.setLong("mapreduce.map.input.start", this.split.getOffset(this.idx)); conf.setLong("mapreduce.map.input.length", this.split.getLength(this.idx)); this.curReader = (RecordReader)this.rrConstructor.newInstance(new Object[]&#123;this.split, this.context, Integer.valueOf(this.idx)&#125;); if(this.idx &gt; 0) &#123; this.curReader.initialize(this.split, this.context); &#125; &#125; catch (Exception var2) &#123; throw new RuntimeException(var2); &#125; ++this.idx; return true; &#125; public boolean nextKeyValue() throws IOException, InterruptedException &#123; do &#123; if(this.curReader != null &amp;&amp; this.curReader.nextKeyValue()) &#123; return true; &#125; &#125; while(this.initNextRecordReader()); return false; &#125; public K getCurrentKey() throws IOException, InterruptedException &#123; return this.curReader.getCurrentKey(); &#125; public V getCurrentValue() throws IOException, InterruptedException &#123; return this.curReader.getCurrentValue(); &#125; ...&#125; 在nextKeyValue方法中通过自定义的RecordReader的nextKeyValue读取当前文件的对象，当读完当前文件中的信息，后会通过initNextRecordReader返回初始化的下一个文件的RecordReader，所以我们只需实现相应的读取一个文件的RecordReader即可。 MyCombineFileRecordReader的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class MyCombineFileRecordReader extends RecordReader&lt;LongWritable, Text&gt; &#123; private CombineFileSplit combineFileSplit; private int currentIndex; private LineRecordReader reader = new LineRecordReader(); private int totalNum; public MyCombineFileRecordReader(CombineFileSplit combineFileSplit, TaskAttemptContext context, Integer index)&#123; super(); this.combineFileSplit = combineFileSplit; this.currentIndex = index; this.totalNum = combineFileSplit.getNumPaths(); &#125; @Override public void initialize(InputSplit inputSplit, TaskAttemptContext context) throws IOException, InterruptedException &#123; FileSplit fileSplit = new FileSplit(combineFileSplit.getPath(currentIndex), combineFileSplit.getOffset(currentIndex), combineFileSplit.getLength(currentIndex), combineFileSplit.getLocations()); context.getConfiguration().set("mapreduce.map.input.file.name", fileSplit.getPath().getName()); this.reader.initialize(fileSplit, context); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if(currentIndex &gt;= 0 &amp;&amp; currentIndex &lt; totalNum)&#123; return reader.nextKeyValue(); &#125;else &#123; return false; &#125; &#125; @Override public LongWritable getCurrentKey() throws IOException, InterruptedException &#123; return reader.getCurrentKey(); &#125; @Override public Text getCurrentValue() throws IOException, InterruptedException &#123; return reader.getCurrentValue(); &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; if(currentIndex &gt;= 0 &amp;&amp; currentIndex &lt; totalNum)&#123; return (float)currentIndex/totalNum; &#125; return 0; &#125; @Override public void close() throws IOException &#123; reader.close(); &#125;&#125; MyCombineFileRecordReader中通过LineRecordReader按行来读取文本记录，在initialize方法中通过CombineFileSplit和index（CombineFileSplit中文件信息的索引位置）来得到相应文件的信息，创建对应的FileSplit，接着创建LineRecordReader对象，在nextKeyValue中委托给LineRecordReader为mapper产生键-值对象。 最后入口函数和map类的实现，将InputFormatClass替换成自定义的MyCombineInputFormat类1234567891011121314151617181920212223242526272829public class CombineInputFromatMain extends Configured implements Tool&#123; public static class CombineInputFormatMap extends Mapper&lt;Object, Text, Text, Text&gt;&#123; private Text outKey = new Text(); @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; outKey.set(context.getConfiguration().get("mapreduce.map.input.file.name")); context.write(outKey, value); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; //设定默认job和设置输入输出路径的函数 Job job = JobDefaultInit.getClusterDefaultJob(this, getConf(), args); job.setJobName("CombineInputFormat Text"); job.setJarByClass(CombineInputFromatMain.class); job.setMapperClass(CombineInputFormatMap.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); job.setNumReduceTasks(0); job.setInputFormatClass(MyCombineInputFormat.class); return job.waitForCompletion(true) ? 0:1; &#125; public static void main(String[] args) throws Exception &#123; System.exit(ToolRunner.run(new CombineInputFromatMain(), args)); &#125;&#125; 在这例子中将三个word*.txt文件打包进一个分片，实际只产生了一个map。 Sequence filesequence file由一系列的二进制key/value组成，如果为key小文件名，value为文件内容，则可以将大批小文件合并成一个大文件。 顺序文件由文件头和随后的记录内容组成，顺序文件的前三个字节为SEQ(顺序文件代码)，紧接着一个字节表示文件的版本号，文件头还包括键和值的类型，数据是否压缩的标志位，是否进行快压缩的标志位， 数据的压缩形式，用户自定义的数据以及同步标识。顺序文件读取内容只能从同步标识开始读取。同步标识位于记录和记录之间，也就是说无法从记录中间开始读取顺序文件的内容。 Sequence file的格式主要有三种，分为未压缩，记录压缩和块压缩。主要格式的存储方式可以查看官方给出的api:http://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/SequenceFile.html 将小文件合并成一个sequence file的实现(代码参考hadoop 权威指南)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117public class SmallFilesToSequenceFileConverter extends Configured implements Tool &#123; public static class WholeFileInputFormat extends FileInputFormat&lt;LongWritable, Text&gt;&#123; /** * 不切分文件，一个split读入整个文件 * @param context * @param filename * @return */ @Override protected boolean isSplitable(JobContext context, Path filename) &#123; return false; &#125; @Override public RecordReader&lt;LongWritable, Text&gt; createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; RecordReader reader = new WholeFileRecordReader(); reader.initialize(inputSplit, taskAttemptContext); return reader; &#125; &#125; /** * 自定义RecordReader，读取整个小文件内容 */ public static class WholeFileRecordReader extends RecordReader&lt;LongWritable, Text&gt;&#123; private FileSplit fileSplit; private Configuration conf; private LongWritable key = new LongWritable(); private Text value = new Text(); private boolean process = false; @Override public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; this.fileSplit = (FileSplit)inputSplit; this.conf = taskAttemptContext.getConfiguration(); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if(!process)&#123; FileSystem fs = fileSplit.getPath().getFileSystem(conf); FSDataInputStream in = null; try &#123; in = new FSDataInputStream(fs.open(fileSplit.getPath())); byte[] contextByte = new byte[(int)fileSplit.getLength()]; IOUtils.readFully(in, contextByte, 0, contextByte.length); //等同于 in.read(contextByte, 0, contextByte.length); String context = new String(contextByte, "utf-8"); key.set(fileSplit.getStart()); value.set(context); &#125;finally &#123; IOUtils.closeStream(in); &#125; process = true; return true; &#125; return false; &#125; @Override public LongWritable getCurrentKey() throws IOException, InterruptedException &#123; return key; &#125; @Override public Text getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return process? 1.0f:1.0f; &#125; @Override public void close() throws IOException &#123; &#125; &#125; public static class SmallFilesToSequenceFileMap extends Mapper&lt;Object, Text, Text, Text&gt;&#123; private Text outKey = new Text(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; outKey.set(((FileSplit)context.getInputSplit()).getPath().toString()); &#125; @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; context.write(outKey, value); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; //设定默认job和设置输入输出路径的函数 Job job = JobDefaultInit.getClusterDefaultJob(this, getConf(), args); job.setJobName("SmallFiles To SequenceFile"); job.setMapperClass(SmallFilesToSequenceFileMap.class); job.setInputFormatClass(WholeFileInputFormat.class); job.setOutputFormatClass(SequenceFileOutputFormat.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); return job.waitForCompletion(true)? 0:1; &#125; public static void main(String[] args) throws Exception &#123; System.exit(ToolRunner.run(new SmallFilesToSequenceFileConverter(), args)); &#125;&#125; hdfs可以通过命令行hadoop fs -text来显示以文本的方式显示顺序文件 读取SequenceFile简单实现1234567891011121314151617181920212223242526272829public class SequenceFileReadMain extends Configured implements Tool&#123; public static class SequenceFileReadMap extends Mapper&lt;Text, Text, Text, Text&gt;&#123; private Text outKey = new Text(); private Text outValue = new Text(); @Override protected void map(Text key, Text value, Context context) throws IOException, InterruptedException &#123; outKey.set("key : " + key.toString()); outValue.set("value : " + value.toString()); context.write(outKey, outValue); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; Job job = JobDefaultInit.getClusterDefaultJob(this, getConf(), args); job.setJobName("Sequence File Read"); job.setMapperClass(SequenceFileReadMap.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); job.setInputFormatClass(SequenceFileInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); return job.waitForCompletion(true)?0:1; &#125; public static void main(String[] args) throws Exception&#123; System.exit(ToolRunner.run(new SequenceFileReadMain(), args)); &#125;&#125; 这时候读取SequenceFile的时候，key对应的是小文件的名字，value是一个小文件的所有内容，所以需要在map编写处理整个小文件内容的代码 参考资料：https://blog.csdn.net/u011007180/article/details/52333387 https://www.cnblogs.com/staryea/p/8603112.html http://dongxicheng.org/mapreduce/hdfs-small-files-solution/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mapreduce 排序]]></title>
    <url>%2Fmapreduce-%E6%8E%92%E5%BA%8F.html</url>
    <content type="text"><![CDATA[mapreduce的排序主要分部分排序、全排序和辅助排序（二次排序） 可以直接在reduce中在对数据进行排序，但是这对于reduce的负担太重，数据处理的时间消耗也会大大增加 mapreduce机制中排序只会针对键进行排序，所以如果想对某个数据进行排序，一定要将其设置为map输出的键，排序主要发生在map的spill和合并spill file阶段和reduce拉取复制map端的数据后合并成reduce文件时。 排序的设置和调用的顺序排序类及其方法调用主要遵循以下顺序： 如果设置mapreduce.job.output.key.comparator.class或设置了Job类的setSortComparatorClass(),则会使用设置的类的实例进行排序。该类需要继承WritableComparator 如果1中没有设置，键的类型是WritableComparable的子类，就会调用该键的compareTo（)的方法 如果上述都RawComparator将字节流反序列化为WritableComparable对象，再调用其compareTo（)进行比较 从以上可以看出，如果重写RawComparator的compare方法，在字节流的时候就进行所需的键的比较是性能最好的，因为这样无需在排序进行反序列化，但编写的难度相对较高。 部分排序mapreduce根据键的排序使得每个reduce输出的文件都是排序好的 全排序对于全排序最简单的方法是将所有的方法是将reduce的个数设置为1，但是如果数据量太大会对reduce造成过大的负担。 解决的方法可以通过改写partition来调整使得每个reduce之间数据保持有序，即保证一段连续区间内的数据都落在一个分区里，这样只要对reduce的输出文件排好序就可以达到全局排序。但是如果靠人为的划分分区，需要知道键的范围，并且很容易造成数据的倾斜，造成有的reduce分到数据特别多，而有的却很少，理想情况下是个分区记录数大致相等。解决方法是通过多数据进行采样，通过采样一小部分数据得到排序数据的大致分布，从而制定出如何划分分区。 hadoop内置了若干采样器，用于需要实现自定义采样的话需要继承InputSampler类(该类实现了Sampler接口)重写getSample方法，返回采样的键。InputSampler实现了静态方法writePartitionFile()方法来在致指定的hdfs路径下创建一个顺序文件来存储定义分区的键。将顺序文件加入分布式缓存，由TotalOrderPartitioner使用并未排序作业创建分区。 例子，使用采样分区的方式对一系列随机生成的服从正态分布的数据进行全排序。 原始输入数据，满足正态分布，随机生成 1234567891011hadoop fs -cat /user/test/yhj/random/input/random.txt | head12816584491071195471-4229131970634916817-557165-14358191924747-262067 重写InputFormat和RecordReader使直接读入的值是IntWritable类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public static class MyFileIntputFormat extends FileInputFormat&lt;LongWritable, IntWritable&gt;&#123; @Override public RecordReader&lt;LongWritable, IntWritable&gt; createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; RecordReader&lt;LongWritable, IntWritable&gt; reader = new MyRecordReader(); reader.initialize(inputSplit, taskAttemptContext); return reader; &#125; &#125; public static class MyRecordReader extends RecordReader&lt;LongWritable, IntWritable&gt;&#123; private LineRecordReader lineRecordReader; public MyRecordReader()&#123; lineRecordReader = new LineRecordReader(); &#125; @Override public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; lineRecordReader.initialize(inputSplit, taskAttemptContext); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; return lineRecordReader.nextKeyValue(); &#125; @Override public LongWritable getCurrentKey() throws IOException, InterruptedException &#123; return lineRecordReader.getCurrentKey(); &#125; @Override public IntWritable getCurrentValue() throws IOException, InterruptedException &#123; IntWritable value = new IntWritable(Integer.parseInt(lineRecordReader.getCurrentValue().toString())); return value; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return lineRecordReader.getProgress(); &#125; @Override public void close() throws IOException &#123; lineRecordReader.close(); &#125; &#125; 由于默认的采样器都是对键进行采样，而我们读入的键是偏移量，真正排序采样的是值，所以需要重写采样器，这里继承了RandomSampler，对值进行随机采样，RandomSampler的初始化需要三个参数RandomSampler(double freq, int numSamples, int maxSplitsSampled) freq代表采样频率，numSamples代表样本最大样本数，maxSplitsSampled代表最大分区数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public static class MySample extends InputSampler.RandomSampler&lt;Object, IntWritable&gt;&#123; public MySample(double freq, int numSamples) &#123; super(freq, numSamples); &#125; public MySample(double freq, int numSamples, int maxSplitsSampled)&#123; super(freq, numSamples, maxSplitsSampled); &#125; @Override public IntWritable[] getSample(InputFormat&lt;Object, IntWritable&gt; inf, Job job) throws IOException, InterruptedException &#123; List&lt;InputSplit&gt; splits = inf.getSplits(job); List&lt;IntWritable&gt; samples = new ArrayList&lt;&gt;(); int splitToSample = Math.min(this.numSamples, splits.size()); Random random = new Random(); long seed = random.nextLong(); random.setSeed(seed); //随机交换split for(int i = 0; i &lt; splits.size(); i++)&#123; InputSplit tmp = splits.get(i); int index = random.nextInt(splits.size()); splits.set(i, splits.get(index)); splits.set(index, tmp); &#125; //采样 for(int i = 0; i &lt; splitToSample || i &lt; this.numSamples &amp;&amp; samples.size() &lt; this.numSamples ; i++)&#123; TaskAttemptContext sampleContext = new TaskAttemptContextImpl(job.getConfiguration(), new TaskAttemptID()); RecordReader&lt;Object, IntWritable&gt; reader = inf.createRecordReader(splits.get(i), sampleContext); while (reader.nextKeyValue()) &#123; //根据采样频率采样 if (random.nextDouble() &lt; this.freq) &#123; if (samples.size() &lt; this.numSamples) &#123; IntWritable value = new IntWritable(); samples.add(ReflectionUtils.copy(job.getConfiguration(), reader.getCurrentValue(), value)); &#125; else &#123; int index = random.nextInt(this.numSamples); if (index != this.numSamples) &#123; IntWritable value = new IntWritable(); samples.set(index, ReflectionUtils.copy(job.getConfiguration(), reader.getCurrentValue(), value)); &#125; this.freq *= (double) (this.numSamples - 1) / (double) this.numSamples; &#125; &#125; &#125; reader.close(); &#125; IntWritable[] result = new IntWritable[samples.size()]; samples.toArray(result); return result; &#125; &#125; 入口函数和map函数，reduce函数采用默认实现 12345678910111213141516171819202122232425262728293031323334public class SortByTotalPartitioner extends Configured implements Tool &#123; public static class SortByTotalPartitionerMap extends Mapper&lt;LongWritable, IntWritable, IntWritable, NullWritable&gt;&#123; @Override protected void map(LongWritable key, IntWritable value, Context context) throws IOException, InterruptedException &#123; context.write(value, NullWritable.get()); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; Job job = JobDefaultInit.getSubmintDefaultJob(this, getConf(), "E:\\JavaProjects\\hdpWork\\target\\hdpWork.jar", args);// InputSampler.Sampler&lt;&gt; job.setMapperClass(SortByTotalPartitionerMap.class); job.setInputFormatClass(MyFileIntputFormat.class); job.setOutputKeyClass(IntWritable.class); job.setOutputValueClass(NullWritable.class); job.setPartitionerClass(TotalOrderPartitioner.class); job.setNumReduceTasks(3); InputSampler.Sampler&lt;Object, IntWritable&gt; sampler = new MySample(0.1, 1000, 10); //构造采样器 TotalOrderPartitioner.setPartitionFile(job.getConfiguration(), new Path("/partitionFile")); //设置共享分区文件路径 InputSampler.writePartitionFile(job, sampler); //写入分区文件,其中调用了 TotalOrderPartitioner.getPartitionFile获取文件路径 //将共享分区文件加入到分布式缓存中 String partitionFile = TotalOrderPartitioner.getPartitionFile(job.getConfiguration()); URI partitionUri = new URI(partitionFile); job.addCacheFile(partitionUri); return job.waitForCompletion(true) ? 0:1; &#125; public static void main(String[] args) throws Exception&#123; System.exit(ToolRunner.run(new SortByTotalPartitioner(), args)); &#125;&#125; 辅助排序有时候我们需要对键和值一起参与排序或者排序收到两个数据的共同影响，由于mapreduce只能支持对键排序，所以只能自定义Writable类型，支持存储多种数据，并重写compareTo()方法或者通过setSortComparatorClass()设置二次排序的方式。由于reduce的输入是key和value的迭代器，默认reduce是将键相同的值放在一个迭代器中，而由于二次排序修改了键的比较方法，因此有时候需要修改分组排序，job.setGroupingComparatorClass设置分组排序，通过分组比较器判断为相同即返回为0的代表在同一组中，这时候的分组的键将会取组中第一个的键，也就是排好序后组中第一个的键。 例子，选取下面数据中，每一年的最高温度 1234567891011121314151617181920212223242526271990 281995 341992 -121994 21995 121993 341992 321993 11994 81995 231993 241993 421994 321992 301991 221993 261990 281992 111990 181994 161992 151994 41990 281995 51990 281992 31990 28 我们需要将年份和温度都参与排序，因此需要自定义Writable类型，同时存储、序列化年份和温度 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;public class IntPairWritable implements WritableComparable&lt;IntPairWritable&gt; &#123; int year; int tempeture; public IntPairWritable()&#123; year = 0; tempeture = 0; &#125; public IntPairWritable(IntWritable year, IntWritable tempeture)&#123; this.year = year.get(); this.tempeture = tempeture.get(); &#125; public IntPairWritable(int year, int tempeture)&#123; this.year = year; this.tempeture = tempeture; &#125; public int getYear() &#123; return year; &#125; public int getTempeture() &#123; return tempeture; &#125; public void setYear(int year) &#123; this.year = year; &#125; public void setTempeture(int tempeture) &#123; this.tempeture = tempeture; &#125; @Override public boolean equals(Object obj) &#123; if(!(obj instanceof IntPairWritable))&#123; return false; &#125; IntPairWritable other = (IntPairWritable)obj; return other.year == year &amp;&amp; other.tempeture == tempeture; &#125; @Override public int compareTo(IntPairWritable o) &#123; int cmp = year - o.getYear(); if(cmp != 0)&#123; return cmp; &#125; return tempeture - o.getTempeture(); &#125; /** * 序列化到输出流中 * @param dataOutput * @throws IOException */ @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeInt(year); dataOutput.writeInt(tempeture); &#125; /** * 从输入流中反序列化读入 * @param dataInput * @throws IOException */ @Override public void readFields(DataInput dataInput) throws IOException &#123; year = dataInput.readInt(); tempeture = dataInput.readInt(); &#125; @Override public int hashCode() &#123; return new Integer(year).hashCode()*163 + new Integer(tempeture).hashCode(); &#125; @Override public String toString() &#123; return year + "\t" + tempeture; &#125; /** * 自定义Writable类型的排序比较器 */ public static class Comparator extends WritableComparator&#123; public Comparator()&#123; super(IntPairWritable.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; int tmp = ((IntPairWritable)a).getYear() - ((IntPairWritable)b).getYear(); if(tmp != 0)&#123; return tmp; &#125; return - (((IntPairWritable)a).getTempeture() - ((IntPairWritable)b).getTempeture()); &#125; &#125;&#125; 按照年份来分区和分组 123456789/** * 只按照年份来分区 */public static class YearPartition extends Partitioner&lt;IntPairWritable, NullWritable&gt;&#123; @Override public int getPartition(IntPairWritable intPairWritable, NullWritable nullWritable, int i) &#123; return Math.abs(intPairWritable.getYear()) % i; &#125;&#125; 12345678910111213/** * 分组只按照年份来比较 */public static class GroupComparator extends WritableComparator&#123; public GroupComparator()&#123; super(IntPairWritable.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; return ((IntPairWritable)a).getYear() - ((IntPairWritable)b).getYear(); &#125;&#125; 这样同年份的数据会在同个分区中，也就是在一个reduce下，reduce的分组是按照年份来比较的，那么每个分组中的数据的年份是相同的，在年份相同的情况下按照温度从大到下排序，因此每个组的键取的是组中的第一个，也就是该年份下最大温度所在的IntPairWritable map、reduce及其入口函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class MaxTemperatureUsingSecondSort extends Configured implements Tool &#123; public static class MaxTemperatureUsingSecondSortMap extends Mapper&lt;LongWritable, Text, IntPairWritable, NullWritable&gt;&#123; IntPairWritable outKey = new IntPairWritable(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] infos = value.toString().split("\t"); outKey.setYear(Integer.parseInt(infos[0])); outKey.setTempeture(Integer.parseInt(infos[1])); context.write(outKey, NullWritable.get()); &#125; &#125; /** * reduce的key存储着该年份下的最高气味 */ public static class MaxTemperatureUsingSecondSortReduce extends Reducer&lt;IntPairWritable, NullWritable, IntPairWritable, NullWritable&gt;&#123; @Override protected void reduce(IntPairWritable key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, NullWritable.get()); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; Job job = JobDefaultInit.getSubmintDefaultJob(this, getConf(), "E:\\JavaProjects\\hdpWork\\target\\hdpWork.jar", args); job.setJobName("Max Temperature Using Second Sort"); job.setMapperClass(MaxTemperatureUsingSecondSortMap.class); job.setReducerClass(MaxTemperatureUsingSecondSortReduce.class); job.setPartitionerClass(YearPartition.class); job.setSortComparatorClass(IntPairWritable.Comparator.class); job.setGroupingComparatorClass(GroupComparator.class); job.setOutputKeyClass(IntPairWritable.class); job.setOutputValueClass(NullWritable.class); return job.waitForCompletion(true) ? 1:0; &#125; public static void main(String[] args) throws Exception &#123; System.exit(ToolRunner.run(new MaxTemperatureUsingSecondSort(), args)); &#125;&#125; 程序结果,由于只有一个reduce，因此年份按照排序从小到大，如果想要在多个reduce中保存年份的全局排序，应该按照全区排序编写合适的分区方法 1234561990 281991 221992 321993 421994 321995 34]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS扩容]]></title>
    <url>%2Fhdfd%E6%89%A9%E5%AE%B9.html</url>
    <content type="text"><![CDATA[hdfs的存储容量不足，需要放入新磁盘扩容扩容有两种方式，一种是linux层面的，一种是hdfs层面的hdfs的datanode存储的目录可以查看hdfs-site.xml的dfs.datanode.data.dir的值 linux层面linux层面的就是将hdfs的datanode所挂载的分区或逻辑卷扩容 如果是直接挂在固定分区上，而磁盘还有余量可以进行分区扩容 参考：http://blog.51cto.com/wutou/1782931这种直接挂载在分区上的磁盘空间管理方式十分不方便，扩容也很容易出错，而且扩容只有在磁盘还有余量的情况下，无法将两块磁盘的空间结合使用，一块磁盘空间占满，即使加入一块新磁盘也无法对原来磁盘上的分区扩容。所以才会有逻辑卷管理LVM的出现 如果是lvm对磁盘分区进行管理，那么可以方便的创建物理卷，扩容datanode所挂载的逻辑卷所属的卷组接着扩容该逻辑卷，具体操作方式参考: https://linux.cn/article-3218-1.html hdfs层面扩容这种不需要对原来的空间扩容，是修改hdfs-site.xml的dfs.datanode.data.dir的值添加新目录，如 1234567&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt; /hadoop/hdfs/data, /data/hadoop/hdfs &lt;/value&gt;&lt;/property 将多个目录用逗号隔开，所以只要将新磁盘分区挂载在新目录上（分区方式可以自己决定,最好永久挂载），将需要添加的新目录设置用户权限1chown -R hdfs:hadoop dirPath 修改dfs.datanode.data.dir，重启datanode即可1./bin/hdfs dfsadmin -report 可以查看各个datanode的存储容量 解决数据倾斜但是这种方式只是增加新磁盘到hdfs中，原来的老磁盘空间依旧被占满。如果老磁盘的容量比新磁盘小，当之后hdfs不断添加新数据，老磁盘依旧占满比新磁盘快，为了防止hdfs的存储容量无限扩张占满磁盘，甚至导致系统盘占满，影响这个系统，通过设置dfs.datanode.du.reserved来预留磁盘空间。该参数在hdfs-site.xml中设置。 1234&lt;property&gt; &lt;name&gt;dfs.datanode.du.reserved&lt;/name&gt; &lt;value&gt;32212254720&lt;/value&gt; &lt;/property&gt; 这里设置预留了30G的磁盘空间（参数单位为byte），也就是当磁盘的空间剩余30G及以下时，将不会存放数据副本到该磁盘。 但是这还是没有解决目前刚刚扩容时，老磁盘空间依旧被占满的问题。这里就时需要移动老磁盘的副本到新磁盘，但是目前hdfs并没有提供该功能，因此只能曲线救国，删除一些不必要的文件（一些文件的副本可能存在老磁盘中）。 如果不想动任何数据的情况下，可以采用升降副本的方法，如原来发副本数量为3，降低副本数量为2，这样会删除一些副本，会删除存在老磁盘上从一些副本，接着在升高副本数量到原来发副本数量，这样由于设置了磁盘预留空间，新副本会存放在新磁盘当中,升降副本采用hdfs的setrep -w命令,-R为递归调整整个文件夹下的所有文件。 12hadoop dfs -setrep -w -R 2 pathhadoop dfs -setrep -w -R 3 path 最后使用Hadoop balancer工具来调整各个datanodes的数据平衡。 官方对balancer的描述： The balancer is a tool that balances disk space usage on an HDFS cluster when some datanodes become full or when new empty nodes join the cluster. The tool is deployed as an application program that can be run by the cluster administrator on a live HDFS cluster while applications adding and deleting files. balancer命令详解： 123456789hdfs balancer [-policy &lt;policy&gt;] [-threshold &lt;threshold&gt;] [-exclude [-f &lt;hosts-file&gt; | &lt;comma-separated list of hosts&gt;]] [-include [-f &lt;hosts-file&gt; | &lt;comma-separated list of hosts&gt;]] [-source [-f &lt;hosts-file&gt; | &lt;comma-separated list of hosts&gt;]] [-blockpools &lt;comma-separated list of blockpool ids&gt;] [-idleiterations &lt;idleiterations&gt;] [-runDuringUpgrade] COMMAND_OPTION Description -policy datanode (default): Cluster is balanced if each datanode is balanced. blockpool: Cluster is balanced if each block pool in each datanode is balanced. -threshold Percentage of disk capacity. This overwrites the default threshold. -exclude -f \ Excludes the specified datanodes from being balanced by the balancer. -include -f \ Includes only the specified datanodes to be balanced by the balancer. -source -f \ Pick only the specified datanodes as source nodes. -blockpools The balancer will only run on blockpools included in this list. -idleiterations Maximum number of idle iterations before exit. This overwrites the default idleiterations. -runDuringUpgrade Whether to run the balancer during an ongoing HDFS upgrade. This is usually not desired since it will not affect used space on over-utilized machines. -h --help 这里我只是调小datanode之间的百分比的差 1hdfs balancer -threshold 5 这样可以解决hadoop数据出现不均衡情况。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[非整数0-1背包问题]]></title>
    <url>%2F%E9%9D%9E%E6%95%B4%E6%95%B00-1%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[0-1背包问题通常情况下物品的重量是整数的，采用动态规划可以解决，在解决物品重量非整数情况下的背包问题之前，我们先来回顾整数背包问题，并从中寻找解决非整数背包问题的方法。 问题定义：有n种物品和一个容量为$c$的背包，第$i$件物品的重量为$wi$，价格为$vi$,求出哪种物品组合放入背包使物品价值总和最大。 整数0-1背包问题设$p(i,j)$表示在容量为j情况下，将物品$i，i+1…n$组合的放入背包的最优解的值 则其转移方程为 如果 $j&gt;=w_i$ ，$p(i,j) = max(p(i+1,j), p(i+1,j-w_i )+v_i )$ 如果 $j&lt;w_i$ , $p(i,j) = p(i+1,j)$ 可以理解为当$j&lt;w_i$ ,背包无法放下第$i$件物品，所以其最优解和考虑放$i+1$到$n$的物品到容量为j的背包的最优解相同。当$j&gt;=w_i$时，可以选择放和不放第$i$件物品，当不放时背包价值和$p(i+1,j)$相同，当放时背包价值为$p(i+1,j-w_i )$再加上第$i$件物品的价值。 所以整数背包问题可以采用动态规划解决，开辟$n*c$的数组，从下往上不断更新数组的值，得到$p(i,j)$的值，最优解就是$p(0,c)$的值 123456789101112131415161718192021public static int solution(int c, int[] v, int[] w)&#123; if(v.length != w.length)&#123; throw new IllegalArgumentException(); &#125; int n = v.length;int m = c+1; int[][] result = new int[n][m]; for(int i = 0;i &lt; m;i++)&#123; result[n-1][i] = i&gt;=w[n-1]? v[n-1]:0; &#125; for(int i = n-2;i&gt;=0;i--)&#123; for(int j = 0;j &lt; m;j++)&#123; if(j &lt; w[i])&#123; result[i][j] = result[i+1][j]; &#125;else &#123; result[i][j] = Math.max(result[i+1][j], result[i+1][j-w[i]]+v[i]); &#125; &#125; &#125; traceBack(c, v, w, result); return result[0][c];&#125; 路径回溯，找到最优组合 123456789101112131415private static void traceBack(int c, int[]v, int[] w, int[][] p)&#123; int k = c; List&lt;Integer&gt; trace = new ArrayList&lt;&gt;(); int i; for(i = 0;i &lt; p.length-1;i++)&#123; if(p[i][k] == p[i+1][k-w[i]]+v[i])&#123; k = k - w[i]; trace.add(i+1); &#125; &#125; if(p[i][k] == v[i])&#123; trace.add(i+1); &#125; System.out.println(trace);&#125; 整数0-1背包问题的改进例如背包的容量为10，5件物品的重量分别为2，2，6，5，4 ，对应价值分别是6，3，5，4，6，则$p(i,j)​$数组的更新情况如下 weight value 1 2 3 4 5 6 7 8 9 10 2 6 0 6 6 9 9 12 12 15 15 15 2 3 0 3 3 6 6 9 9 9 10 11 6 5 0 0 0 6 6 6 6 6 10 11 5 4 0 0 0 6 6 6 6 6 10 10 4 6 0 0 0 6 6 6 6 6 6 6 当背包的容量很大（即c的值特别大），则这个数组将会异常的庞大，并且数组的每一个数都需要更新，算法需要的计算时间较多。 所以可以进行适当的改进，从上面的表格来看，我们无需记录数组种的每一个数，只需要记录下每行的跳跃点即可，比如最后一行从第4列开始跳跃，我们只需记录{(0,0),(4,6)}即可,倒数第二行只需记录{(0,0),(4,6),(9,10)}即可。接下来的问题是如何更新每行的跳跃点。 p[5] = {(0,0),(4,6)} 将p[5]的跳跃点加上下一个需要放入的物品的重量和价值，则可以得到q[5] = p[5]+(5,4) = {(5,4),(9,10)}, 将p[5]和q[5]合并得到{(0,0),(4,6),(5,4),(9,10)}，之后去除重量大却价值小的点，如(5,4)点，放入背包的物品总重量大于4，价值却只有4小于6，所以通过比较(5,4)和(4,6)需要去掉(5,4)得到p[4] = {(0,0),(4,6),(9,10)} 非整数0-1背包问题非整数0-1背包问题可以转化为整数0-1背包问题，如果非整数可以用保留三位小数来表示的话，那么可以将非整数背包问题的所有值乘上1000,全部转为整数，采用整数背包问题解决，但是这样必须牺牲一定的精度，而且会增加开辟数组的大小（乘上1000，数组的列肯定超过1000以上），这对计算时间也有很大影响，因为需要更新每一个数组中的元素。 有效的解决方法和上述对于整数0-1背包问题的改进是一样的，而且发现上述的跳跃点并不要求是整数，对于实数一样适用，因此可以采用更新跳跃点的动态规划的方式解决非整数0-1背包问题。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public static double solution(double c, double[] v, double[] w)&#123; if(v.length != w.length)&#123; throw new IllegalArgumentException(); &#125; double[][] p = new double[10000][2]; int n = v.length; p[0][0] = 0;p[0][1] = 0; int left = 0, right = 0,next = 1; int[] head = new int[n+2]; head[n+1] = 0; head[n] = 1; for(int i = n-1; i &gt;= 0;i--)&#123; int k = left; for(int j = left; j &lt;= right;j++)&#123; if(p[j][0] + w[i] &gt; c)&#123; break; &#125; double nw = p[j][0] + w[i]; double nv = p[j][1] + v[i]; //放入比nw小的跳跃点，因为重量小的价值无论大小 for(;k&lt;=right &amp;&amp; p[k][0] &lt; nw;k++,next++)&#123; p[next][0] = p[k][0]; p[next][1] = p[k][1]; &#125; //如果重量相等，取价值大的跳跃点 if(k &lt;= right &amp;&amp; p[k][0] == nw)&#123; if(p[k][1] &gt; nv)&#123; nv = p[k][1]; &#125; k++; &#125; //放入更新的跳跃点 if(nv &gt; p[next-1][1])&#123; p[next][0] = nw; p[next][1] = nv; next++; &#125; /*去除比更新的跳跃点重量大却价值小的点， 由于是每一次更新完之后结果都是重量和价值都是递增的跳跃点排列 一旦出现价值超过当前的点，那后续的点一定都是超过的*/ for(;k &lt;= right &amp;&amp; p[k][1] &lt;= nv;k++); &#125; //将后续的点放入 for (;k &lt;= right; k++,next++)&#123; p[next][0] = p[k][0]; p[next][1] = p[k][1]; &#125; left = right+1;right = next - 1;head[i] = next; &#125; traceBack(v, w, p, head); return p[next-1][1];&#125; 路径回溯，找到最优组合 1234567891011121314151617private static void traceBack(double[] v, double[] w, double[][] p, int[] head)&#123; List&lt;Integer&gt; trace = new ArrayList&lt;&gt;(); int k = head[0]-1; int n = w.length; for(int i = 1;i &lt;= n;i++)&#123; int left = head[i+1]; int right = head[i]-1; for(int j = left;j&lt;=right;j++)&#123; if(p[j][0] + w[i-1] == p[k][0] &amp;&amp; p[j][1] + v[i-1] == p[k][1])&#123; k = j; trace.add(i); break; &#125; &#125; &#125; System.out.println(trace);&#125; 如果不考虑放入哪些物品（即不考虑路径回溯），之关心最优解的值，可以只存放当前的跳跃点集和下一步的跳跃点集，无需记录每一步的跳跃点集 12345678910111213141516171819202122232425262728293031323334353637public static double solution2(double c, double[] v, double[] w)&#123; if(v.length != w.length)&#123; throw new IllegalArgumentException(); &#125; int n = v.length; List&lt;double[]&gt; p = new ArrayList&lt;&gt;(); p.add(new double[]&#123;0, 0&#125;); for(int i = n - 1; i &gt;= 0;i--)&#123; int k = 0; List&lt;double[]&gt; q = new ArrayList&lt;&gt;(); for(double[] element: p)&#123; if(w[i] + element[0] &gt; c)&#123; break; &#125; double nw = w[i] + element[0]; double nv = v[i] + element[1]; for(;k &lt; p.size() &amp;&amp; p.get(k)[0] &lt; nw;k++)&#123; q.add(p.get(k)); &#125; if(k &lt; p.size() &amp;&amp; p.get(k)[0] == nw)&#123; if(p.get(k)[1] &gt; nv)&#123; nv = p.get(k)[1]; &#125; k++; &#125; if(nv &gt; q.get(q.size()-1)[1])&#123; q.add(new double[]&#123;nw, nv&#125;); &#125; for(;k &lt; p.size() &amp;&amp; p.get(k)[1] &lt; nv;k++); &#125; for(;k &lt; p.size();k++)&#123; q.add(p.get(k)); &#125; p = q; &#125; return p.get(p.size()-1)[1];&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>背包问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop的RPC分析]]></title>
    <url>%2FHadoop%E7%9A%84RPC%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[RPCRPC就是远程过程调用,具体什么是RPC，看一个例子就会明白。比如客户端有一个RPC协议类Protocol。 123interfce Protocol&#123; int add(int a, int b);&#125; 但是客户端没有其实现的具体类，该类在服务端12345Class ProtocolImpl implenets Protocol&#123; int add(int a, int b)&#123; return a + b; &#125;&#125; 则客户端需要调用ProtocolImpl的add方法，需要将调用的方法及其参数等信息发送给服务端，服务端解析信息，调用ProtocolImpl的add方法，将结果在传输给客户端，而RPC的目的就是使客户端仿佛在调用自身的方法来得到该方法的结果。在这其中，Protocol就是一个RPC的协议，这个协议其实就是一个接口类，接口中的方法就是对外提供的远程调用方法。 RPC由四个模块组成：1、通信模块。两个相互协作的通信模块实现请求-应答协议,它们在客户和服务器之间传递请求和应答消息,一般不会对数据包进行任何处理。请求–应答协议的实现方式有同步方式和异步方式两种。同步模式下客户端程序一直阻塞到服务器端发送的应答请求到达本地; 而异步模式不同,客户端将请求发送到服务器端后,不必等待应答返回,可以做其他事情,待服务器端处理完请求后,主动通知客户端。在高并发应用场景中,一般采用异步模式以降低访问延迟和提高带宽利用率。2、Stub 程序（代理程序）。客户端和服务器端均包含Stub程序,可将之看做代理程序。它使得远程函数调用表现得跟本地调用一样,对用户程序完全透明。在客户端,它表现得就像一个本地程序,但不直接执行本地调用,而是将请求信息通过网络模块发送给服务器端。此外,当服务器发送应答后,它会解码对应结果。在服务器端,Stub程序依次进行解码请求消息中的参数、调用相应的服务过程和编码应答结果的返回值等处理。3、调度程序。调度程序接收来自通信模块的请求消息,并根据其中的标识选择一个Stub程序进行处理。通常客户端并发请求量比较大时,会采用线程池提高处理效率。4、客户程序/服务过程。请求的发出者和请求的处理者。 Hadoop RPCHadoop RPC主要分为四个部分,分别是序列化层、函数调用层、网络传输层和服务器端处理框架,具体实现机制如下: 序列化层。序列化主要作用是将结构化对象转为字节流以便于通过网络进行传输或写入持久存储,在RPC框架中,它主要用于将用户请求中的参数或者应答转化成字节流以便跨机器传输。Hadoop2.0之后，主要用Protocol Buffers和Apache Avro，Hadoop本身也提供了一套序列化框架，一个类只要实现Writable接口即可支持对象序列化与反序列化。 函数调用层。函数调用层主要功能是定位要调用的函数并执行该函数，Hadoop RPC采用了Java反射机制（服务器端）与动态代理（客户端）实现了函数调用。 网络传输层。网络传输层描述了Client与Server之间消息传输的方式，Hadoop RPC采用了基于TCP/IP的Socket机制。 服务器端处理框架。服务器端处理框架可被抽象为网络I/O模型，它描述了客户端与服务器端间信息交互方式,它的设计直接决定着服务器端的并发处理能力,常见的网络 I/O 模型有阻塞式 I/O、非阻塞式 I/O、事件驱动 I/O 等,而Hadoop RPC采用了基于Reactor设计模式的事件驱动 I/O 模型（NIO）。 Hadoop RPC的简单使用首先需要定义一个PRC协议，该接口必须继承VersionedProtocol接口1234public interface IProxyProtocol extends VersionedProtocol&#123; long versionID = 1234L; int add(int a, int b);&#125; 接着需要一个类实现PRC的接口用于服务端的调用123456789101112131415161718public class MyProxyProtocol implements IProxyProtocol &#123; @Override public int add(int a, int b) &#123; System.out.println("I am adding"); return a+b; &#125; @Override public long getProtocolVersion(String s, long l) throws IOException &#123; System.out.println("MyProxy.ProtocolVersion = " + IProxyProtocol.versionID ); return IProxyProtocol.versionID ; &#125; @Override public ProtocolSignature getProtocolSignature(String s, long l, int i) throws IOException &#123; return new ProtocolSignature(IProxyProtocol.versionID , null); &#125;&#125; 最后是实现客户端和服务端1234567891011121314public class MyRPCClient &#123; public final static int PORT = 8888; public final static String ADDRESS = "localhost"; public static void main(String[] args) throws Exception&#123; Configuration conf = new Configuration(); IProxyProtocol proxy; proxy = RPC.getProxy(IProxyProtocol.class, 111 , new InetSocketAddress(ADDRESS, PORT), conf); int result = proxy.add(2, 3); System.out.println(result); RPC.stopProxy(proxy); &#125;&#125; 1234567891011121314public class MyRPCServer &#123; public final static int PORT = 8888; public final static String ADDRESS = "localhost"; public static void main(String[] args) throws IOException&#123; RPC.Server server = new RPC.Builder(new Configuration()) .setProtocol(IProxyProtocol.class) .setInstance(new MyProxyProtocol()) .setBindAddress(ADDRESS) .setPort(PORT) .setNumHandlers(10) .build(); server.start(); &#125;&#125; 上述的Hadoop RPC的实现，客户端是调用了 RPC.getProxy方法，生成了IProxyProtocol的动态代理，在调用协议的方法时，代理类会将方法和参数出给服务端，服务端使用具体的实现类来执行方法并返回结果给客户端。需要注意的是RPC协议必须声明versionID这个变量或者定义ProtocolInfo注解（包含看协议的名字和versionID） 客户端的实现首先我们来看Hadoop RPC的客户端代码，调用RPC.getProxy方法，追踪这个方法，可以看到其内部实现1234public static &lt;T&gt; T getProxy(Class&lt;T&gt; protocol,long clientVersion,InetSocketAddress addr, Configuration conf) throws IOException &#123; return getProtocolProxy(protocol, clientVersion, addr, conf).getProxy();&#125; 内部调用了getProtocolProxy，而getProtocolProxy又经过几次重载函数的调用，最后的实现是123456789101112131415161718192021222324252627282930313233/** * Get a protocol proxy that contains a proxy connection to a remote server * and a set of methods that are supported by the server * @param protocol protocol RPC协议 * @param clientVersion client's version * @param addr server address 服务端的地址 ip:port * @param ticket security ticket * @param conf configuration * @param factory socket factory * @param rpcTimeout max time for each rpc; 0 means no timeout * @param connectionRetryPolicy retry policy * @param fallbackToSimpleAuth set to true or false during calls to indicate if * a secure client falls back to simple auth * @return the proxy * @throws IOException if any error occurs */ public static &lt;T&gt; ProtocolProxy&lt;T&gt; getProtocolProxy(Class&lt;T&gt; protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth) throws IOException &#123; if (UserGroupInformation.isSecurityEnabled()) &#123; SaslRpcServer.init(conf); &#125; return getProtocolEngine(protocol, conf).getProxy(protocol, clientVersion, addr, ticket, conf, factory, rpcTimeout, connectionRetryPolicy, fallbackToSimpleAuth);&#125; 其中ProtocolProxy封装了动态代理类和PRC协议，ProtocolProxy.getProxy会返回生成的动态代理类。而ProtocolProxy类是getProtocolEngine返回的。12345678910111213141516/** * 如果缓存中存在该协议的RpcEngine，则直接调用 * 否则就从Configuration中取出rpc.engine.protocol.getName()的RpeEngine类 * 默认为WritableRpcEngine类 */static synchronized RpcEngine getProtocolEngine(Class&lt;?&gt; protocol, Configuration conf) &#123; RpcEngine engine = PROTOCOL_ENGINES.get(protocol); if (engine == null) &#123; Class&lt;?&gt; impl = conf.getClass(ENGINE_PROP+"."+protocol.getName(), WritableRpcEngine.class); engine = (RpcEngine)ReflectionUtils.newInstance(impl, conf); PROTOCOL_ENGINES.put(protocol, engine); &#125; return engine;&#125; 可以看到RpcEngine从Configuration读取，如果没有，默认设置是WritableRpcEngine。这是Hadoop RPC对序列化方式多样性的支持，目前提供了Writable(WritableRpcEngine)和Protocol Buffer(ProtocolRpcEngine)两种，用户也可以通过RPC.setProtocolEngine()设置。这里我们只分析WritableRpcEngine，跳转到WritableRpcEngine.getProxy方法123456789101112131415public &lt;T&gt; ProtocolProxy&lt;T&gt; getProxy(Class&lt;T&gt; protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth) throws IOException &#123; //默认为null if (connectionRetryPolicy != null) &#123; throw new UnsupportedOperationException( "Not supported: connectionRetryPolicy=" + connectionRetryPolicy); &#125; T proxy = (T) Proxy.newProxyInstance(protocol.getClassLoader(), new Class[] &#123; protocol &#125;, new Invoker(protocol, addr, ticket, conf, factory, rpcTimeout, fallbackToSimpleAuth)); return new ProtocolProxy&lt;T&gt;(protocol, proxy, true);&#125; 可以看到这里使用了动态代理生成的了代理类，封装进了ProtocolProxy类中，而RPC.getProxy中是返回了ProtocolProxy.getProxy的结果。则调用PRC协议的方法会触发代理类的invoke方法，客户端就是在invoke方法中实现了方法名和参数的传递和结果的接受。因此我们继续看Invoker类，这是WritableRpcEngine的内部类123456789101112131415161718192021222324252627282930313233343536373839404142434445private static class Invoker implements RpcInvocationHandler &#123; //ConnectionId中包括了服务端的地址，协议类，安全令牌等，用来唯一标识Client.Connection类 private Client.ConnectionId remoteId; private Client client; private boolean isClosed = false; private final AtomicBoolean fallbackToSimpleAuth; public Invoker(Class&lt;?&gt; protocol, InetSocketAddress address, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, AtomicBoolean fallbackToSimpleAuth) throws IOException &#123; this.remoteId = Client.ConnectionId.getConnectionId(address, protocol, ticket, rpcTimeout, conf); this.client = CLIENTS.getClient(conf, factory); this.fallbackToSimpleAuth = fallbackToSimpleAuth; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; long startTime = 0; if (LOG.isDebugEnabled()) &#123; startTime = Time.now(); &#125; TraceScope traceScope = null; if (Trace.isTracing()) &#123; traceScope = Trace.startSpan(RpcClientUtil.methodToTraceString(method)); &#125; ObjectWritable value; try &#123; value = (ObjectWritable) client.call(RPC.RpcKind.RPC_WRITABLE, new Invocation(method, args), remoteId, fallbackToSimpleAuth); &#125; finally &#123; if (traceScope != null) traceScope.close(); &#125; if (LOG.isDebugEnabled()) &#123; long callTime = Time.now() - startTime; LOG.debug("Call: " + method.getName() + " " + callTime); &#125; return value.get(); &#125; ...&#125; 可以看到invoke中是调用了Client.call方法来进行远程函数调用，Client的获得是先从CLIENTS中的缓存（其实就是内部维护了一个HashMap&lt;SocketFactory, Client&gt;）,如果没有就根据SocketFactory和序列化类型实例化一个并放入其中。call方法的一个参数new Invocation(method, args),其实是用来序列化方法类及其传递的参数的。Invocation类实现了Writable接口，Writable是hadoop用来序列化的接口。12345678910111213141516171819202122232425262728293031323334353637383940private static class Invocation implements Writable, Configurable &#123; private String methodName; //方法名 private Class&lt;?&gt;[] parameterClasses; //参数类型 private Object[] parameters; //参数实例对象 private Configuration conf; private long clientVersion; //客户端version,主要从方法所在类的ProtocolInfo注解中的version或者versionID中获得 private int clientMethodsHash; //方法所在类所有方法的形成的hash private String declaringClassProtocolName; //要从方法所在类的ProtocolInfo注解中的name ... public void readFields(DataInput in) throws IOException &#123; rpcVersion = in.readLong(); declaringClassProtocolName = UTF8.readString(in); methodName = UTF8.readString(in); clientVersion = in.readLong(); clientMethodsHash = in.readInt(); parameters = new Object[in.readInt()]; parameterClasses = new Class[parameters.length]; ObjectWritable objectWritable = new ObjectWritable(); for (int i = 0; i &lt; parameters.length; i++) &#123; parameters[i] = ObjectWritable.readObject(in, objectWritable, this.conf); parameterClasses[i] = objectWritable.getDeclaredClass(); &#125; &#125; @Override @SuppressWarnings("deprecation") public void write(DataOutput out) throws IOException &#123; out.writeLong(rpcVersion); UTF8.writeString(out, declaringClassProtocolName); UTF8.writeString(out, methodName); out.writeLong(clientVersion); out.writeInt(clientMethodsHash); out.writeInt(parameterClasses.length); for (int i = 0; i &lt; parameterClasses.length; i++) &#123; ObjectWritable.writeObject(out, parameters[i], parameterClasses[i], conf, true); &#125; &#125; ...&#125; 继续我们追踪Client.call，来了的Client类中（客户端核心的类），通过重载函数的调用，最后定位到了1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Make a call, passing &lt;code&gt;rpcRequest&lt;/code&gt;, to the IPC server defined by * &lt;code&gt;remoteId&lt;/code&gt;, returning the rpc response. * @param rpcKind * @param rpcRequest - contains serialized method and method parameters 包含序列化的方法和方法参数 * @param remoteId - the target rpc server * @param serviceClass - service class for RPC * @param fallbackToSimpleAuth - set to true or false during this method to * indicate if a secure client falls back to simple auth * @returns the rpc response */ public Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass, AtomicBoolean fallbackToSimpleAuth) throws IOException &#123; final Call call = createCall(rpcKind, rpcRequest); //将序列化的信息封装进Call中 Connection connection = getConnection(remoteId, call, serviceClass, fallbackToSimpleAuth); //获得Connection对象，其中封装了socket，连接到服务端 try &#123; connection.sendRpcRequest(call); // send the rpc &#125; catch (RejectedExecutionException e) &#123; throw new IOException("connection has been closed", e); &#125; catch (InterruptedException e) &#123; Thread.currentThread().interrupt(); LOG.warn("interrupted waiting to send rpc request to server", e); throw new IOException(e); &#125; synchronized (call) &#123; while (!call.done) &#123; try &#123; call.wait(); // wait for the result &#125; catch (InterruptedException ie) &#123; Thread.currentThread().interrupt(); throw new InterruptedIOException("Call interrupted"); &#125; &#125; if (call.error != null) &#123; if (call.error instanceof RemoteException) &#123; call.error.fillInStackTrace(); throw call.error; &#125; else &#123; // local exception InetSocketAddress address = connection.getRemoteAddress(); throw NetUtils.wrapException(address.getHostName(), address.getPort(), NetUtils.getHostname(), 0, call.error); &#125; &#125; else &#123; return call.getRpcResponse(); &#125; &#125; &#125; 从中可以看出首先是实例化一个Call对象，封装了输送的内容,Connection负责连接服务端，接受返回信息，放入对应的Call中，并唤醒Call,读出返回数据。其中一个Connection负责同个服务端地址，同个RPC协议，同个安全令牌的连接下的所有Call中内容的发送和接受，connection维护了Call的集合，通过callid知道对应返回的数据和发送的数据属于哪个Call对象，从后文可以看出。这里主要调用了createCall创造了Call对象，getConnection连接服务端，connection.sendRpcRequest发送请求并接受返回。接下来就深入这三个函数。1234567891011121314151617181920212223242526272829303132333435 Call createCall(RPC.RpcKind rpcKind, Writable rpcRequest) &#123; return new Call(rpcKind, rpcRequest); &#125; static class Call &#123; final int id; // call id final int retry; // retry count final Writable rpcRequest; // the serialized rpc request Writable rpcResponse; // null if rpc has error IOException error; // exception, null if success final RPC.RpcKind rpcKind; // Rpc EngineKind boolean done; // true when call is done private Call(RPC.RpcKind rpcKind, Writable param) &#123; this.rpcKind = rpcKind; this.rpcRequest = param; //生成callId,每一个Call实例对象都有唯一的id,为了connection接受消息后放入对应id的Call中 final Integer id = callId.get(); if (id == null) &#123; this.id = nextCallId(); &#125; else &#123; callId.set(null); this.id = id; &#125; final Integer rc = retryCount.get(); if (rc == null) &#123; this.retry = 0; &#125; else &#123; this.retry = rc; &#125; &#125; ...&#125; 12345678910111213141516171819202122232425262728293031/** Get a connection from the pool, or create a new one and add it to the * pool. Connections to a given ConnectionId are reused. */private Connection getConnection(ConnectionId remoteId, Call call, int serviceClass, AtomicBoolean fallbackToSimpleAuth) throws IOException &#123; if (!running.get()) &#123; // the client is stopped throw new IOException("The client is stopped"); &#125; Connection connection; /* we could avoid this allocation for each RPC by having a * connectionsId object and with set() method. We need to manage the * refs for keys in HashMap properly. For now its ok. */ do &#123; synchronized (connections) &#123; connection = connections.get(remoteId); if (connection == null) &#123; connection = new Connection(remoteId, serviceClass); connections.put(remoteId, connection); &#125; &#125; &#125; while (!connection.addCall(call)); //we don't invoke the method below inside "synchronized (connections)" //block above. The reason for that is if the server happens to be slow, //it will take longer to establish a connection and that will slow the //entire system down. connection.setupIOstreams(fallbackToSimpleAuth); return connection;&#125; 其中维护了一个连接池（其实就是一个HashTable，保证线程安全），remoteId作为其key，当连接池没有连接则会新建一个Connection并将其添加到连接池中，初始化Connection就是一些赋值，可以在Connection中看到Socket，实际上是调用了Socket建立了TCP连接，并且Connection继承了Thread，在建立连接后会启动线程，不断等待结果的相应，在下文中可以看到。在”synchronized (connections)”代码块中是不会开始建立连接的，因为如果在同步代码块中连接会造成阻塞，降低整个系统的效率。真正的建立连接是connection.setupIOstreams中会调用的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/** Connect to the server and set up the I/O streams. It then sends * a header to the server and starts * the connection thread that waits for responses. * 建立channle的in和outStream,启动connection的线程接受返回消息 */private synchronized void setupIOstreams( AtomicBoolean fallbackToSimpleAuth) &#123; if (socket != null || shouldCloseConnection.get()) &#123; return; &#125; try &#123; if (LOG.isDebugEnabled()) &#123; LOG.debug("Connecting to "+server); &#125; if (Trace.isTracing()) &#123; Trace.addTimelineAnnotation("IPC client connecting to " + server); &#125; short numRetries = 0; Random rand = null; while (true) &#123; setupConnection(); //建立socket连接 InputStream inStream = NetUtils.getInputStream(socket); //获得输入流 OutputStream outStream = NetUtils.getOutputStream(socket); //获得输出流 writeConnectionHeader(outStream); //传输连接头 ... if (doPing) &#123; inStream = new PingInputStream(inStream); &#125; this.in = new DataInputStream(new BufferedInputStream(inStream)); // SASL may have already buffered the stream if (!(outStream instanceof BufferedOutputStream)) &#123; outStream = new BufferedOutputStream(outStream); &#125; this.out = new DataOutputStream(outStream); writeConnectionContext(remoteId, authMethod); //传输上下文 // update last activity time touch(); if (Trace.isTracing()) &#123; Trace.addTimelineAnnotation("IPC client connected to " + server); &#125; // start the receiver thread after the socket connection has been set // up start(); return; &#125; &#125; catch (Throwable t) &#123; if (t instanceof IOException) &#123; markClosed((IOException)t); &#125; else &#123; markClosed(new IOException("Couldn't set up IO streams", t)); &#125; close(); &#125;&#125;/** * Write the connection header - this is sent when connection is established * +----------------------------------+ * | "hrpc" 4 bytes | * +----------------------------------+ * | Version (1 byte) | * +----------------------------------+ * | Service Class (1 byte) | * +----------------------------------+ * | AuthProtocol (1 byte) | * +----------------------------------+ */private void writeConnectionHeader(OutputStream outStream) throws IOException &#123; DataOutputStream out = new DataOutputStream(new BufferedOutputStream(outStream)); // Write out the header, version and authentication method out.write(RpcConstants.HEADER.array()); out.write(RpcConstants.CURRENT_VERSION); out.write(serviceClass); out.write(authProtocol.callId); out.flush();&#125; 这里的整个过程就是setupConnection方法建立连接，得到输入输出流，再传输Hadoop PRC的连接头和上下文（Configuration中设置的一些RPC的参数）。连接头从注释中可以看到，下文可以看到server端接受解析连接头。最后启动线程，不断等待接收相应结果。主要连接方法是setupConnection方法。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253private synchronized void setupConnection() throws IOException &#123; short ioFailures = 0; short timeoutFailures = 0; while (true) &#123; try &#123; this.socket = socketFactory.createSocket(); this.socket.setTcpNoDelay(tcpNoDelay); this.socket.setKeepAlive(true); /* * Bind the socket to the host specified in the principal name of the * client, to ensure Server matching address of the client connection * to host name in principal passed. */ UserGroupInformation ticket = remoteId.getTicket(); if (ticket != null &amp;&amp; ticket.hasKerberosCredentials()) &#123; KerberosInfo krbInfo = remoteId.getProtocol().getAnnotation(KerberosInfo.class); if (krbInfo != null &amp;&amp; krbInfo.clientPrincipal() != null) &#123; String host = SecurityUtil.getHostFromPrincipal(remoteId.getTicket().getUserName()); // If host name is a valid local address then bind socket to it InetAddress localAddr = NetUtils.getLocalInetAddress(host); if (localAddr != null) &#123; this.socket.bind(new InetSocketAddress(localAddr, 0)); &#125; &#125; &#125; NetUtils.connect(this.socket, server, connectionTimeout); if (rpcTimeout &gt; 0) &#123; pingInterval = rpcTimeout; // rpcTimeout overwrites pingInterval &#125; this.socket.setSoTimeout(pingInterval); return; &#125; catch (ConnectTimeoutException toe) &#123; /* Check for an address change and update the local reference. * Reset the failure counter if the address was changed */ if (updateAddress()) &#123; timeoutFailures = ioFailures = 0; &#125; handleConnectionTimeout(timeoutFailures++, maxRetriesOnSocketTimeouts, toe); &#125; catch (IOException ie) &#123; if (updateAddress()) &#123; timeoutFailures = ioFailures = 0; &#125; handleConnectionFailure(ioFailures++, ie); &#125; &#125;&#125; 12345public class StandardSocketFactory extends SocketFactory &#123; public Socket createSocket() throws IOException &#123; return SocketChannel.open().socket(); &#125;&#125; setupConnection中调用了socketFactory.createSocket()根据具体不同实现大SocketFactory来创建Socket，默认是StandardSocketFactory实现，这里通过SocketChannel来得到socket,socket在setupConnection中设置，建立长连接和超时时间（默认为一分钟，socket.setSoTimeout设置）。最后通过NetUtils.connect建立连接。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103public static void connect(Socket socket, SocketAddress address, int timeout) throws IOException &#123; connect(socket, address, null, timeout); &#125; public static void connect(Socket socket, SocketAddress endpoint, SocketAddress localAddr, int timeout) throws IOException &#123; if (socket == null || endpoint == null || timeout &lt; 0) &#123; throw new IllegalArgumentException("Illegal argument for connect()"); &#125; SocketChannel ch = socket.getChannel(); if (localAddr != null) &#123; Class localClass = localAddr.getClass(); Class remoteClass = endpoint.getClass(); Preconditions.checkArgument(localClass.equals(remoteClass), "Local address %s must be of same family as remote address %s.", localAddr, endpoint); socket.bind(localAddr); &#125; try &#123; if (ch == null) &#123; // let the default implementation handle it. socket.connect(endpoint, timeout); &#125; else &#123; SocketIOWithTimeout.connect(ch, endpoint, timeout); &#125; &#125; catch (SocketTimeoutException ste) &#123; throw new ConnectTimeoutException(ste.getMessage()); &#125; // There is a very rare case allowed by the TCP specification, such that // if we are trying to connect to an endpoint on the local machine, // and we end up choosing an ephemeral port equal to the destination port, // we will actually end up getting connected to ourself (ie any data we // send just comes right back). This is only possible if the target // daemon is down, so we'll treat it like connection refused. if (socket.getLocalPort() == socket.getPort() &amp;&amp; socket.getLocalAddress().equals(socket.getInetAddress())) &#123; LOG.info("Detected a loopback TCP socket, disconnecting it"); socket.close(); throw new ConnectException( "Localhost targeted connection resulted in a loopback. " + "No daemon is listening on the target port."); &#125; &#125; /** * SocketIOWithTimeout.connect方法 */ static void connect(SocketChannel channel, SocketAddress endpoint, int timeout) throws IOException &#123; boolean blockingOn = channel.isBlocking(); if (blockingOn) &#123; channel.configureBlocking(false); &#125; try &#123; if (channel.connect(endpoint)) &#123; return; &#125; long timeoutLeft = timeout; long endTime = (timeout &gt; 0) ? (Time.now() + timeout): 0; while (true) &#123; // we might have to call finishConnect() more than once // for some channels (with user level protocols) int ret = selector.select((SelectableChannel)channel, SelectionKey.OP_CONNECT, timeoutLeft); if (ret &gt; 0 &amp;&amp; channel.finishConnect()) &#123; return; &#125; if (ret == 0 || (timeout &gt; 0 &amp;&amp; (timeoutLeft = (endTime - Time.now())) &lt;= 0)) &#123; throw new SocketTimeoutException( timeoutExceptionString(channel, timeout, SelectionKey.OP_CONNECT)); &#125; &#125; &#125; catch (IOException e) &#123; // javadoc for SocketChannel.connect() says channel should be closed. try &#123; channel.close(); &#125; catch (IOException ignored) &#123;&#125; throw e; &#125; finally &#123; if (blockingOn &amp;&amp; channel.isOpen()) &#123; channel.configureBlocking(true); &#125; &#125; &#125; 可以看到实际上就是socket或socketChannel（设置为不阻塞）的连接。和服务端的连接建立连接之后就是发送Call中的内容到服务端，并接收其相应结果，返回到了Client中的call方法，通过Connection.sendRpcRequest来发送方法信息和实际参数，并在其中启动Connection对象的线程，等待接收服务端响应消息。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public void sendRpcRequest(final Call call) throws InterruptedException, IOException &#123; if (shouldCloseConnection.get()) &#123; return; &#125; // Serialize the call to be sent. This is done from the actual // caller thread, rather than the sendParamsExecutor thread, // so that if the serialization throws an error, it is reported // properly. This also parallelizes the serialization. // // Format of a call on the wire: // 0) Length of rest below (1 + 2) // 1) RpcRequestHeader - is serialized Delimited hence contains length // 2) RpcRequest // // Items '1' and '2' are prepared here. final DataOutputBuffer d = new DataOutputBuffer(); RpcRequestHeaderProto header = ProtoUtil.makeRpcRequestHeader( call.rpcKind, OperationProto.RPC_FINAL_PACKET, call.id, call.retry, clientId); header.writeDelimitedTo(d); call.rpcRequest.write(d); synchronized (sendRpcRequestLock) &#123; Future&lt;?&gt; senderFuture = sendParamsExecutor.submit(new Runnable() &#123; @Override public void run() &#123; try &#123; synchronized (Connection.this.out) &#123; if (shouldCloseConnection.get()) &#123; return; &#125; if (LOG.isDebugEnabled()) LOG.debug(getName() + " sending #" + call.id); byte[] data = d.getData(); int totalLength = d.getLength(); out.writeInt(totalLength); // Total Length out.write(data, 0, totalLength);// RpcRequestHeader + RpcRequest out.flush(); &#125; &#125; catch (IOException e) &#123; // exception at this point would leave the connection in an // unrecoverable state (eg half a call left on the wire). // So, close the connection, killing any outstanding calls markClosed(e); &#125; finally &#123; //the buffer is just an in-memory buffer, but it is still polite to // close early IOUtils.closeStream(d); &#125; &#125; &#125;); try &#123; senderFuture.get(); &#125; catch (ExecutionException e) &#123; Throwable cause = e.getCause(); // cause should only be a RuntimeException as the Runnable above // catches IOException if (cause instanceof RuntimeException) &#123; throw (RuntimeException) cause; &#125; else &#123; throw new RuntimeException("unexpected checked exception", cause); &#125; &#125; &#125;&#125; 将call中的消息写入到输出流中，写入的格式可以从注释中看到，首先是报文的长度，接着是报文头，最后是发送的请求内容。其中先将消息写入到临时的DataOutputBuffer，最后将其放入线程池中发送，避免了阻塞。前文讲到我们在建立连接之后启动了Connection的线程，不断从服务端接受响应消息。接下来我们看如何接受服务端返回的函数结果，即Connection的run方法。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public void run() &#123; if (LOG.isDebugEnabled()) LOG.debug(getName() + ": starting, having connections " + connections.size()); try &#123; while (waitForWork()) &#123;//wait here for work - read or close connection receiveRpcResponse(); &#125; &#125; catch (Throwable t) &#123; ...&#125;/* Receive a response. * Because only one receiver, so no synchronization on in. */private void receiveRpcResponse() &#123; if (shouldCloseConnection.get()) &#123; return; &#125; touch(); try &#123; int totalLen = in.readInt(); RpcResponseHeaderProto header = RpcResponseHeaderProto.parseDelimitedFrom(in); checkResponse(header); int headerLen = header.getSerializedSize(); headerLen += CodedOutputStream.computeRawVarint32Size(headerLen); int callId = header.getCallId(); if (LOG.isDebugEnabled()) LOG.debug(getName() + " got value #" + callId); Call call = calls.get(callId); RpcStatusProto status = header.getStatus(); if (status == RpcStatusProto.SUCCESS) &#123; Writable value = ReflectionUtils.newInstance(valueClass, conf); value.readFields(in); // read value calls.remove(callId); call.setRpcResponse(value); // verify that length was correct // only for ProtobufEngine where len can be verified easily if (call.getRpcResponse() instanceof ProtobufRpcEngine.RpcWrapper) &#123; ProtobufRpcEngine.RpcWrapper resWrapper = (ProtobufRpcEngine.RpcWrapper) call.getRpcResponse(); if (totalLen != headerLen + resWrapper.getLength()) &#123; throw new RpcClientException( "RPC response length mismatch on rpc success"); &#125; &#125; &#125; else &#123; // Rpc Request failed // Verify that length was correct if (totalLen != headerLen) &#123; throw new RpcClientException( "RPC response length mismatch on rpc error"); &#125; final String exceptionClassName = header.hasExceptionClassName() ? header.getExceptionClassName() : "ServerDidNotSetExceptionClassName"; final String errorMsg = header.hasErrorMsg() ? header.getErrorMsg() : "ServerDidNotSetErrorMsg" ; final RpcErrorCodeProto erCode = (header.hasErrorDetail() ? header.getErrorDetail() : null); if (erCode == null) &#123; LOG.warn("Detailed error code not set by server on rpc error"); &#125; RemoteException re = ( (erCode == null) ? new RemoteException(exceptionClassName, errorMsg) : new RemoteException(exceptionClassName, errorMsg, erCode)); if (status == RpcStatusProto.ERROR) &#123; calls.remove(callId); call.setException(re); &#125; else if (status == RpcStatusProto.FATAL) &#123; // Close the connection markClosed(re); &#125; &#125; &#125; catch (IOException e) &#123; markClosed(e); &#125;&#125; 可以看到首先是获得消息的长度，再是解析消息头，根据callId将发消息内容放入对应的Call中，将call.done赋值为true，并唤醒对应的Call。最后回到Client.call方法，返回call.getRpcResponse();到此hadoop 客户端的代码分析完毕。 服务端的实现服务端首先是使用build模式创建一个RPC.Server对象 12345678910111213141516public Server build() throws IOException, HadoopIllegalArgumentException &#123; if (this.conf == null) &#123; throw new HadoopIllegalArgumentException("conf is not set"); &#125; if (this.protocol == null) &#123; throw new HadoopIllegalArgumentException("protocol is not set"); &#125; if (this.instance == null) &#123; throw new HadoopIllegalArgumentException("instance is not set"); &#125; return getProtocolEngine(this.protocol, this.conf).getServer( this.protocol, this.instance, this.bindAddress, this.port, this.numHandlers, this.numReaders, this.queueSizePerHandler, this.verbose, this.conf, this.secretManager, this.portRangeConfig);&#125; 可以看到和客户端一样是通过getProtocolEngine得到不同的ProtocolEngine类来生成不同的Server对象，同样我们来看默认的ProtocolEngine类WritableRpcEngine。调用了WritableRpcEngine的getServer的方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103public RPC.Server getServer(Class&lt;?&gt; protocolClass, Object protocolImpl, String bindAddress, int port, int numHandlers, int numReaders, int queueSizePerHandler, boolean verbose, Configuration conf, SecretManager&lt;? extends TokenIdentifier&gt; secretManager, String portRangeConfig) throws IOException &#123;return new Server(protocolClass, protocolImpl, conf, bindAddress, port, numHandlers, numReaders, queueSizePerHandler, verbose, secretManager, portRangeConfig);&#125;public static class Server extends RPC.Server &#123; ... /** * Construct an RPC server. * @param protocolClass - the protocol being registered * can be null for compatibility with old usage (see below for details) * @param protocolImpl the protocol impl that will be called * @param conf the configuration to use * @param bindAddress the address to bind on to listen for connection * @param port the port to listen for connections on * @param numHandlers the number of method handler threads to run * @param verbose whether each call should be logged */ public Server(Class&lt;?&gt; protocolClass, Object protocolImpl, Configuration conf, String bindAddress, int port, int numHandlers, int numReaders, int queueSizePerHandler, boolean verbose, SecretManager&lt;? extends TokenIdentifier&gt; secretManager, String portRangeConfig) throws IOException &#123; super(bindAddress, port, null, numHandlers, numReaders, queueSizePerHandler, conf, classNameBase(protocolImpl.getClass().getName()), secretManager, portRangeConfig); this.verbose = verbose; Class&lt;?&gt;[] protocols; if (protocolClass == null) &#123; // derive protocol from impl /* * In order to remain compatible with the old usage where a single * target protocolImpl is suppled for all protocol interfaces, and * the protocolImpl is derived from the protocolClass(es) * we register all interfaces extended by the protocolImpl */ protocols = RPC.getProtocolInterfaces(protocolImpl.getClass()); &#125; else &#123; if (!protocolClass.isAssignableFrom(protocolImpl.getClass())) &#123; throw new IOException("protocolClass "+ protocolClass + " is not implemented by protocolImpl which is of class " + protocolImpl.getClass()); &#125; // register protocol class and its super interfaces registerProtocolAndImpl(RPC.RpcKind.RPC_WRITABLE, protocolClass, protocolImpl); protocols = RPC.getProtocolInterfaces(protocolClass); &#125; for (Class&lt;?&gt; p : protocols) &#123; if (!p.equals(VersionedProtocol.class)) &#123; registerProtocolAndImpl(RPC.RpcKind.RPC_WRITABLE, p, protocolImpl); &#125; &#125; &#125;&#125; // Register protocol and its impl for rpc calls void registerProtocolAndImpl(RpcKind rpcKind, Class&lt;?&gt; protocolClass, Object protocolImpl) &#123; String protocolName = RPC.getProtocolName(protocolClass); long version; try &#123; version = RPC.getProtocolVersion(protocolClass); &#125; catch (Exception ex) &#123; LOG.warn("Protocol " + protocolClass + " NOT registered as cannot get protocol version "); return; &#125; getProtocolImplMap(rpcKind).put(new ProtoNameVer(protocolName, version), new ProtoClassProtoImpl(protocolClass, protocolImpl)); LOG.debug("RpcKind = " + rpcKind + " Protocol Name = " + protocolName + " version=" + version + " ProtocolImpl=" + protocolImpl.getClass().getName() + " protocolClass=" + protocolClass.getName()); &#125; Map&lt;ProtoNameVer, ProtoClassProtoImpl&gt; getProtocolImplMap(RPC.RpcKind rpcKind) &#123; if (protocolImplMapArray.size() == 0) &#123;// initialize for all rpc kinds for (int i=0; i &lt;= RpcKind.MAX_INDEX; ++i) &#123; protocolImplMapArray.add( new HashMap&lt;ProtoNameVer, ProtoClassProtoImpl&gt;(10)); &#125; &#125; return protocolImplMapArray.get(rpcKind.ordinal()); &#125; WritableRpcEngine是继承自RPC.Server，而RPC.Server是继承ipc的Server的，getServer直接初始化了WritableRpcEngine.Server对象，可以看到在初始化的过程中，首先是调用了父类的构造函数，接着将协议的具体实现类根据RPC_WRITABLE和协议接口名放入到缓存中。接着我们来看父类的构造函数。 12345678910111213141516171819202122232425public abstract static class Server extends org.apache.hadoop.ipc.Server &#123; ... protected Server(String bindAddress, int port, Class&lt;? extends Writable&gt; paramClass, int handlerCount, int numReaders, int queueSizePerHandler, Configuration conf, String serverName, SecretManager&lt;? extends TokenIdentifier&gt; secretManager, String portRangeConfig) throws IOException &#123; super(bindAddress, port, paramClass, handlerCount, numReaders, queueSizePerHandler, conf, serverName, secretManager, portRangeConfig); initProtocolMetaInfo(conf); &#125; private void initProtocolMetaInfo(Configuration conf) &#123; RPC.setProtocolEngine(conf, ProtocolMetaInfoPB.class, ProtobufRpcEngine.class); ProtocolMetaInfoServerSideTranslatorPB xlator = new ProtocolMetaInfoServerSideTranslatorPB(this); BlockingService protocolInfoBlockingService = ProtocolInfoService .newReflectiveBlockingService(xlator); addProtocol(RpcKind.RPC_PROTOCOL_BUFFER, ProtocolMetaInfoPB.class, protocolInfoBlockingService); &#125; ...&#125; 123456789101112131415161718192021222324252627282930313233public abstract class Server &#123; protected Server(String bindAddress, int port, Class&lt;? extends Writable&gt; rpcRequestClass, int handlerCount, int numReaders, int queueSizePerHandler, Configuration conf, String serverName, SecretManager&lt;? extends TokenIdentifier&gt; secretManager, String portRangeConfig) throws IOException &#123; this.bindAddress = bindAddress; this.conf = conf; this.portRangeConfig = portRangeConfig; this.port = port; this.rpcRequestClass = rpcRequestClass; this.handlerCount = handlerCount; this.socketSendBufferSize = 0; ... listener = new Listener(); this.port = listener.getAddress().getPort(); connectionManager = new ConnectionManager(); this.rpcMetrics = RpcMetrics.create(this, conf); this.rpcDetailedMetrics = RpcDetailedMetrics.create(this.port); this.tcpNoDelay = conf.getBoolean( CommonConfigurationKeysPublic.IPC_SERVER_TCPNODELAY_KEY, CommonConfigurationKeysPublic.IPC_SERVER_TCPNODELAY_DEFAULT); this.setLogSlowRPC(conf.getBoolean( CommonConfigurationKeysPublic.IPC_SERVER_LOG_SLOW_RPC, CommonConfigurationKeysPublic.IPC_SERVER_LOG_SLOW_RPC_DEFAULT)); // Create the responder here responder = new Responder(); ... &#125;&#125; 可以看到父类的初始化主要是对一些变量的赋值，最主要的是初始化了listener和responder对象。 123456789101112131415161718192021222324252627282930313233343536373839/** Listens on the socket. Creates jobs for the handler threads*/ private class Listener extends Thread &#123; private ServerSocketChannel acceptChannel = null; //the accept channel private Selector selector = null; //the selector that we use for the server private Reader[] readers = null; private int currentReader = 0; private InetSocketAddress address; //the address we bind at private int backlogLength = conf.getInt( CommonConfigurationKeysPublic.IPC_SERVER_LISTEN_QUEUE_SIZE_KEY, CommonConfigurationKeysPublic.IPC_SERVER_LISTEN_QUEUE_SIZE_DEFAULT); public Listener() throws IOException &#123; address = new InetSocketAddress(bindAddress, port); // Create a new server socket and set to non blocking mode acceptChannel = ServerSocketChannel.open(); acceptChannel.configureBlocking(false); // Bind the server socket to the local host and port bind(acceptChannel.socket(), address, backlogLength, conf, portRangeConfig); port = acceptChannel.socket().getLocalPort(); //Could be an ephemeral port // create a selector; selector= Selector.open(); //如果没有指定值，readThreads默认为1 readers = new Reader[readThreads]; for (int i = 0; i &lt; readThreads; i++) &#123; Reader reader = new Reader( "Socket Reader #" + (i + 1) + " for port " + port); readers[i] = reader; reader.start(); &#125; // Register accepts on the server socket with the selector. acceptChannel.register(selector, SelectionKey.OP_ACCEPT); this.setName("IPC Server listener on " + port); this.setDaemon(true); &#125; ... &#125; Listener初始化了ServerSocketChannel，绑定了指定的一个或者一串中的一个端口，初始化了Selector,向其中注册了”连接就绪“的事件。并定义启动了几个读线程（Reader类）。Reader类主要用于读取客户端发送的信息，范序列化，生成相应的Call对象。每个Reader类中都有一个Selector，用于监听“读就绪”事件。在Listener的选择器中收到连接就绪的事件就会socketChannel封装进Connection中，将读事件注册给Reader的选择器，有相应的reader来负责读取信息。（下文会详细看到）现在先大致看一下responder类。 12345678910111213141516171819202122232425262728293031 // Sends responses of RPC back to clients.private class Responder extends Thread &#123; private final Selector writeSelector; private int pending; // connections waiting to register final static int PURGE_INTERVAL = 900000; // 15mins Responder() throws IOException &#123; this.setName("IPC Server Responder"); this.setDaemon(true); writeSelector = Selector.open(); // create a selector pending = 0; &#125; @Override public void run() &#123; LOG.info(Thread.currentThread().getName() + ": starting"); SERVER.set(Server.this); try &#123; doRunLoop(); &#125; finally &#123; LOG.info("Stopping " + Thread.currentThread().getName()); try &#123; writeSelector.close(); &#125; catch (IOException ioe) &#123; LOG.error("Couldn't close write selector in " + Thread.currentThread().getName(), ioe); &#125; &#125; &#125; ... &#125; 可以看到responder也是一个线程类，这个线程类主要是将RPC的相应信息传给客户端，可以看到其内部也有一个selector对象，主要是一些写事件会注册在其中。在初始化Listener和Responder对象，生成Server实例之后，就需要调用Server.start()来启动服务端。 12345678910public synchronized void start() &#123; responder.start(); listener.start(); handlers = new Handler[handlerCount]; for (int i = 0; i &lt; handlerCount; i++) &#123; handlers[i] = new Handler(i); handlers[i].start(); &#125;&#125; 分别启动了listener、responder和handlers的线程。接下来看listener的run方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public void run() &#123; LOG.info(Thread.currentThread().getName() + ": starting"); SERVER.set(Server.this); connectionManager.startIdleScan(); while (running) &#123; SelectionKey key = null; try &#123; getSelector().select(); //阻塞直到注册的socket管道有事件触发为止 Iterator&lt;SelectionKey&gt; iter = getSelector().selectedKeys().iterator(); while (iter.hasNext()) &#123; key = iter.next(); iter.remove(); try &#123; if (key.isValid()) &#123; if (key.isAcceptable()) doAccept(key); &#125; &#125; catch (IOException e) &#123; &#125; key = null; &#125; &#125; catch (OutOfMemoryError e) &#123; // we can run out of memory if we have too many threads // log the event and sleep for a minute and give // some thread(s) a chance to finish LOG.warn("Out of Memory in server select", e); closeCurrentConnection(key, e); connectionManager.closeIdle(true); try &#123; Thread.sleep(60000); &#125; catch (Exception ie) &#123;&#125; &#125; catch (Exception e) &#123; closeCurrentConnection(key, e); &#125; &#125; LOG.info("Stopping " + Thread.currentThread().getName()); synchronized (this) &#123; try &#123; acceptChannel.close(); selector.close(); &#125; catch (IOException e) &#123; &#125; selector= null; acceptChannel= null; // close all connections connectionManager.stopIdleScan(); connectionManager.closeAll(); &#125;&#125;void doAccept(SelectionKey key) throws InterruptedException, IOException, OutOfMemoryError &#123; ServerSocketChannel server = (ServerSocketChannel) key.channel(); SocketChannel channel; while ((channel = server.accept()) != null) &#123; channel.configureBlocking(false); channel.socket().setTcpNoDelay(tcpNoDelay); channel.socket().setKeepAlive(true); Reader reader = getReader(); //数组递增，获取下一个reader Connection c = connectionManager.register(channel); //构造Connection对象并放入connectionManager管理，如果connectionManager的最大连接数将无法建立连接 // If the connectionManager can't take it, close the connection. if (c == null) &#123; if (channel.isOpen()) &#123; IOUtils.cleanup(null, channel); &#125; connectionManager.droppedConnections.getAndIncrement(); continue; &#125; //将其添加到SelectionKey中，当发生错误时可以从SelectionKey中获得Connection将其关闭 key.attach(c); // so closeCurrentConnection can get the object reader.addConnection(c); &#125;&#125; listener线程等待注册的连接事件触发，调用doAccept函数，doAccept中获得连接的socket,构造一个Connection对象放入connectionManager的缓存中（内部有一个Set connections变量），这个Connection类是ipc.Server的内部类，和上述Client中的内部类不同。接着在递增的取出一个reader将connection放入reader内部的阻塞队列中。 123456public void addConnection(Connection conn) throws InterruptedException &#123; //pendingConnections为阻塞队列 BlockingQueue&lt;Connection&gt; pendingConnections.put(conn); //唤醒readSelector readSelector.wakeup();&#125; 接着我们来看Reader的run方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public void run() &#123; LOG.info("Starting " + Thread.currentThread().getName()); try &#123; doRunLoop(); &#125; finally &#123; try &#123; readSelector.close(); &#125; catch (IOException ioe) &#123; LOG.error("Error closing read selector in " + Thread.currentThread().getName(), ioe); &#125; &#125;&#125;private synchronized void doRunLoop() &#123; while (running) &#123; SelectionKey key = null; try &#123; // consume as many connections as currently queued to avoid // unbridled acceptance of connections that starves the select int size = pendingConnections.size(); for (int i=size; i&gt;0; i--) &#123; Connection conn = pendingConnections.take(); conn.channel.register(readSelector, SelectionKey.OP_READ, conn); &#125; readSelector.select(); //不用经过判断，如果没有channel触发事件，阻塞直到有一个注册的事件就绪为止或者调用weakup() Iterator&lt;SelectionKey&gt; iter = readSelector.selectedKeys().iterator(); while (iter.hasNext()) &#123; key = iter.next(); iter.remove(); try &#123; if (key.isReadable()) &#123; doRead(key); &#125; &#125; catch (CancelledKeyException cke) &#123; // something else closed the connection, ex. responder or // the listener doing an idle scan. ignore it and let them // clean up. LOG.info(Thread.currentThread().getName() + ": connection aborted from " + key.attachment()); &#125; key = null; &#125; &#125; catch (InterruptedException e) &#123; if (running) &#123; // unexpected -- log it LOG.info(Thread.currentThread().getName() + " unexpectedly interrupted", e); &#125; &#125; catch (IOException ex) &#123; LOG.error("Error in Reader", ex); &#125; catch (Throwable re) &#123; LOG.fatal("Bug in read selector!", re); ExitUtil.terminate(1, "Bug in read selector!"); &#125; &#125;&#125; 从阻塞队列中取出connection，在readSelector注册读事件，选择器响应，调用doRead方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227void doRead(SelectionKey key) throws InterruptedException &#123; int count = 0; Connection c = (Connection)key.attachment(); if (c == null) &#123; return; &#125; c.setLastContact(Time.now()); try &#123; count = c.readAndProcess(); &#125; catch (InterruptedException ieo) &#123; ... &#125;&#125; public int readAndProcess() throws WrappedRpcServerException, IOException, InterruptedException &#123; while (true) &#123; /* Read at most one RPC. If the header is not read completely yet * then iterate until we read first RPC or until there is no data left. */ int count = -1; //dataLengthBuffer为四个字节，可能是接受报头，也可能是数据的长度 if (dataLengthBuffer.remaining() &gt; 0) &#123; count = channelRead(channel, dataLengthBuffer); if (count &lt; 0 || dataLengthBuffer.remaining() &gt; 0) return count; &#125; /** * 如果connectionHeaderRead是true, * 表示已经读过连接头，那么不需要读取头部数据，dataLengthBuffer存储的数据的长度。 * 如果connectionHeaderRead是false, * 表示dataLengthBuffer读取的是hrpc * +----------------------------------+ * | "hrpc" 4 bytes | * +----------------------------------+ * | Version (1 byte) | * +----------------------------------+ * | Service Class (1 byte) | * +----------------------------------+ * | AuthProtocol (1 byte) | * +----------------------------------+ */ if (!connectionHeaderRead) &#123; //Every connection is expected to send the header. //connectionHeaderBuf主要存放后三个字节 if (connectionHeaderBuf == null) &#123; connectionHeaderBuf = ByteBuffer.allocate(3); &#125; count = channelRead(channel, connectionHeaderBuf); if (count &lt; 0 || connectionHeaderBuf.remaining() &gt; 0) &#123; return count; &#125; int version = connectionHeaderBuf.get(0); // TODO we should add handler for service class later this.setServiceClass(connectionHeaderBuf.get(1)); dataLengthBuffer.flip(); // Check if it looks like the user is hitting an IPC port // with an HTTP GET - this is a common error, so we can // send back a simple string indicating as much. if (HTTP_GET_BYTES.equals(dataLengthBuffer)) &#123; setupHttpRequestOnIpcPortResponse(); return -1; &#125; //RpcConstants.HEADER为“hrpc” if (!RpcConstants.HEADER.equals(dataLengthBuffer) || version != CURRENT_VERSION) &#123; //Warning is ok since this is not supposed to happen. LOG.warn("Incorrect header or version mismatch from " + hostAddress + ":" + remotePort + " got version " + version + " expected version " + CURRENT_VERSION); setupBadVersionResponse(version); return -1; &#125; // this may switch us into SIMPLE authProtocol = initializeAuthContext(connectionHeaderBuf.get(2)); dataLengthBuffer.clear(); connectionHeaderBuf = null; connectionHeaderRead = true; continue; &#125; //分配空间为dataLength的长度 if (data == null) &#123; dataLengthBuffer.flip(); dataLength = dataLengthBuffer.getInt(); checkDataLength(dataLength); data = ByteBuffer.allocate(dataLength); &#125; //读取数据 count = channelRead(channel, data); //第一次是读取PRC的context，connectionContextRead为false,读取完context后为true if (data.remaining() == 0) &#123; dataLengthBuffer.clear(); data.flip(); boolean isHeaderRead = connectionContextRead; //如果是context,则读取context,否则读取其数据 processOneRpc(data.array()); data = null; if (!isHeaderRead) &#123; continue; &#125; &#125; return count; &#125;&#125;/** * Process an RPC Request - handle connection setup and decoding of * request into a Call * @param buf - contains the RPC request header and the rpc request * @throws IOException - internal error that should not be returned to * client, typically failure to respond to client * @throws WrappedRpcServerException - an exception to be sent back to * the client that does not require verbose logging by the * Listener thread * @throws InterruptedException */ private void processOneRpc(byte[] buf) throws IOException, WrappedRpcServerException, InterruptedException &#123; int callId = -1; int retry = RpcConstants.INVALID_RETRY_COUNT; try &#123; final DataInputStream dis = new DataInputStream(new ByteArrayInputStream(buf)); final RpcRequestHeaderProto header = decodeProtobufFromStream(RpcRequestHeaderProto.newBuilder(), dis); callId = header.getCallId(); retry = header.getRetryCount(); if (LOG.isDebugEnabled()) &#123; LOG.debug(" got #" + callId); &#125; checkRpcHeaders(header); //这里通过callId来判数据是context还是实际数据，如果是context,callId为CONNECTION_CONTEXT_CALL_ID = -3，否则callId &gt; 0 if (callId &lt; 0) &#123; // callIds typically used during connection setup //connectionContextRead在该方法中赋值为true processRpcOutOfBandRequest(header, dis); &#125; else if (!connectionContextRead) &#123; throw new WrappedRpcServerException( RpcErrorCodeProto.FATAL_INVALID_RPC_HEADER, "Connection context not established"); &#125; else &#123; //读取数据 processRpcRequest(header, dis); &#125; &#125; catch (WrappedRpcServerException wrse) &#123; // inform client of error Throwable ioe = wrse.getCause(); final Call call = new Call(callId, retry, null, this); setupResponse(authFailedResponse, call, RpcStatusProto.FATAL, wrse.getRpcErrorCodeProto(), null, ioe.getClass().getName(), ioe.getMessage()); call.sendResponse(); throw wrse; &#125;&#125;/** * Process an RPC Request - the connection headers and context must * have been already read * @param header - RPC request header * @param dis - stream to request payload * @throws WrappedRpcServerException - due to fatal rpc layer issues such * as invalid header or deserialization error. In this case a RPC fatal * status response will later be sent back to client. * @throws InterruptedException */private void processRpcRequest(RpcRequestHeaderProto header, DataInputStream dis) throws WrappedRpcServerException, InterruptedException &#123; Class&lt;? extends Writable&gt; rpcRequestClass = getRpcRequestWrapper(header.getRpcKind()); if (rpcRequestClass == null) &#123; LOG.warn("Unknown rpc kind " + header.getRpcKind() + " from client " + getHostAddress()); final String err = "Unknown rpc kind in rpc header" + header.getRpcKind(); throw new WrappedRpcServerException( RpcErrorCodeProto.FATAL_INVALID_RPC_HEADER, err); &#125; Writable rpcRequest; try &#123; //Read the rpc request //利用反射实例化对象，并读取信息，得到调用方法和参数的数据。如果是WritableRpcEngine，对应的是Invocation rpcRequest = ReflectionUtils.newInstance(rpcRequestClass, conf); rpcRequest.readFields(dis); &#125; catch (Throwable t) &#123; // includes runtime exception from newInstance LOG.warn("Unable to read call parameters for client " + getHostAddress() + "on connection protocol " + this.protocolName + " for rpcKind " + header.getRpcKind(), t); String err = "IPC server unable to read call parameters: "+ t.getMessage(); throw new WrappedRpcServerException( RpcErrorCodeProto.FATAL_DESERIALIZING_REQUEST, err); &#125; Span traceSpan = null; if (header.hasTraceInfo()) &#123; // If the incoming RPC included tracing info, always continue the trace TraceInfo parentSpan = new TraceInfo(header.getTraceInfo().getTraceId(), header.getTraceInfo().getParentId()); traceSpan = Trace.startSpan(rpcRequest.toString(), parentSpan).detach(); &#125; //将信息封装成Call对象，包括callId,调用对象信息的等，和Client的Call相对应 Call call = new Call(header.getCallId(), header.getRetryCount(), rpcRequest, this, ProtoUtil.convert(header.getRpcKind()), header.getClientId().toByteArray(), traceSpan); if (callQueue.isClientBackoffEnabled()) &#123; // if RPC queue is full, we will ask the RPC client to back off by // throwing RetriableException. Whether RPC client will honor // RetriableException and retry depends on client ipc retry policy. // For example, FailoverOnNetworkExceptionRetry handles // RetriableException. queueRequestOrAskClientToBackOff(call); &#125; else &#123; callQueue.put(call); // queue the call; maybe blocked here &#125; incRpcCount(); // Increment the rpc count&#125; 内部调用了Connection.readAndProcess方法，dataLengthBuffer可能读取到连接头，如果是连接头则dataLengthBuffer里的内容是“hrpc”,否则dataLengthBuffer读取的是数据的长度，这里有分context和实际的数据，这个都由processOneRpc处理，这里通过callId是否小于0来判断是否是context（context的callId为-3），如果是context，则会将connectionContextRead设置true，最后通过processRpcRequest方法读取实际数据。通过反射机制创建具体的Writable子类，通过ReadFile的发序列化得到内容，最后将CallId，Wriable等封装进Call中（和Client的Call相对应），放入callQueue这个队列值，而callQueue中的Call的处理是交给Handler处理。我们来看Handler的run方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384private class Handler extends Thread &#123; public Handler(int instanceNumber) &#123; this.setDaemon(true); this.setName("IPC Server handler "+ instanceNumber + " on " + port); &#125; @Override public void run() &#123; LOG.debug(Thread.currentThread().getName() + ": starting"); SERVER.set(Server.this); ByteArrayOutputStream buf = new ByteArrayOutputStream(INITIAL_RESP_BUF_SIZE); while (running) &#123; TraceScope traceScope = null; try &#123; //从阻塞队列中取出一个Call对象 final Call call = callQueue.take(); // pop the queue; maybe blocked here if (LOG.isDebugEnabled()) &#123; LOG.debug(Thread.currentThread().getName() + ": " + call + " for RpcKind " + call.rpcKind); &#125; if (!call.connection.channel.isOpen()) &#123; LOG.info(Thread.currentThread().getName() + ": skipped " + call); continue; &#125; String errorClass = null; String error = null; RpcStatusProto returnStatus = RpcStatusProto.SUCCESS; RpcErrorCodeProto detailedErr = null; Writable value = null; //放入ThreadLocal，以便获取当前处理的call等信息 CurCall.set(call); if (call.traceSpan != null) &#123; traceScope = Trace.continueSpan(call.traceSpan); &#125; try &#123; // Make the call as the user via Subject.doAs, thus associating // the call with the Subject //调用call调用对应的方法 if (call.connection.user == null) &#123; value = call(call.rpcKind, call.connection.protocolName, call.rpcRequest, call.timestamp); &#125; else &#123; value = call.connection.user.doAs (new PrivilegedExceptionAction&lt;Writable&gt;() &#123; @Override public Writable run() throws Exception &#123; // make the call return call(call.rpcKind, call.connection.protocolName, call.rpcRequest, call.timestamp); &#125; &#125; ); &#125; &#125; catch (Throwable e) &#123; ... &#125; CurCall.set(null); synchronized (call.connection.responseQueue) &#123; setupResponse(buf, call, returnStatus, detailedErr, value, errorClass, error); // Discard the large buf and reset it back to smaller size // to free up heap. if (buf.size() &gt; maxRespSize) &#123; LOG.warn("Large response size " + buf.size() + " for call " + call.toString()); buf = new ByteArrayOutputStream(INITIAL_RESP_BUF_SIZE); &#125; call.sendResponse(); &#125; &#125; catch (InterruptedException e) &#123; ... &#125; finally &#123; ... &#125; &#125; LOG.debug(Thread.currentThread().getName() + ": exiting"); &#125;&#125; 可以看到handler的run方法，从callQueue阻塞队列中取出Call对象，接着调用call方法来执行指定的方法，我们接着来看call方法。 123456789101112131415161718/** Called for each call. * call的接口方法 */public abstract Writable call(RPC.RpcKind rpcKind, String protocol, Writable param, long receiveTime) throws Exception;//具体实现@Overridepublic Writable call(RPC.RpcKind rpcKind, String protocol, Writable rpcRequest, long receiveTime) throws Exception &#123; return getRpcInvoker(rpcKind).call(this, protocol, rpcRequest, receiveTime);&#125;public static RpcInvoker getRpcInvoker(RPC.RpcKind rpcKind) &#123; RpcKindMapValue val = rpcKindMap.get(rpcKind); return (val == null) ? null : val.rpcInvoker; &#125; 可以看到实际到最好是调用看RpcInvoker的具体实现类的call方法，而该类是从rpcKindMap中通过rpcKind取出的，那么这个RpcInvoker的具体实现类是什么时候放进去的，具体是什么。追踪rpcKindMap使用的地方可以看到。 12345678910111213141516171819202122232425public class WritableRpcEngine implements RpcEngine &#123; /** * Register the rpcRequest deserializer for WritableRpcEngine */ private static synchronized void initialize() &#123; org.apache.hadoop.ipc.Server.registerProtocolEngine(RPC.RpcKind.RPC_WRITABLE, Invocation.class, new Server.WritableRpcInvoker()); isInitialized = true; &#125;&#125;public static void registerProtocolEngine(RPC.RpcKind rpcKind, Class&lt;? extends Writable&gt; rpcRequestWrapperClass, RpcInvoker rpcInvoker) &#123; RpcKindMapValue old = rpcKindMap.put(rpcKind, new RpcKindMapValue(rpcRequestWrapperClass, rpcInvoker)); if (old != null) &#123; rpcKindMap.put(rpcKind, old); throw new IllegalArgumentException("ReRegistration of rpcKind: " + rpcKind); &#125; LOG.debug("rpcKind=" + rpcKind + ", rpcRequestWrapperClass=" + rpcRequestWrapperClass + ", rpcInvoker=" + rpcInvoker);&#125; 可以看出在当初WritableRpcEngine类加载时就已经将对应的初始化的Server.WritableRpcInvoker类放入rpcKindMap中，所以如果是采用WritableRpcEngine类的，那现在从rpcKindMap取出的就是Server.WritableRpcInvoker类。具体看该类下的call方法如果通过反射技术调用过程得到具体的值。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120static class WritableRpcInvoker implements RpcInvoker &#123; @Override public Writable call(org.apache.hadoop.ipc.RPC.Server server, String protocolName, Writable rpcRequest, long receivedTime) throws IOException, RPC.VersionMismatch &#123; //得到当初客户端序列化的Invocation对象 Invocation call = (Invocation)rpcRequest; if (server.verbose) log("Call: " + call); // Verify writable rpc version if (call.getRpcVersion() != writableRpcVersion) &#123; // Client is using a different version of WritableRpc throw new RpcServerException( "WritableRpc version mismatch, client side version=" + call.getRpcVersion() + ", server side version=" + writableRpcVersion); &#125; long clientVersion = call.getProtocolVersion(); final String protoName; ProtoClassProtoImpl protocolImpl; if (call.declaringClassProtocolName.equals(VersionedProtocol.class.getName())) &#123; // VersionProtocol methods are often used by client to figure out // which version of protocol to use. // // Versioned protocol methods should go the protocolName protocol // rather than the declaring class of the method since the // the declaring class is VersionedProtocol which is not // registered directly. // Send the call to the highest protocol version VerProtocolImpl highest = server.getHighestSupportedProtocol( RPC.RpcKind.RPC_WRITABLE, protocolName); if (highest == null) &#123; throw new RpcServerException("Unknown protocol: " + protocolName); &#125; protocolImpl = highest.protocolTarget; &#125; else &#123; protoName = call.declaringClassProtocolName; // Find the right impl for the protocol based on client version. //根据协议名和客户端的版本号，RpcKind来找到具体实现类 ProtoNameVer pv = new ProtoNameVer(call.declaringClassProtocolName, clientVersion); protocolImpl = server.getProtocolImplMap(RPC.RpcKind.RPC_WRITABLE).get(pv); if (protocolImpl == null) &#123; // no match for Protocol AND Version VerProtocolImpl highest = server.getHighestSupportedProtocol(RPC.RpcKind.RPC_WRITABLE, protoName); if (highest == null) &#123; throw new RpcServerException("Unknown protocol: " + protoName); &#125; else &#123; // protocol supported but not the version that client wants throw new RPC.VersionMismatch(protoName, clientVersion, highest.version); &#125; &#125; &#125; // Invoke the protocol method long startTime = Time.now(); int qTime = (int) (startTime-receivedTime); Exception exception = null; try &#123; //根据方法名来找到方法，利用反射来调用该方法得到结果 Method method = protocolImpl.protocolClass.getMethod(call.getMethodName(), call.getParameterClasses()); method.setAccessible(true); server.rpcDetailedMetrics.init(protocolImpl.protocolClass); Object value = method.invoke(protocolImpl.protocolImpl, call.getParameters()); if (server.verbose) log("Return: "+value); //封装进ObjectWritable类中返回 return new ObjectWritable(method.getReturnType(), value); &#125; catch (InvocationTargetException e) &#123; Throwable target = e.getTargetException(); if (target instanceof IOException) &#123; exception = (IOException)target; throw (IOException)target; &#125; else &#123; IOException ioe = new IOException(target.toString()); ioe.setStackTrace(target.getStackTrace()); exception = ioe; throw ioe; &#125; &#125; catch (Throwable e) &#123; if (!(e instanceof IOException)) &#123; LOG.error("Unexpected throwable object ", e); &#125; IOException ioe = new IOException(e.toString()); ioe.setStackTrace(e.getStackTrace()); exception = ioe; throw ioe; &#125; finally &#123; int processingTime = (int) (Time.now() - startTime); if (LOG.isDebugEnabled()) &#123; String msg = "Served: " + call.getMethodName() + " queueTime= " + qTime + " procesingTime= " + processingTime; if (exception != null) &#123; msg += " exception= " + exception.getClass().getSimpleName(); &#125; LOG.debug(msg); &#125; String detailedMetricsName = (exception == null) ? call.getMethodName() : exception.getClass().getSimpleName(); server.rpcMetrics.addRpcQueueTime(qTime); server.rpcMetrics.addRpcProcessingTime(processingTime); server.rpcDetailedMetrics.addProcessingTime(detailedMetricsName, processingTime); if (server.isLogSlowRPC()) &#123; server.logSlowRpcCalls(call.getMethodName(), processingTime); &#125; &#125; &#125;&#125; WritableRpcInvoker的call方法中将rpcRequest转型为Invocation对象，取出其中的协议名和版本号，根据其从server.getProtocolImplMap中得到协议具体实现类对象，从而通过方法名利用反射调用相应的方法，返回结果封装进ObjectWritable中。至于是如何得到协议具体实现类对象的，可以从以下方法中看出。 12345678910 Map&lt;ProtoNameVer, ProtoClassProtoImpl&gt; getProtocolImplMap(RPC.RpcKind rpcKind) &#123; if (protocolImplMapArray.size() == 0) &#123;// initialize for all rpc kinds for (int i=0; i &lt;= RpcKind.MAX_INDEX; ++i) &#123; protocolImplMapArray.add( new HashMap&lt;ProtoNameVer, ProtoClassProtoImpl&gt;(10)); &#125; &#125; return protocolImplMapArray.get(rpcKind.ordinal()); &#125; 根据rpcKind从protocolImplMapArray取出Map&lt;ProtoNameVer, ProtoClassProtoImpl&gt;对象，再根据ProtoNameVer取出具体实例对象。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public Server(Class&lt;?&gt; protocolClass, Object protocolImpl, Configuration conf, String bindAddress, int port, int numHandlers, int numReaders, int queueSizePerHandler, boolean verbose, SecretManager&lt;? extends TokenIdentifier&gt; secretManager, String portRangeConfig) throws IOException &#123; super(bindAddress, port, null, numHandlers, numReaders, queueSizePerHandler, conf, classNameBase(protocolImpl.getClass().getName()), secretManager, portRangeConfig); this.verbose = verbose; Class&lt;?&gt;[] protocols; if (protocolClass == null) &#123; // derive protocol from impl /* * In order to remain compatible with the old usage where a single * target protocolImpl is suppled for all protocol interfaces, and * the protocolImpl is derived from the protocolClass(es) * we register all interfaces extended by the protocolImpl */ protocols = RPC.getProtocolInterfaces(protocolImpl.getClass()); &#125; else &#123; if (!protocolClass.isAssignableFrom(protocolImpl.getClass())) &#123; throw new IOException("protocolClass "+ protocolClass + " is not implemented by protocolImpl which is of class " + protocolImpl.getClass()); &#125; // register protocol class and its super interfaces //将protocolClass和protocolImpl放入map中 registerProtocolAndImpl(RPC.RpcKind.RPC_WRITABLE, protocolClass, protocolImpl); //得到protocolClass的所有接口 protocols = RPC.getProtocolInterfaces(protocolClass); &#125; //将protocolClass的所有接口和protocolImpl放入map中 for (Class&lt;?&gt; p : protocols) &#123; if (!p.equals(VersionedProtocol.class)) &#123; // registerProtocolAndImpl(RPC.RpcKind.RPC_WRITABLE, p, protocolImpl); &#125; &#125;&#125;void registerProtocolAndImpl(RpcKind rpcKind, Class&lt;?&gt; protocolClass, Object protocolImpl) &#123; String protocolName = RPC.getProtocolName(protocolClass); long version; try &#123; version = RPC.getProtocolVersion(protocolClass); &#125; catch (Exception ex) &#123; LOG.warn("Protocol " + protocolClass + " NOT registered as cannot get protocol version "); return; &#125; //放入map中 getProtocolImplMap(rpcKind).put(new ProtoNameVer(protocolName, version), new ProtoClassProtoImpl(protocolClass, protocolImpl)); LOG.debug("RpcKind = " + rpcKind + " Protocol Name = " + protocolName + " version=" + version + " ProtocolImpl=" + protocolImpl.getClass().getName() + " protocolClass=" + protocolClass.getName()); &#125; 可以从上面看出，在初始化Server对象时，就已经将协议具体类和其所有接口类放入一个Map中，而这个map根据rpcKind放在了列表的rpcKind位置处。 现在回到Handler的run方法中，在调用call方法获得结果值后，调用setupResponse和call.sendResponse()方法，我们来具体看看这两个方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/*** Setup response for the IPC Call.* * @param responseBuf buffer to serialize the response into* @param call &#123;@link Call&#125; to which we are setting up the response* @param status of the IPC call* @param rv return value for the IPC Call, if the call was successful* @param errorClass error class, if the the call failed* @param error error message, if the call failed* @throws IOException*/private static void setupResponse(ByteArrayOutputStream responseBuf, Call call, RpcStatusProto status, RpcErrorCodeProto erCode, Writable rv, String errorClass, String error) throws IOException &#123; responseBuf.reset(); DataOutputStream out = new DataOutputStream(responseBuf); //构造响应数据头 RpcResponseHeaderProto.Builder headerBuilder = RpcResponseHeaderProto.newBuilder(); headerBuilder.setClientId(ByteString.copyFrom(call.clientId)); headerBuilder.setCallId(call.callId); headerBuilder.setRetryCount(call.retryCount); headerBuilder.setStatus(status); headerBuilder.setServerIpcVersionNum(CURRENT_VERSION); if (status == RpcStatusProto.SUCCESS) &#123; RpcResponseHeaderProto header = headerBuilder.build(); final int headerLen = header.getSerializedSize(); int fullLength = CodedOutputStream.computeRawVarint32Size(headerLen) + headerLen; try &#123; if (rv instanceof ProtobufRpcEngine.RpcWrapper) &#123; ProtobufRpcEngine.RpcWrapper resWrapper = (ProtobufRpcEngine.RpcWrapper) rv; fullLength += resWrapper.getLength(); out.writeInt(fullLength); header.writeDelimitedTo(out); rv.write(out); &#125; else &#123; // Have to serialize to buffer to get len //写入数据长度，数据头，数据内容（序列化响应返回的内容） final DataOutputBuffer buf = new DataOutputBuffer(); rv.write(buf); byte[] data = buf.getData(); fullLength += buf.getLength(); out.writeInt(fullLength); header.writeDelimitedTo(out); out.write(data, 0, buf.getLength()); &#125; &#125; catch (Throwable t) &#123; LOG.warn("Error serializing call response for call " + call, t); // Call back to same function - this is OK since the // buffer is reset at the top, and since status is changed // to ERROR it won't infinite loop. setupResponse(responseBuf, call, RpcStatusProto.ERROR, RpcErrorCodeProto.ERROR_SERIALIZING_RESPONSE, null, t.getClass().getName(), StringUtils.stringifyException(t)); return; &#125; &#125; else &#123; // Rpc Failure headerBuilder.setExceptionClassName(errorClass); headerBuilder.setErrorMsg(error); headerBuilder.setErrorDetail(erCode); RpcResponseHeaderProto header = headerBuilder.build(); int headerLen = header.getSerializedSize(); final int fullLength = CodedOutputStream.computeRawVarint32Size(headerLen) + headerLen; out.writeInt(fullLength); header.writeDelimitedTo(out); &#125; //将其设置在Call对象中 call.setResponse(ByteBuffer.wrap(responseBuf.toByteArray()));&#125; setupResponse中主要是序列化响应的对象，并将其放入Call对象的rpcResponse中。接着就需要将Call传递给Responder来处理了。可以从Call.sendResponse()看到。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131//Call.sendResponse()@InterfaceStability.Unstable@InterfaceAudience.LimitedPrivate(&#123;"HDFS"&#125;)public void sendResponse() throws IOException &#123; int count = responseWaitCount.decrementAndGet(); assert count &gt;= 0 : "response has already been sent"; if (count == 0) &#123; connection.sendResponse(this); &#125;&#125;//Connection.sendResponse(Call call)private void sendResponse(Call call) throws IOException &#123; responder.doRespond(call);&#125;//// Enqueue a response from the application.//void doRespond(Call call) throws IOException &#123; synchronized (call.connection.responseQueue) &#123; // must only wrap before adding to the responseQueue to prevent // postponed responses from being encrypted and sent out of order. if (call.connection.useWrap) &#123; ByteArrayOutputStream response = new ByteArrayOutputStream(); wrapWithSasl(response, call); call.setResponse(ByteBuffer.wrap(response.toByteArray())); &#125; //放入responseQueue中 call.connection.responseQueue.addLast(call); if (call.connection.responseQueue.size() == 1) &#123; processResponse(call.connection.responseQueue, true); &#125; &#125;&#125;// Processes one response. Returns true if there are no more pending// data for this channel.//private boolean processResponse(LinkedList&lt;Call&gt; responseQueue, boolean inHandler) throws IOException &#123; boolean error = true; boolean done = false; // there is more data for this channel. int numElements = 0; Call call = null; try &#123; synchronized (responseQueue) &#123; // // If there are no items for this channel, then we are done // numElements = responseQueue.size(); if (numElements == 0) &#123; error = false; return true; // no more data for this channel. &#125; // // Extract the first call // 从列表中取出第一个元素 // call = responseQueue.removeFirst(); SocketChannel channel = call.connection.channel; if (LOG.isDebugEnabled()) &#123; LOG.debug(Thread.currentThread().getName() + ": responding to " + call); &#125; // // Send as much data as we can in the non-blocking fashion // 尝试一次性发送结果 // int numBytes = channelWrite(channel, call.rpcResponse); if (numBytes &lt; 0) &#123; return true; &#125; //如果没有余留，则表示发送成功 if (!call.rpcResponse.hasRemaining()) &#123; //Clear out the response buffer so it can be collected call.rpcResponse = null; call.connection.decRpcCount(); if (numElements == 1) &#123; // last call fully processes. done = true; // no more data for this channel. &#125; else &#123; done = false; // more calls pending to be sent. &#125; if (LOG.isDebugEnabled()) &#123; LOG.debug(Thread.currentThread().getName() + ": responding to " + call + " Wrote " + numBytes + " bytes."); &#125; &#125; else &#123; // // If we were unable to write the entire response out, then // insert in Selector queue. // 否则需要放入列表中，向responser的writeSelector注册写事件 // call.connection.responseQueue.addFirst(call); if (inHandler) &#123; // set the serve time when the response has to be sent later call.timestamp = Time.now(); incPending(); try &#123; // Wakeup the thread blocked on select, only then can the call // to channel.register() complete. writeSelector.wakeup(); //向选择器注册 channel.register(writeSelector, SelectionKey.OP_WRITE, call); &#125; catch (ClosedChannelException e) &#123; //Its ok. channel might be closed else where. done = true; &#125; finally &#123; decPending(); &#125; &#125; if (LOG.isDebugEnabled()) &#123; LOG.debug(Thread.currentThread().getName() + ": responding to " + call + " Wrote partial " + numBytes + " bytes."); &#125; &#125; error = false; // everything went off well &#125; &#125; finally &#123; if (error &amp;&amp; call != null) &#123; LOG.warn(Thread.currentThread().getName()+", call " + call + ": output error"); done = true; // error. no more data for this channel. closeConnection(call.connection); &#125; &#125; return done;&#125; 可以看到，最后放到了responseQueue中，responseQueue是一个列表，不是一个阻塞对列，每个Call封装了从客户端反序列化的对象信息，发送回客户端的数据还有Connection(每一个客户端和服务端维持一个Connection，因此call.connection.responseQueue代表着连接着同个客户端的响应消息发送队列，发送到同个客户端的call存放在同一个responseQueue中)。当responseQueue的长度为1时会调用processResponse，但如果存在一些特殊情况（返回的结果数量太大或者网络缓慢），没能一次性将结果发送，则会向Responser的selector注册写事件，由Responser将响应结果采用异步的方式继续发送未发送完的数据。来看Responser的run方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117@Overridepublic void run() &#123; LOG.info(Thread.currentThread().getName() + ": starting"); SERVER.set(Server.this); try &#123; doRunLoop(); &#125; finally &#123; LOG.info("Stopping " + Thread.currentThread().getName()); try &#123; writeSelector.close(); &#125; catch (IOException ioe) &#123; LOG.error("Couldn't close write selector in " + Thread.currentThread().getName(), ioe); &#125; &#125;&#125;private void doRunLoop() &#123; long lastPurgeTime = 0; // last check for old calls. while (running) &#123; try &#123; waitPending(); // If a channel is being registered, wait. writeSelector.select(PURGE_INTERVAL); Iterator&lt;SelectionKey&gt; iter = writeSelector.selectedKeys().iterator(); while (iter.hasNext()) &#123; SelectionKey key = iter.next(); iter.remove(); try &#123; if (key.isWritable()) &#123; doAsyncWrite(key); &#125; &#125; catch (CancelledKeyException cke) &#123; // something else closed the connection, ex. reader or the // listener doing an idle scan. ignore it and let them clean // up Call call = (Call)key.attachment(); if (call != null) &#123; LOG.info(Thread.currentThread().getName() + ": connection aborted from " + call.connection); &#125; &#125; catch (IOException e) &#123; LOG.info(Thread.currentThread().getName() + ": doAsyncWrite threw exception " + e); &#125; &#125; //等待时间设置，如果消息超过该时间还没有发送成功，则会执行后面的代码，关闭管道 long now = Time.now(); if (now &lt; lastPurgeTime + PURGE_INTERVAL) &#123; continue; &#125; lastPurgeTime = now; // // If there were some calls that have not been sent out for a // long time, discard them. // if(LOG.isDebugEnabled()) &#123; LOG.debug("Checking for old call responses."); &#125; ArrayList&lt;Call&gt; calls; // get the list of channels from list of keys. // 搜集超过时间还未发送的call synchronized (writeSelector.keys()) &#123; calls = new ArrayList&lt;Call&gt;(writeSelector.keys().size()); iter = writeSelector.keys().iterator(); while (iter.hasNext()) &#123; SelectionKey key = iter.next(); Call call = (Call)key.attachment(); if (call != null &amp;&amp; key.channel() == call.connection.channel) &#123; calls.add(call); &#125; &#125; &#125; //逐个关闭 for(Call call : calls) &#123; doPurge(call, now); &#125; &#125; catch (OutOfMemoryError e) &#123; // // we can run out of memory if we have too many threads // log the event and sleep for a minute and give // some thread(s) a chance to finish // LOG.warn("Out of Memory in server select", e); try &#123; Thread.sleep(60000); &#125; catch (Exception ie) &#123;&#125; &#125; catch (Exception e) &#123; LOG.warn("Exception in Responder", e); &#125; &#125;&#125;private void doAsyncWrite(SelectionKey key) throws IOException &#123; Call call = (Call)key.attachment(); if (call == null) &#123; return; &#125; if (key.channel() != call.connection.channel) &#123; throw new IOException("doAsyncWrite: bad channel"); &#125; synchronized(call.connection.responseQueue) &#123; if (processResponse(call.connection.responseQueue, false)) &#123; try &#123; key.interestOps(0); &#125; catch (CancelledKeyException e) &#123; /* The Listener/reader might have closed the socket. * We don't explicitly cancel the key, so not sure if this will * ever fire. * This warning could be removed. */ LOG.warn("Exception while changing ops : " + e); &#125; &#125; &#125;&#125; Responser异步继续发送在选择器中注册的写事件中的响应消息（还是调用），并设定超时时间，超过时间还未发送则对其进行关闭。 总体流程客户端客户端实际时使用了动态代理在在Invoker方法中发送了远程过程调用的请求到服务端，并等待接受结果。 服务端服务端采用reactor基于事件驱动的设计模式，利用JDK自带的socket通信机制和线程池完成。 参考http://bigdatadecode.club/Hadoop%20RPC%20%E8%A7%A3%E6%9E%90.html 《Hadoop技术内幕：深入解析YARN架构设计与实现原理》,图片也来自此。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字符串编辑距离]]></title>
    <url>%2F%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB.html</url>
    <content type="text"><![CDATA[编辑距离（Edit Distance），这里指的是Levenshtein距离，也就是字符串S1通过插入、修改、删除三种操作最少能变换成字符串S2的次数。接下来介绍利用动态规划来求解字符串的编辑距离。 定义：$s_1$和$s_2$表示两字符串，$dist(i, j)$表示字符串$s_1$的前$i$个字符串和$s_2$的前$j$个字符串的编辑距离，$s_1(i)$和$s_2(j)$分别表示$s1$的第$i$个字符和$s2$的第$j$字符。 若$s_1(i) = s_2(j)$,则$dist(i, j)$就等于$s_1$的前$i-1$个字符串和$s_2$的前$j-1$个字符串的编辑距离即 $dist(i, j) = dist(i-1,j-1) $ 若$s_1(i) \neq s_2(j)$,则为了使$s_1(i) = s_2(j)$,可以通过在$s_1$的第$i$个字符处插入$s_2$的第$j$个字符，或替换$s_1$的第$i$个字符为$s_2$的第$j$个字符,或删除$s_1(s_2)$的第$i(j)$个字符(即使删除之后还是会出现不同),但是上述都是在进行了一次字符的操作之后,将转化为字问题求解，如上述的替换使$s_1(i) = s_2(j)$，则 $dist(i, j) = dist(i-1,j-1) + 1$ ，插入和删除使 $dist(i, j) = dist(i-1,j) +1$或者 $dist(i, j) = dist(i,j-1) +1$ 。 基于上述的情况可以得出递推公式：$$dist(i,j) = \begin{equation}\left\{ \begin{array}{lr} i, &amp; j = 0 \\ j, &amp; i = 0 \\ dist(i-1,j-1), &amp; s_1(i) = s_2(j) \\ min(d(i-1,j), d(i,j-1),d(i-1,j-1)), &amp; s_1(i) \neq s_2(j) \\ \end{array}\right.\end{equation}$$例如：$s_1 = “abcd”$,$s_2 = “abfce”$,则利用动态规划求解的矩阵为 a b f c e 0 1 2 3 4 5 a 1 0 1 2 3 4 b 2 1 0 1 2 3 c 3 2 1 1 1 2 d 4 3 2 2 2 2 代码实现1234567891011121314151617181920212223242526272829public static int levenshtein(String s1, String s2)&#123; if(s1 == null)&#123; return s2 == null? 0:s2.length(); &#125; if(s2 == null)&#123; return s1 == null? 0:s1.length(); &#125; int n = s1.length(); int m = s2.length(); int[][] matrix = new int[n+1][m+1]; for(int i = 0;i &lt;= n;i++)&#123; matrix[i][0] = i; &#125; for(int j = 0;j &lt;= m;j++)&#123; matrix[0][j] = j; &#125; for(int i = 1;i &lt;= n;i++)&#123; for(int j = 1;j &lt;= m;j++)&#123; if(s1.charAt(i-1) == s2.charAt(j-1))&#123; matrix[i][j] = matrix[i-1][j-1]; &#125;else &#123; matrix[i][j] = Math.min(Math.min(matrix[i-1][j], matrix[i][j-1]), matrix[i-1][j-1]) + 1; &#125; &#125; &#125; return matrix[n][m];&#125; 上述的方法其中有一个不足之处是当两个字符串太长时，则对应申请的数组占用的空间也变大。而上述的算法中，在更新$dist(i,j)$的过程中只用到了$dist(i-1,j)$、$dist(i,j-1)$、$dist(i-1,j-1)$这三个数，因此我们可以只存储更新的上一行的数据，这样就可以得到$dist(i-1,j)$，$dist(i-1,j-1)$这两个数，而$d(i,0) = i$,可以在此基础上根据上一行的$d(i-1,0)$和$d(i-1,1)$推出$d(i-1,1)$,在结合上一行的$d(i-1,1)$和$d(i-1,2)$推出$d(i-1,2)$…以此类推得到第$i$行的数据。接下来是改进后的代码实现 1234567891011121314151617181920212223242526public static int levenshteinImprove(String s1, String s2)&#123; if(s1 == null) return s2 == null? 0:s2.length(); if(s2 == null) return s1 == null? 0:s1.length(); int n = s1.length(),m = s2.length(); int[] matrix = new int[m+1]; for(int i = 0;i &lt;= m;i++)&#123; matrix[i] = i; &#125; for(int i = 1;i &lt;= n;i++)&#123; int pre = matrix[0]; matrix[0] = i; for(int j = 1;j &lt;= m;j++)&#123; int tmp = matrix[j]; if(s1.charAt(i-1) == s2.charAt(j-1))&#123; matrix[j] = pre; &#125;else &#123; matrix[j] = Math.min(Math.min(pre, matrix[j-1]), matrix[j]) + 1; &#125; pre = tmp; &#125; &#125; return matrix[m];&#125; 上述代码还可以改进一个小细节，当比较的字符串长短一个很长，一个很短时，我们可以比较其长短，取较短的字符串长度作为开辟的数组的长度。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>字符串</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
</search>
